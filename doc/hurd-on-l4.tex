\documentclass[9pt,a4paper]{extarticle}
%\usepackage{german}
%\usepackage[margin=2.5cm]{geometry}

\newenvironment{comment}{\footnotesize \begin{quote}}{\end{quote}}

\title{Porting the GNU Hurd to the L4 Micorkernel}
\author{Marcus Brinkmann, Peter De Schrijver, Daniel Wagner}
\date{August 2003}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage

\section{Introduction}

The GNU Hurd is a multi-server operating system running on top of a
microkernel (currently Mach variants).  The core motivation of the
Hurd is the following:

\begin{quote}
  \emph{The operating system should enable its users to share the
    resources of the system without harming each other.}
\end{quote}

The focus is on the user, the system should try to allow the user to
do anything that is not harmful for other users.  Many operating
systems either restrict what the user can do to be more secure, while
others allow the user to do everything, but fail on protecting the
users from each other effectively.

The Hurd is designed to minimize the system code that the user is
required to use, while allowing the user to use, ignore or replace the
remaining system code, and this without harming other users.

So while the L4 microkernel tries to minimize the policy that the
kernel enforces on the software running on it, the Hurd tries to
minimize the policy that the operating system enforces on its users.
Furthermore, the Hurd also aims to provide a POSIX compatible general
purpose operating system.  However, this POSIX personality of the Hurd
is provided for convenience only, and to make the Hurd useful.  Other
personalities can be implemented and used by the users of the system
along with the POSIX personality.  This default personality of the
Hurd also provides some convenient features that allow the user to
extend the system so that all POSIX compatible programs can take
advantage of it.

These notes are a moving target in the effort to find the best
strategy to port the Hurd to the L4 microkernel.

\begin{comment}
  Remarks about the history of a certain feature and implementation
  details are set in a smaller font and separated from the main text,
  just like this paragraph.  Because this is work in progress, there
  are naturally a lot of such comments.
\end{comment}


\section{Booting}

A multiboot-compliant bootloader, for example GRUB, loads the loader
program \texttt{laden}, the kernel, $\sigma_0$, the rootserver and
further modules.  The loader is started, patches the kernel interface
page, and starts the kernel.  The kernel starts $\sigma_0$ and the
rootserver.  The rootserver has to deal with the other modules.


\subsection{The loader \texttt{laden}}

\texttt{laden} is a multiboot compliant kernel from the perspective of
GRUB.  It expects at least three modules.  The first module is the L4
kernel image, the second module is the $\sigma_0$ server image, and
the third module is the rootserver image.

\begin{comment}
  Later, the L4 kernel will support the optional UTCB paging server
  $\sigma_1$, which has to be treated like the other initial servers
  by \texttt{laden}.  A command line option to \texttt{laden} will
  allow the user to specify if the third module is the rootserver or
  $\sigma_1$.  If $\sigma_1$ is used, the rootserver is the fourth
  module in the list.
\end{comment}

\texttt{laden} copies (or moves) the three executable images to the
right location in memory, according to their respective ELF headers.
It also initializes the BSS section to zero.

\begin{comment}
  Laden has to deal with overlapping source and destination memory
  areas in an intelligent way.  It currently will detect such
  situations, but is not always able to find a solution, even if one
  exists.
\end{comment}

Then it searches for the kernel interface page (KIP) in the L4 kernel
image and modifies it in the following way:

\begin{itemize}
\item The memory descriptors are filled in according to the memory
  layout of the system.  On ia32, this information is -- at least
  partially -- provided by GRUB.

  \begin{comment}
    GRUB seems to omit information about the memory that is shared
    with the VGA card.  \texttt{laden} creates a special entry for
    that region, overriding any previous memory descriptor.
  \end{comment}
  
\item The start and end addresses and the entry point of the initial
  servers are filled in.

  \begin{comment}
    A future version of L4 should support adding information about the
    UTCB area of the initial rootserver as well.  Until then, the
    rootserver has no clean way to create a new thread (a hack is used
    by the rootserver to calculate the UTCB addresses for other
    threads).
  \end{comment}

\item The \verb/boot_info/ field is initialized.

  \begin{comment}
    The \verb/boot_info/ field is currently set to the GRUB
    \verb/multiboot_info/ structure.  This only works for the ia32
    architecture of course.  We might want to have a more architecture
    independent way to pass the information about further modules to
    the rootserver.  We also might want to gather the information
    provided by GRUB in a single page (if it is not).
  \end{comment}
\end{itemize}


\subsection{The L4 kernel}

The L4 kernel initializes itself and then creates the address spaces
and threads for the initial servers $\sigma_0$ and the rootserver.  It
maps all physical memory idempotently into $sigma_0$, and sets the
pager of the rootserver thread to $sigma_0$.  Then it starts the
initial servers.


\subsection{The initial server $\sigma_0$}

$\sigma_0$ acts as the pager for the rootserver, answering page fault
messages by mapping the page at the fault address idempotently in the
rootserver.

\begin{comment}
  $\sigma_0$ can also be used directly by sending messages to it,
  according to the $sigma_0$ RPC protocol.  This is used by the kernel
  to allocate reserved memory, but can also be used by the user to
  explicitely allocate more memory than single pages indirectly via
  page faults.
\end{comment}

The thread ID of $\sigma_0$ is (\verb/UserBase, 1)/.

\begin{comment}
  We will write all thread IDs in the form (\verb/thread nr/,
  \verb/version/).
\end{comment}

Any fpage will only be provided to one thread.  $\sigma_0$ will return
an error if another thread attempts to map or manipulate an fpage that
has already been given to some other thread, even if both threads
reside in the same address space.


\subsection{The initial server $\sigma_1$}

$\sigma_1$ is intended to provide a paging service for UTCB memory.
This will allow orthogonal persistence to be implemented.  It is not
yet supported.

The thread ID of $\sigma_1$ is (\verb/UserBase + 1, 1)/.


\subsection{The rootserver}

The rootserver is the only task in the system which threads can
perform privileged system calls.  So the rootserver must provide
wrappers for the system calls to other unprivileged system tasks.

\begin{comment}
  For this, a simple authentication scheme is required.  The
  rootserver can keep a small, statically allocated table of threads
  which are granted access to the system call wrappers.  The caller
  could provide the index in the table for fast O(1) lookup instead
  linear search.  Threads with access could be allowed to add other
  threads or change existing table entries.  The same scheme can be
  used int the device driver framework.
  
  The rootserver should have one thread per CPU, and run at a high
  priority.
\end{comment}

The rootserver has the following initial state:

\begin{itemize}
\item Its thread ID is (\verb/UserBase + 2/, 1).

\item The priority is set to the 255, the maximum value.

\item The instruction pointer \verb/%eip/ is set to the entry point,
all other registers are undefined (including the stack pointer).

\item The pager is set to $\sigma_0$.
  
\item The exception handler set to \verb/nilthread/.
  
\item The scheduler is set to the rootserver thread itself.
\end{itemize}

So the first thing the rootserver has to do is to set up a simple
stack.

Then the rootserver should evaluate the \verb/boot_info/ field in the
KIP to find the information about the other modules.  It should parse
the information and create the desired initial tasks of the operating
system.  The Hurd uses a boot script syntax to allow to pass
information about other initial tasks and the root tasks to each
initial task in a generalized manner.

\begin{comment}
  The exact number and type of initial tasks necessary to boot the
  Hurd are not yet known.  Chances are that this list includes the
  task server, the physical memory server, the device servers, and the
  boot filesystem.  The boot filesystem might be a small simple
  filesystem, which also includes the device drivers needed to access
  the real root filesystem.
\end{comment}


\section{Inter-process communication (IPC)}

The Hurd requires a capability system.  Capabilities are used to proof
your identity to other servers (authentication), and access
server-side implemented objects like devices, files, directories,
terminals, and other things.  The server can use a capability for
whatever it wants.  Capabilities provide interfaces.  Interfaces can
be invoked by sending messages to the capability.  In L4, this means
that a message is sent to a thread in the server providing the
capability, with the identifier for the capability in the message.

Capabilities are protected objects.  Access to a capability needs to
be granted by the server.  Once you have a capability, you can copy it
to other tasks (if the server permits it, which is usually the case).
In the Hurd, access to capabilities is always granted to a whole task,
not to individual threads.

\begin{comment}
  There is no reason for the server not to permit it, because the
  holder of the capability could also just act as a proxy for the
  intended receiver instead copying the capability to it.  The
  operation might fail anyway, for example because of resource
  shortage, in particular if the server puts a quota on the number of
  capabilities a user can hold.
\end{comment}

Capabilities provide two essential services to the Hurd.  They are
used to restrict access to a server function, and they are the
standard interface the components in the Hurd use to communicate with
each others.  Thus, it is important that their implementation is fast
and secure.

\begin{comment}
  There are several ways to implement such a capability system.  A
  more traditional design would be a global, trusted capability server
  that provides capabilities to all its users.  The L4 redirector
  could be used to reroute all client traffic automatically through
  this server.  This approach has several disadvantages:

  \begin{itemize}
  \item It adds a lot of overhead to every single RPC, because all
    traffic has to be routed through the capability server, which must
    then perform the authentication on the server's behalf.
  \item It would be difficult to copy a capability to another task.
    Either the cap server would have to provide interfaces for clients
    to do it, or it would be have to know the message format for every
    interface and do it automatically.
  \item It would be a single point of failure.  If it had a bug and
    crashed, the whole system would be affected.
  \item Users could not avoid it, it would be enforced system code.
  \item It is inflexible.  It would be hard to replace or extend at
    run-time.
  \end{itemize}
  
  Another approach is taken by CORBA with IORs.  IORs contain long
  random numbers which allow the server to identify a user of an
  object.  This approach is not feasible for the following reasons:

  \begin{itemize}
  \item Even good random numbers can be guessed.  Long enough random
    numbers can reduce the likelihood to arbitrary small numbers,
    though (below the probability of a hardware failure).
  \item Good random numbers are in short supply, and is slow to
    generate.  Good pseudo random is faster, but it is still difficult
    to generate.  The random number generator would become a critical
    part of the operating system.
  \item The random number had to be transfered in every single
    message.  Because it would have to be long, it would have a
    significant negative impact on IPC performance.
  \end{itemize}
\end{comment}

The Hurd implements the capability system locally in each task.  A
common default implementation will be shared by all programs.
However, a malicious untrusted program could do nothing to disturb the
communication of other tasks.  A capability will be identified in the
client task by a the server thread and a local identifier (which can
be different from client to client).  The server thread will receive
messages for the capabilities.  The first argument in the message is
the capability identifier.  Although every task can get different IDs
for the same capability, a well-behaving server will give the same ID
to a client which already has a capability and gets the same
capability from another client.  So clients can compare capability IDs
from the server numerically to check if two capabilities are the same,
but only if one of the two IDs is received while the client already
had the other one.

Because access to a capability must be restricted, the server needs to
be careful in only allowing registered and known users to access the
capability.  For this, the server must be sure that it can determine
the sender of a message.  In L4, this is easy on the surface: The
kernel provides the receiving thread with the sender's thread ID,
which also contains the task ID in the version field.  However, the
server must also know for sure if this task is the same task that it
gave access to the capability.  Comparing the task IDs numerically is
not good enough, the server must also somehow have knowledge or
influence on how task IDs are reused when tasks die and are created.

The same is true for the client, of course, which trusts the server
and thus must be sure that it is not tricked into trusting on
unreliable data from an imposter, or sends sensitive data to it.

\begin{comment}
  The task server wants to reuse thread numbers because that makes
  best use of kernel memory.  Reusing task IDs, the version field of a
  thread ID, is not so important, but there are only 14 bits for the
  version field (and the lower six bits must not be all zero).  So a
  thread ID is bound to be reused eventually.
  
  Using the version field in a thread ID as a generation number is not
  good enough, because it is so small.  Even on 64-bit architectures,
  where it is 32 bit long, it can eventually overflow.
\end{comment}

The best way to prevent that a task can be tricked into talking to an
imposter is to have the task server notify the task if the
communication partner dies.  The task server must guarantee that the
task ID is not reused until all tasks that got such a notification
acknowledge that it is processed, and thus no danger of confusion
exists anymore.

The task server will provide references to task IDs in form of
\emph{task info capabilities}.  If a task has a task info capability
for another task, it will prevent that this other task's task ID is
reused even if that task dies, and it will also make sure that task
death notifications are delivered in that case.

\begin{comment}
  Because only the task server can create and destroy tasks, and
  assign task IDs, there is no need to hold such task info
  capabilities for the task server, nor does the task server need to
  hold task info capabilities for its clients.  This avoids the
  obvious bootstrap problem in providing capabilities in the task
  server.  This will even work if the task server is not the real task
  server, but a proxy task server (see section \ref{proxytaskserver}
  on page \pageref{proxytaskserver}).
\end{comment}

As task IDs are a global resource, care has to be taken that this
approach does not allow for a DoS-attack by exhausting the task ID
number space.

\begin{comment}
  Several strategies can be taken:

  \begin{itemize}
  \item Task death notifications can be monitored.  If there is no
    acknowdgement within a certain time period, the task server could
    be allowed to reuse the task ID anyway.  This is not a good
    strategy because it can considerably weaken the security of the
    system (capabilities might be leaked to tasks which reuse such a
    task ID reclaimed by force).
  \item The proc server can show dead task IDs which are not released
    yet, in analogy to the zombie processes in Unix.  It can also make
    available the list of tasks which prevent reusing the task ID, to
    allow users or the system administrator to clean up manually.
  \item Quotas can be used to punish users which do not acknowledge
    task death timely.  For example, if the number of tasks the user
    is allowed to create is restricted, the task info caps that the
    user holds for dead tasks could be counted toward that limit.
  \item Any task could be restricted to as many task ID references as
    there are live tasks in the system, plus some slack.  That would
    prevent the task from creating new task info caps if it does not
    release old ones from death tasks.  The slack would be provided to
    not unnecessarily slow down a task that processes task death
    notifications asynchronously to making connections with new tasks.
  \end{itemize}
  
  In particular the last two approaches should proof to be effective
  in providing an incentive for tasks to release task info caps they
  do not need anymore.
\end{comment}


\subsection{Capabilities}

This subsection contains implementation details about capabilities.

A server will usually operate on objects, and not capabilities.  In
the case of a filesystem, this could be file objects, for example.

\begin{comment}
  In the Hurd, filesystem servers have to keep different objects for
  each time a file is looked up (or ``opened''), because some state,
  for example authentication, open flags and record locks, are
  associated not with the file directly, but with this instance of
  opening the file.  Such a state structure (``credential'') will also
  contain a pointer and reference to the actual file node.  For
  simplicity, we will assume that the capability is associated with a
  file node directly.
\end{comment}

To provide access to the object to another task, the server creates a
capability, and associates it with the object (by setting a hook
variable in the capability).  From this capability, the server can
either create send references to itself, or to other tasks.  If the
server creates send references for itself, it can use the capability
just as it can use capabilities implemented by other servers.  This
makes access to locally and remotely implemented capabilities
identical.  If you write code to work on capabilities, it can be used
for remote objects as well as for local objects.

If the server creates a send reference for another task (a client), a
new capability ID will be created for this task.  This ID will only be
valid for this task, and should be returned to the client.

The client itself will create a capability object from this capability
ID.  The capability will also contain information about the server,
for example the server thread which should be used for sending
messages to the capability.

If the client wants to send a message, it will send it to the provided
server thread, and use the capability ID it got from the server as the
first argument in the RPC.  The server receives the message, and now
has to look up the capability ID in the list of capabilties for this
task.

\begin{comment}
  The server knows the task ID from the version field of the sender's
  thread ID.  It can look up the list of capabilities for this task in
  a hash table.  The capability ID can be an index into an array, so
  the server only needs to perform a range check.  This allows to
  verify quickly that the user is allowed to access the object.
  
  This is not enough if several systems run in parallel on the same
  host.  Then the version ID for the threads in the other systems will
  not be under the control of the Hurd's task server, and can thus not
  be trusted.  The server can still use the version field to find out
  the task ID, which will be correct \emph{if the thread is part of
    the same subsystem}.  It also has to verify that the thread
  belongs to this subsystem.  Hopefully the subsystem will be encoded
  in the thread ID.  Otherwise, the task server has to be consulted
  (and, assuming that thread numbers are not shared by the different
  systems, the result can be cached).
\end{comment}

The server reads out the capability associated with the capability ID,
and invokes the server stub according to the message ID field in the
message.

After the message is processed, the server sends it reply to the
sender thread with a zero timeout.

\begin{comment}
  Servers must never block on sending messages to clients.  Even a
  small timeout can be used for DoS-attacks.  The client can always
  make sure that it receives the reply by using a combined send and
  receive operation together with an infinite timeout.
\end{comment}

The above scheme assumes that the server and the client already have
task info caps for the respective other task.  This is the normal
case, because acquiring these task info caps is part of the protocol
that is used when a capability is copied from one task to another.


\subsubsection{Bootstrapping a client-server connection}

If the client and the server do not know about each other yet, then
they can bootstrap a connection without support from any other task
except the task server.  The purpose of the initial handshake is to
give both participants a chance to acquire a task info cap for the
other participants task ID, so they can be sure that from there on
they will always talk to the same task as they talked to before.

\paragraph{Preconditions}
The client knows the thread ID of the server thread that receives and
processes the bootstrap messages.  Some other task might hold a task
info capability to the server the client wants to connect to.

\begin{comment}
  If no such other tasks exists, the protocol will still work.
  However, the client might not get a connection to the server that
  run at the time the client started the protocol, but rather to the
  server that run at the time the client acquired the task info cap
  for the server's task ID (after step 1 below).
 
  This is similar to how sending signals works in Unix: Technically,
  at the time you write \texttt{kill 203}, and press enter, you do not
  know if the process with the PID 203 you thought of will receive the
  signal, or some other process that got the PID in the time between
  you getting the information about the PID and writing the
  \texttt{kill}-command.
\end{comment}

FIXME: Here should be the pseudo code for the protocol.  For now, you
have to take it out of the long version.

\begin{enumerate}
  
\item The client acquires a task info capability for the server's task
  ID, either directly from the task server, or from another task in a
  capability copy.  From that point on, the client can be sure to
  always talk to the same task when talking to the server.
  
  Of course, if the client already has a task info cap for the server
  it does not need to do anything in this step.

\begin{comment}
  As explained above, if the client does not have any other task
  holding the task info cap already, it has no secure information
  about what this task is for which it got a task info cap.
\end{comment}

\item The client sends a message to the server, requesting the initial
  handshake.
  
\item The server receives the message, and acquires a task info cap
  for the client task (directly from the task server).
  
  Of course, if the server already has a task info cap for the client
  it does not need to do anything in this step.

\begin{comment}
  At this point, the server knows that future messages from this task
  will come from the same task as it got the task info cap for.
  However, it does not know that this is the same task that sent the
  initial handshake request in step 2 above.  This shows that there is
  no sense in verifying the task ID or perform any other
  authentication before acquiring the task info cap.
\end{comment}

\item The server replies to the initial handshake request with an
  empty reply message.

\begin{comment}
  Because the reply now can go to a different task than the request
  came from, sending the reply might fail.  It might also succeed and
  be accepted by the task that replaced the requestor.  Or it might
  succeed normally.  The important thing is that it does not matter to
  the server at all.  It would have provided the same ``service'' to
  the ``imposter'' of the client, if he had bothered to do the
  request.  As no authentication is done yet, there is no point for
  the server to bother.
  
  This means however, that the server needs to be careful in not
  consuming too many resources for this service.  However, this is
  easy to achieve.  Only one task info cap per client task will ever
  be held in the server.  The server can either keep it around until
  the task dies (and a task death notification is received), or it can
  clean it up after some timeout if the client does not follow up and
  do some real authentication.
\end{comment}

\item The client receives the reply message to its initial handshake
  request.
  
\item The client sends a request to create its initial capability.
  How this request looks depends on the type of the server and the
  initial capabilities it provides.  Here are some examples:

  \begin{itemize}
  \item A filesystem might provide an unauthenticated root directory
    object in return of the underlying node capability, which is
    provided by the parent filesystem and proves to the filesystem
    that the user was allowed to look up the root node of this
    filesystem (see section \ref{xfslookup} on page
    \pageref{xfslookup}).

    \begin{comment}
      In this example, the parent filesystem will either provide the
      task info cap for the child filesystem to the user, or it will
      hold the task info cap while the user is creating their own
      (which the user has to verify by repeating the lookup, though).
      Again, see section \ref{xfslookup} on page \pageref{xfslookup}.
      
      The unauthenticated root directory object will then have the be
      authenticated using the normal reauthentication mechanism (see
      section \ref{auth} on pageref{auth}).  This can also be combined
      in a single RPC.
    \end{comment}
    
  \item Every process acts as a server that implements the signal
    capability for this process.  Tasks who want to send a signal to
    another task can perform the above handshake, and then provide
    some type of authentication capability that indicates that they
    are allowed to send a signal.  Different authentication
    capabilities can be accepted by the signalled task for different
    types of signals.

    \begin{comment}
      The Hurd used to store the signal capability in the proc server,
      where authorized tasks could look it up.  This is no longer
      possible because a server can not accept capabilities
      implemented by untrusted tasks, see below.
    \end{comment}
  \end{itemize}
  
\item The server replies with whatever capability the client
  requested, provided that the client could provide the necessary
  authentication capabilities, if any.

  \begin{comment}
    It is not required that the server performs any authentication at
    all, but it is recommended, and all Hurd servers will do so.
    
    In particular, the server should normally only allow access from
    tasks running in the same system, if running multiple systems on
    the same host is possible.
  \end{comment}
\end{enumerate}

\paragraph{Result}
The client has a task info capability for the server and an
authenticated capability.  The server has a task info capability for
the client and seen some sort of authentication for the capability it
gave to the client.

\begin{comment}
  If you think that the above protocol is complex, you have seen
  nothing yet!  Read on.
\end{comment}


\subsubsection{Returning a capability from a server to a client}

Before we go on to the more complex case of copying a capability from
one client to another, let us point out that once a client has a
capability from a server, it is easy for the server to return more
capabilities it implements to the client.

The server just needs to create the capability, acquire a capability
ID in the client's cap ID space, and return the information in the
reply RPC.

FIXME: Here should be the pseudo code for the protocol.  For now, you
have to take it out of the long version.

\begin{comment}
  The main point of this section is to point out that only one task
  info capability is required to protect all capabilities provided to
  a single task.  The protocols described here always assume that no
  task info caps are held by anyone (except those mentioned in the
  preconditions).  In reality, sometimes the required task info caps
  will already be held.
\end{comment}


\subsubsection{Copying a capability from one client to another task}

The most complex operation in managing capabilities is to copy or move
a capability from the client to another task, which subsequently
becomes a client of the server providing the capability.  The
difficulty here lies in the fact that the protocol should be fast, but
also robust and secure.  If any of the participants dies unexpectedly,
or any of the untrusted participants is malicious, the others should
not be harmed.

\paragraph{Preconditions}
The client $C$ has a capability from server $S$ (this implies that $C$
has a task info cap for $S$ and $S$ has a task info cap for $C$).  It
wants to copy the capability to the destination task $D$.  For this,
it will have to make RPCs to $D$, so $C$ has also a capability from
$D$ (this implies that $C$ has a task info cap for $D$ and $D$ has a
task info cap for $C$).  Of course, the client $C$ trusts its servers
$S$ and $D$.  $D$ might trust $S$ or not, and thus accept or reject
the capability that $C$ wants to give to $D$.  $S$ does not trust
either $C$ or $D$.
  
The task server is also involved, because it provides the task info
capabilities.  Everyone trusts the task server they use.  This does
not need to be the same one for every participant.

FIXME: Here should be the pseudo code for the protocol.  For now, you
have to take it out of the long version.

\begin{enumerate}
\item The client invokes the \verb/cap_ref_cont_create/ RPC on the
  capability, providing the task ID of the intended receiver $D$ of
  the capability.
  
\item The server receives the \verb/cap_ref_cont_create/ RPC from the
  client.  It requests a task info cap for $D$ from its trusted task
  server, under the constraint that $C$ is still living.

  \begin{comment}
    A task can provide a constraint when creating a task info cap in
    the task server.  The constraint is a task ID.  The task server
    will only create the task info cap and return it if the task with
    the constraint task ID is not destroyed.  This allows for a task
    requesting a task info capability to make sure that another task,
    which also holds this task info cap, is not destroyed.  This is
    important, because if a task is destroyed, all the task info caps
    it held are released.

    In this case, the server relies on the client to hold a task info
    cap for $D$ until it established its own.  See below for what can
    go wrong if the server would not provide a constraint and both,
    the client and the destination task would die unexpectedly.
  \end{comment}
  
  Now that the server established its own task info cap for $D$, it
  creates a reference container for $D$, that has the following
  properties:

  \begin{itemize}
  \item The reference container has a single new reference for the
    capability.
    
  \item The reference container has an ID that is unique among all
    reference container IDs for the client $C$.
    
  \item The reference container is associated with the client $C$.  If
    $C$ dies, and the server processes the task death notification for
    it, the server will destroy the reference container and release
    the capability reference it has (if any).  All resources
    associated with the reference container will be released.  If this
    reference container was the only reason for $S$ to hold the task
    info cap for $D$, the server will also release the task info cap
    for $D$.
    
  \item The reference container is also associated with the
    destination task $D$.  If $D$ dies, and the server processes the
    task death notification for it, the server will release the
    capability reference that is in the reference container (if any).
    It will not destroy the part of the container that is associated
    with $C$.
  \end{itemize}

  The server returns the reference container ID $R$ to the client.

\item The client receives the reference container ID $R$.

  \begin{comment}
    If several capabilities have to be copied in one message, the
    above steps need to be repeated for each capability.  With
    appropriate interfaces, capabilities could be collected so that
    only one call per server has to be made.  We are assuming here
    that only one capability is copied.
  \end{comment}

\item The client sends the server thread ID $T$ and the reference
  container ID $R$ to the destination task $D$.
  
\item The destination task $D$ receives the server thread ID $T$ and
  the reference container ID $R$ from $C$.
  
  It now inspects the server thread ID $T$, and in particular the task
  ID component of it.  $D$ has to make the decision if it trusts this
  task to be a server for it, or if it does not trust this task.
  
  If $D$ trusts $C$, it might decide to always trust $T$, too,
  irregardless of what task contains $T$.
  
  If $D$ does not trust $C$, it might be more picky about the task
  that contains $T$.  This is because $D$ will have to become a client
  of $T$, so it will trust it.  For example, it will block on messages
  it sends to $T$.

  \begin{comment}
    If $D$ is a server, it will usually only accept capabilities from
    its client that are provided by specific other servers it trusts.
    This can be the authentication server, for example (see section
    \ref{auth} on page \pageref{auth}).
    
    Usually, the type of capability that $D$ wants to accept from $C$
    is then further restricted, and only one possible trusted server
    implements that type of capabilities.  Thus, $D$ can simply
    compare the task ID of $T$ with the task ID of its trusted server
    (authentication server, ...) to make the decision if it wants to
    accept the capability or not.
  \end{comment}
  
  If $D$ does not trust $T$, it replies to $C$ (probably with an error
  value indicating why the capability was not accepted).  In that
  case, jump to step 8.
  
  Otherwise, it requests a task info cap for $S$ from its trusted task
  server, under the constraint that $C$ is still living.
  
  Then $D$ sends a \verb/cap_ref_cont_accept/ RPC to the server $S$,
  providing the task ID of the client $C$ and the reference container
  ID $R$.

\begin{comment}
  \verb/cap_ref_cont_accept/ is one of the few interfaces that is not
  sent to a (real) capability, of course.  Nevertheless, it is part of
  the capability object interface, hence the name.  You can think of
  it as a static member in the capability class, that does not require
  an instance of the class.
\end{comment}
  
\item The server receives the \verb/cap_ref_cont_accept/ RPC from the
  destination task $D$.  It verifies that a reference container exists
  with the ID $R$, that is associated with $D$ and $C$.
  
  \begin{comment}
    The server will store the reference container in data structures
    associated with $C$, under an ID that is unique but local to $C$.
    So $D$ needs to provide both information, the task ID and the
    reference container ID of $C$.
  \end{comment}

  If that is the case, it takes the reference from the reference
  container, and creates a capability ID for $D$ from it.  The
  capability ID for $D$ is returned in the reply message.
  
  From that moment on, the reference container is deassociated from
  $D$.  It is still associated with $C$, but it does not contain any
  reference for the capability.

  \begin{comment}
    It is not deassociated from $C$ and removed completely, so that
    its ID $R$ (or at least the part of it that is used for $C$) is
    not reused.  $C$ must explicitely destroy the reference container
    anyway because $D$ might die unexpectedly or return an error that
    gives no indication if it accepted the reference or not.
  \end{comment}
  
\item The destination task $D$ receives the capability ID and enters
  it into its capability system.  It sends a reply message to $C$.

  \begin{comment}
    If the only purpose of the RPC was to copy the capability, the
    reply message can be empty.  Usually, capabilities will be
    transfered as part of a larger operation, though, and more work
    will be done by $D$ before returning to $C$.
  \end{comment}
  
\item The client $C$ receives the reply from $D$.  Irregardless if it
  indicated failure or success, it will now send the
  \verb/cap_ref_cont_destroy/ message to the server $S$, providing the
  reference container $R$.

  \begin{comment}
    This message can be a simple message.  It does not require a reply
    from the server.
  \end{comment}
  
\item The server receives the \verb/cap_ref_cont_destroy/ message and
  removes the reference container $R$.  The reference container is
  deassociated from $C$ and $D$.  If this was the only reason that $S$
  held a task info cap for $D$, this task info cap is also released.

  \begin{comment}
    Because the reference container can not be deassociated from $C$
    by any other means than this interface, the client does not need
    to provide $D$.  $R$ can not be reused without the client $C$
    having it destroyed first.  This is different from the
    \verb/cap_ref_cont_accept/ call made by $D$, see above.
  \end{comment}

\end{enumerate}

\paragraph{Result}
For the client $C$, nothing has changed.  The destination task $D$
either did not accept the capability, and nothing has changed for it,
and also not for the server $S$.  Or $D$ accepted the capability, and
it now has a task info cap for $S$ and a reference to the capability
provided by $S$.  In this case, the server $S$ has a task info cap for
$D$ and provides a capability ID for this task.

The above protocol is for copying a capability from $C$ to $D$.  If
the goal was to move the capability, then $C$ can now release its
reference to it.

\begin{comment}
  Originally we considered to move capabilities by default, and
  require the client to acquire an additional reference if it wanted
  to copy it instead.  However, it turned out that for the
  implementation, copying is easier to handle.  One reason is that the
  client usually will use local reference counting for the
  capabilities it holds, and with local reference counting, one
  server-side reference is shared by many local references.  In that
  case, you would need to acquire a new server-side reference even if
  you want to move the capability.  The other reason is cancellation.
  If an RPC is cancelled, and you want to back out of it, you need to
  restore the original situation.  And that is easier if you do not
  change the original situation in the first place until the natural
  ``point of no return''.
\end{comment}

The above protocol quite obviously achieves the result as described in
the above concluding paragraph.  However, many other, and often
simpler, protocols would also do that.  The other protocols we looked
at are not secure or robust though, or require more operations.  To
date we think that the above is the shortest (in particular in number
of IPC operations) protocol that is also secure and robust (and if it
is not we think it can be fixed to be secure and robust with minimal
changes).  We have no proof for its correctness.  Our confidence comes
from the scrutiny we applied to it.  If you find a problem with the
above protocol, or if you can prove various aspects of it, we would
like to hear about it.

To understand why the protocol is laid out as it is, and why it is a
secure and robust protocol, one has to understand what could possibly
go wrong and why it does not cause any problems for any participant if
it follows its part of the protocol (independent on what the other
participants do).  In the following paragraphs, various scenarios are
suggested where things do not go as expected in the above protocol.
This is probably not a complete list, but it should come close to it.
If you find any other problematic scenario, again, let us know.

\begin{comment}
  Although some comments like this appear in the protocol description
  above, many comments have been spared for the following analysis of
  potential problems.  Read the analysis carefully, as it provides
  important information about how, and more importantly, why it works.
\end{comment}

\paragraph{The server $S$ dies}
What happens if the server S dies unexpectedly sometime throughout the
protocol?

\begin{comment}
  At any time a task dies, the task info caps it held are released.
  Also, task death notifications are sent to any task that holds task
  info caps to the now dead task.  The task death notifications will
  be processed asynchrnouly, so they might be processed immediately,
  or at any later time, even much later after the task died!  So one
  important thing to keep in mind is that the release of task info
  caps a task held, and other tasks noticing the task death, are
  always some time apart.
\end{comment}

Because the client $C$ holds a task info cap for $S$ no imposter can
get the task ID of $S$.  $C$ and $D$ will get errors when trying to
send messages to $S$.

\begin{comment}
  You might now wonder what happens if $C$ also dies, or if $C$ is
  malicious and does not hold the task info cap.  You can use this as
  an exercise, and try to find the answer on your own.  The answers
  are below.
\end{comment}

Eventually, $C$ (and $D$ if it already got the task info cap for $S$)
will process the task death notification and clean up their state.

\paragraph{The client $C$ dies}
The server $S$ and the destination task $D$ hold a task info cap for
$C$, so no imposter can get its task ID.  $S$ and $D$ will get errors
when trying to send messages to $C$.  Depending on when $C$ dies, the
capability might be copied successfully or not at all.

Eventually, $S$ and $D$ will process the task death notification and
release all resources associated with $C$.  If the reference was not
yet copied, this will include the reference container associated with
$C$, if any.  If the reference was already copied, this will only
include the empty reference container, if any.

\begin{comment}
  Of course, the participants need to use internal locking to protect
  the integrity of their internal data structures.  The above protocol
  does not show where locks are required.  In the few cases where some
  actions must be performed atomically, a wording is used that
  suggests that.
\end{comment}

\paragraph{The destination task $D$ dies}

The client $C$ holds a task info cap for $D$ over the whole operation,
so no imposter can get its task ID.  Depending on when $D$ dies, it
has either not yet accepted the capability, then $C$ will clean up by
destroying the reference container, or it has, and then $S$ will clean
up its state when it processes the task death notification for $D$.

\paragraph{The client $C$ and the destination task $D$ die}

This scenario is the reason why the server acquires its own task info
cap for $D$ so early, and why it must do that under the constraint
that $C$ still lives.  If $C$ and $D$ die before the server created
the reference container, then either no request was made, or creating
the task info cap for $D$ fails because of the constraint.  If $C$ and
$D$ die afterwards, then no imposter can get the task ID of $D$ and
try to get at the reference in the container, because the server has
its own task info cap for $D$.

\begin{comment}
  This problem was identified very late in the development of this
  protocol.  We just did not think of both clients dieing at the same
  time!  In an earlier version of the protocol, the server would
  acquire its task info cap when $D$ accepts its reference.  This is
  too late: If $C$ and $D$ die just before that, an imposter with
  $D$'s task ID can try to get the reference in the container before
  the server processes the task death notification for $C$ and
  destroys it.
\end{comment}

Eventually, the server will receive and process the task death
notifications.  If it processes the task death notification for $C$
first, it will destroy the whole container immediately, including the
reference, if any.  If it processes the task death notification for
$D$ first, it will destroy the reference, and leave behind the empty
container associated with $C$, until the other task death notification
is processed.  Either way no imposter can get at the capability.

Of course, if the capability was already copied at the time $C$ and
$D$ die, the server will just do the normal cleanup.

\paragraph{The client $C$ and the server $S$ die}

This scenario does not cause any problems, because on the one hand,
the destination task $D$ holds a task info cap for $C$, and it
acquires its own task info cap for $S$.  Although it does this quite
late in the protocol, it does so under the constraint that $C$ still
lives, which has a task info cap for $S$ for the whole time (until it
dies).  It also gets the task info cap for $S$ before sending any
message to it.  An imposter with the task ID of $S$, which it was
possible to get because $C$ died early, would not receive any message
from $D$ because $D$ uses $C$ as its constraint in acquireing the task
info cap for $S$.

\paragraph{The destination task $D$ and the server $S$ die}

As $C$ holds task info caps for $S$ and $D$, there is nothing that can
go wrong here.  Eventually, the task death notifications are
processed, but the task info caps are not released until the protocol
is completed or aborted because of errors.

\paragraph{The client $C$, the destination task $D$ and the server $S$ die}

Before the last one of these dies, you are in one of the scenarios
which already have been covered.  After the last one dies, there is
nothing to take care of anymore.

\begin{comment}
  In this case your problem is probably not the capability copy
  protocol, but the stability of your software!  Go fix some bugs.
\end{comment}

So far the scenarios where one or more of the participating tasks die
unexpectedly.  They could also die purposefully.  Other things that
tasks can try to do purposefully to break the protocol are presented
in the following paragraphs.

\begin{comment}
  A task that tries to harm other tasks by not following a protocol
  and behaving as other tasks might expect it is malicious.  Beside
  security concerns, this is also an issue of robustness, because
  malicious behaviour can also be triggered by bugs rather than bad
  intentions.
  
  It is difficult to protect against malicious behaviour by trusted
  components, like the server $S$, which is trusted by both $C$ and
  $D$.  If a trusted component is compromised or buggy, ill
  consequences for software that trusts it must be expected.  Thus, no
  analysis is provided for scenarious involving a malicious or buggy
  server $S$.
\end{comment}

\paragraph{The client $C$ is malicious}

If the client $C$ wants to break the protocol, it has numerous
possibilities to do so.  The first thing it can do is to provide a
wrong destination task ID when creating the container.  But in this
case, the server will return an error to $D$ when it tries to accept
it, and this will give $D$ a chance to notice the problem and clean
up.  This also would allow for some other task to receive the
container, but the client can give the capability to any other task it
wants to anyway, so this is not a problem.

\begin{comment}
  If a malicious behaviour results in an outcome that can also be
  achieved following the normal protocol with different parameters,
  then this not a problem at all.
\end{comment}

The client could also try to create a reference container for $D$ and
then not tell $D$ about it.  However, a reference container should not
consume a lot of resources in the server, and all such resources
should be attributed to $C$.  When $C$ dies eventually, the server
will clean up any such pending containers when the task death
notification is processed.

The same argument holds when $C$ leaves out the call to
\verb/cap_ref_cont_destroy/.

The client $C$ could also provide wrong information to $D$.  It could
supply a wrong server thread ID $T$.  It could supply a wrong
reference container ID $R$.  If $D$ does not trust $C$ and expects a
capability implemented by some specific trusted server, it will verify
the thread ID numerically and reject it if it does not match.  The
reference container ID will be verified by the server, and it will
only be accepted if the reference container was created by the client
task $C$.  Thus, the only wrong reference container IDs that the
client $C$ could use to not provoke an error message from the server
(which then lead $D$ to abort the operation) would be a reference
container that it created itself in the first place.  However, $C$
already is frree to send $D$ any reference container it created.

\begin{comment}
  Again $C$ can not achieve anything it could not achieve by just
  following the protocol as well.  If $C$ tries to use the same
  reference container with several RPCs in $D$, one of them would
  succeed and the others would fail, hurting only $C$.
  
  If $D$ does trust $C$, then it can not protect against malicious
  behaviour by $C$.
\end{comment}

To summarize the result so far: $C$ can provide wrong data in the
operations it does, but it can not achieve anything this way that it
could not achieve by just following the protocol.  In most cases the
operation would just fail.  If it leaves out some operations, trying
to provoke resource leaks in the server, it will only hurt itself (as
the reference container is strictly associated with $C$ until the
reference is accepted by $D$).

\begin{comment}
  For optimum performance, the server should be able to keep the
  information about the capabilities and reference containers a client
  holds on memory that is allocated on the clients behalf.
  
  It might also use some type of quota system.
\end{comment}

Another attack that $C$ can attempt is to deny a service that $S$ and
$D$ are expecting of it.  Beside not doing one or more of the RPCs,
this is in particular holding the task info caps for the time span as
described in the protocol.  Of course, this can only be potentially
dangerous in combination with a task death.  If $C$ does not hold the
server task info capability, then an imposter of $S$ could trick $D$
into using the imposter as the server.  However, this is only possible
if $D$ already trusts $C$.  Otherwise it would only allow servers that
it already trusts, and it would always hold task info caps to such
trusted servers when making the decision that it trusts them.
However, if $D$ trusts $C$, it can not protect against $C$ being
malicious.

\begin{comment}
  If $D$ does not trust $C$, it should only ever compare the task ID
  of the server thread against trusted servers it has a task info cap
  for.  It must not rely on $C$ doing that for $D$.
  
  However, if $D$ does trust $C$, it can rely on $C$ holding the
  server task info cap until it got its own.  Thus, the task ID of $C$
  can be used as the constraint when acquiring the task info cap in
  the protocol.
\end{comment}

If $C$ does not hold the task info cap of $D$, and $D$ dies before the
server acquires its task info cap for $D$, it might get a task info
cap for an imposter of $D$.  But if the client wants to achieve that,
it could just follow the protocol with the imposter as the destination
task.

\paragraph{The destination task $D$ is malicious}

The destination task has not as many possibilities as $C$ to attack
the protocol.  This is because it is trusted by $C$.  So the only
participant that $D$ can try to attack is the server $S$.  But the
server $S$ does not rely on any action by $D$.  $D$ does not hold any
task info caps for $S$.  The only operation it does is an RPC to $S$
accepting the capability, and if it omits that it will just not get
the capability (the reference will be cleaned up by $C$ or by the
server when $C$ dies).

The only thing that $D$ could try is to provide false information in
the \verb/cap_ref_cont_accept/ RPC.  The information in that RPC is
the task ID of the client $C$ and the reference container ID $R$.  The
server will verify that the client $C$ has previously created a
reference container with the ID $R$ that is destined for $D$.  So $D$
will only be able to accept references that it is granted access to.
So it can not achieve anything that it could not achieve by following
the protocol (possibly the protocol with another client).  If $D$
accepts capabilities from other transactions outside of the protocol,
it can only cause other transactions in its own task to fail.

\begin{comment}
  If you can do something wrong and harm yourself that way, then this
  is called ``shooting yourself in your foot''.
  
  The destination task $D$ is welcome to shoot itself in its foot.
\end{comment}

\paragraph{The client $C$ and the destination task $D$ are malicious}

The final question we want to raise is what can happen if the client
$C$ and the destination task $D$ are malicious.  Can $C$ and $D$
cooperate and attacking $S$ in a way that $C$ or $D$ alone could not?

In the above analysis, there is no place where we assume any specific
behaviour of $D$ to help $S$ in preventing an attack on $S$.  There is
only one place where we make an assumption for $C$ in the analysis of
a malicious $D$.  If $D$ does not accept a reference container, we
said that $C$ would clean it up by calling
\verb/cap_ref_cont_destroy/.  So we have to look at what would happen
if $C$ were not to do that.

Luckily, we covered this case already.  It is identical to the case
where $C$ does not even tell $D$ about the reference container and
just do nothing.  In this case, as said before, the server will
eventually release the reference container when $C$ dies.  Before
that, it only occupies resources in the server that are associated
with $C$.

This analysis is sketchy in parts, but it covers a broad range of
possible attacks.  For example, all possible and relevant combinations
of task deaths and malicious tasks are covered.  Although by no means
complete, it can give us some confidence about the rightness of the
protocol.  It also provides a good set of test cases that you can test
your own protocols, and improvements to the above protocol against.



\subsection{Synchronous IPC}
  
The Hurd only needs synchronous IPC.  Asynchronous IPC is usually not
required.  An exception are notifications (see below).
  
There are possibly some places in the Hurd source code where
asynchronous IPC is assumed.  These must be replaced with different
strategies.  One example is the implementation of select() in the GNU
C library.

\begin{comment}
  A naive implementation would use one thread per capability to select
  on.  A better one would combine all capabilities implemented by the
  same server in one array and use one thread per server.
  
  A more complex scheme might let the server process select() calls
  asynchronously and report the result back via notifications.
\end{comment}

In other cases the Hurd receives the reply asynchronously from sending
the message.  This works fine in Mach, because send-once rights are
used as reply ports and Mach guarantees to deliver the reply message,
ignoring the kernel queue limit.  In L4, no messages are queued and
such places need to be rewritten in a different way (for example using
extra threads).


\subsection{Notifications}
  
Notifications to untrusted tasks happen frequently.  One case is
object death notifications, in particular task death notifications.
Other cases might be select() or notifications of changes to the
filesystem.

The console uses notifications to broadcast change events to the
console content, but it also uses shared memory to broadcast the
actual data, so not all notifications need to be received for
functional operation.  Still, at least one notification is queued by
Mach, and this is sufficient for the console to wakeup whenever
changes happened, even if the changes can not be processed
immediately.
  
From the servers point of view, notifications are simply messages with
a send and xfer timeout of 0 and without a receive phase.
  
For the client, however, there is only one way to ensure that it will
receive the notification: It must have the receiving thread in the
receive phase of an IPC.  While this thread is processing the
notification (even if it is only delegating), it might be preempted
and another (or the same) server might try to send a second
notification.
  
It is an open challenge how the client can ensure that it either
receives the notification or at least knows that it missed it, while
the server remains save from potential DoS attacks.  The usual
strategy, to give receivers of notifications a higher scheduling
priority than the sender, is not usable in a system with untrusted
receivers (like the Hurd).  The best strategy determined so far is to
have the servers retry to send the notification several times with
small delays inbetween.  This can increase the chance that a client is
able to receive the notification.  However, there is still the
question what a server can do if the client is not ready.
  
An alternative might be a global trusted notification server that runs
at a higher scheduling priority and records which servers have
notifications for which clients, and that can be used by clients to be
notified of pending notifications.  Then the clients can poll the
notifications from the servers.


\section{Threads and Tasks}
  
The Hurd will encode the task ID in the version part of the L4 thread
ID.  The version part can only be changed by the privileged system
code, so it is protected by the kernel.  This allows recipients of a
message to quickly determine the task from the sender's thread ID.

Task IDs will not be reused as long as there are still tasks that
might actively communicate with the (now destroyed) task.  Task info
capabilities provided by the task server can be used for that.  The
task info capability will also receive the task death notification (as
a normap capability death notification).  The task server will reuse a
task ID only when all task info capabilities for the task with that ID
have been released.

This of course can open a DoS attack.  Programs can attempt to acquire
task info capabilities and never release them.  Several strategies can
be applied to compensate that: The task server can automatically time
out task info capability references to dead tasks.  The proc server
can show dead task IDs with task info capability references as some
variant of zombie tasks, and provide a way to list all tasks
preventing the task ID from being reused, allowing the system
administrator to identify malicious or faulty users.  Task ID
references can be taken into account in quota restrictions, to
encourage a user to release them when they are not needed anymore (in
particular, a user holding a task ID reference to a dead task could be
punished with the same costs as for an additional normal task owned by
the user).  Another idea is to not allow any task to allocate more
task info capabilities than there are live tasks in the system, plus
some slack.  This provides a high incentive for tasks to release their
info caps (and if they get an error, they could block until their
notification system has processed the task death notification and
released the reference, and try again).

Access to task info capabilities can be open to everyone.  The above
strategies to prevent tasks from allocating too many of them for too
long work even if access to task info capabilities is given out
without any preconditions, and there is no real incentive other than
those above for a task to not pass on a task info capability to any
interested task anyway.  Allowing every task to create task info
capabilities for other tasks simplifies the protocols involved and
allows for some optimizations.


\subsection{Proxy Task Server}
\label{proxytaskserver}

The task server can be safely proxied, and the users of such a proxy
task server can use it like the real task server, even though
capabilities work a bit different for the task server than for other
servers.

The problem exists because the proxy task server would hold the real
task info capabilities for the task info capabilities that it provides
to the proxied task.  So if the proxy task server dies, all such task
info capabilities would be released, and the tasks using the proxy
task server would become insecure and open to attacks by imposters.

However, this is not really a problem, because the proxy task server
will also provide proxy objects for all task control capabilities.  So
it will be the only task which holds task control capabilities for the
tasks that use it.  When the proxy task server dies, all tasks that
were created with it will be destroyed when these tak control
capabilities are released.  The proxy task server is a vital system
component for the tasks that use it, just as the real task server is a
vital system component for the whole system.


\section{Virtual Memory Management}

Traditionally, monolithical kernels, but even kernels like Mach,
provide a virtual memory management system in the kernel.  All paging
decisions are made by the kernel itself.  This requires good
heuristics.  Smart paging decisions are often not possible because the
kernel lacks the information about how the data is used.

In the Hurd, paging will be done locally in each task.  A physical
memory server provides a number of guaranteed physical pages to tasks.
It will also provide a number of excess pages (over-commit).  The task
might have to return any number of excess pages on short notice.  If
the task does not comply, all mappings are revoked (essentially
killing the task).

A problem arises when data has to be exchanged between a client and a
server, and the server wants to have control over the content of the
pages (for example, pass it on to other servers, like device drivers).
The client can not map the pages directly into the servers address
space, as it is not trusted.  Container objects created in the
physical memory server and mapped into the client and/or the servers
address space will provide the necessary security features to allow
this.  This can be used for DMA and zero-copying in the data exchange
between device drivers and (untrusted) user tasks.


\section{Task Management}

A task server will provide the ability to create and destroy tasks and
threads, nd get some basic information about them.  The task server
might also server as the initial scheduler for simple usage statistics
(cpu time of a process), which is not otherwise provided by L4.  Of
course, other information like creation time of a process will also be
provided.

A proc server (which is logically different but might be implemented
in the same process as the task server) will provide POSIX process
semantics for tasks.  Registration with the proc server is optional.

An accounting ID that can be set by the proc server and is inherited
at task creation allows to kill a group of (from proc's point of view)
unregistered tasks at once.  This is also useful to prevent left-over
of child processes that are incapable of running with exec() (see
below).  The accounting ID will usually be set to the PID of a process
as soon as it registers itself with proc.

If the last reference to a task control capability is released, the
task should be destroyed and the task server should release all task
control and info capabilities it held.  This should happen
recursively, of course.  However, it is important that the task
control capabilities are released before the info capabilities (so
that tasks for which this tasked had the only control capability,
which relied on this task to hold info capabilities for them, are
killed and not attackable by an imposter).  This is important for
tasks creating new tasks (which have to talk to other tasks, for
example their parent, before they get their own control capability),
or for proxy task servers (which hold the task control and info
capabilities for all tasks they proxy).

Other operations, like starting and stopping threads in a task, can
not be supported by the task server, but have to be implemented in
locally in each task because of the minimality of L4.


\subsection{Exec}

The exec() operation will be done locally in a task.  Traditionally,
exec() overlays the same task with a new process image, because
creating a new task and transferring the associated state is
expensive.  In L4, only the threads and virtual memory mappings are
actually kernel state associated with a task, and exactly those have
to be destroyed by exec() anyway.  There is a lot of Hurd specific
state associated with a task (capabilities, for example), but it is
difficult to preserve that.  There are security concerns, because
POSIX programs do not know about Hurd features like capabilities, so
inheriting all capabilities across exec() seems dangerous.  There are
also implementation obstacles, because only local threads can
manipulate the virtual memory mappings, and there is a lot of local
state that has to be kept somewhere between the time the old program
becomes defunct and the new binary image is installed and used (not to
speak of the actual program snippet that runs during the transition).

So a decision was made to always create a new task with exec(), and
move the desired state over from the current task to the new task.
This is a clean solution, because a new task will always start out
without any capabilities in servers, etc, and thus there is no need
for the old task to try to destroy all unneeded capabilities and other
local state before exec().  Also, in case the exec fails, the old
program can continue to run, even if the exec fails at a very late
point (there is no ``point of no return'' until the new task is
actually up and running).

For suid/sgid applications, the actual exec has to be done by the
filesystem.  However, the filesystem can not be bothered to also
transfer all the user state into the new task.  It can not even do
that, because it can not accept capabilities implemented by untrusted
servers from the user.  Also, the filesystem does not want to rely on
the new task to be cooperative, because it does not necessarily trust
the code.  (This actually depends on if users are allowed to set the
suid/sgid flag on their own programs.  If not, then it might be ok for
the filesystem to trust the program, but it is assumed that the Hurd
will not be so restrictive).  Here is how it can be done.  Only the
suid/sgid case is provided, the other one is naturally easier but
comparable.

\begin{enumerate}
\item The user creates a new task and a container with a single
  physical page, and makes the exec() call to the file capability,
  providing the task control capability.  Before that, it creates a
  task info capability from it for its own use.
\item The filesystem checks permission and then revokes all other
  users on the task control capability.  This will revoke the users
  access to the task, and will fail if the user did not provide a
  pristine task object.  (It is assumed that the filesystem should not
  create the task itself so the user can not use suid/sgid
  applications to escape from their quota restriction).
\item Then it revokes access to the provided physical page and writes
  a trusted startup code to it.
\item The filesystem will also prepare all capability transactions and
  write the required information (together with other useful
  information) in a stack on the physical page.
\item Then it creates a thread in the task, and starts it.  At
  pagefault, it will provide the physical page.
\item The startup code on the physical page completes the capability
  transfer.  It will also install a small pager that can install file
  mappings for this binary image.  Then it jumps to the entry point.
\item The filesystem in the meanwhile has done all it can do to help
  the task startup.  It will provide the content of the binary or
  script via paging or file reads, but that happens asynchronously,
  and as for any other task.  So the filesystem returns to the client.
\item The client can then send its untrusted information to the new
  task.  The new task got the client's thread ID from the filesystem
  (possibly provided by the client), and thus knows to which thread it
  should listen.  The new task will not trust this information
  ultimatively (ie, the new task will use the authentication, root
  directory and other capabilities it got from the filesystem), but it
  will accept all capabilities and make proper use of them.
\item Then the new task will send a message to proc to take over the
  old PID and other process state.  How this can be done best is still
  to be determined (likely the old task will provide a process control
  capability to the new task).  At that moment, the old task is
  desrtoyed by the proc server.
\end{enumerate}

This is a coarse and incomplete description, but it shows the general
idea.  The details will depend a lot on the actual implementation.


\section{Authentication}
\label{auth}

The auth server gives out auth objects that contain zero or more of
effective user IDs, available user IDs, effective group IDs and
available group IDs.  New objects can be created from existing
objects, but only as subsets from the union of the IDs a user
possesses.  If an auth object has an effective or available user ID 0,
then arbitrary new auth objects can be created from that.

A passport can be created from an auth object that can be used by
everyone who possesses a handle to the passport object to verify the
IDs of the auth object that the passport was created from, and if the
auth object is owned by any particular task (normally the user
requesting the.

The auth server should always create new passport objects for
different tasks, even if the underlying auth object is the same, so
that a task having the passport capability can not spy on other tasks
unless they were given the passport object by that task.


\section{Unix Domain Sockets and Pipes}

In the Hurd on Mach, there was a global pflocal server that provided
unix domain sockets and pipes to all users.  This will not work very
well in the Hurd on L4, because for descriptor passing, read:
capability passing, the unix domain socket server needs to accept
capabilities in transit.  User capabilities are often implemented by
untrusted servers, though, and thus a global pflocal server running as
root can not accept them.

However, unix domain sockets and pipes can not be implemented locally
in the task.  An external task is needed to hold buffered data
capabilities in transit.  in theory, a new task could be used for
every pipe or unix domain socketpair.  However, in practice, one
server for each user would suffice and perform better.

This works, because access to Unix Domain Sockets is controlled via
the filesystem, and access to pipes is controlled via file
descriptors, usually by inheritance.  For example, if a fifo is
installed as a passive translator in the filesystem, the first user
accessing it will create a pipe in his pflocal server.  From then on,
an active translator must be installed in the node that redirects any
other users to the right pflocal server implementing this fifo.  This
is asymmetrical in that the first user to access a fifo will implement
it, and thus pay the costs for it.  But it does not seem to cause any
particular problems in implementing the POSIX semantics.

The GNU C library can contact ~/servers/socket/pflocal to implement
socketpair, or start a pflocal server for this task's exclusive use if
that node does not exist.

All this are optimizations: It should work to have one pflocal process
for each socketpair.  However, performance should be better with a
shared pflocal server, one per user.


\section{Filesystem Translators}

\label{xfslookup}

The Hurd has the ability to let users mount filesystems and other
servers providing a filesystem-like interface.  Such filesystem
servers are called translators.  In the Hurd on GNU Mach, the parent
filesystem would automatically start up such translators from passive
translator settings in the inode.  It would then block until the child
filesystem sends a message to its bootstrap port (provided by the
parent fs) with its root directory port.  This root directory port can
then be given to any client looking up the translated node.

There are several things wrong with this scheme, which becomes
apparent in the Hurd on L4.  The parent filesystem must be careful to
not block on creating the child filesystem task.  It must also be
careful to not block on receiving any acknowledgement or startup
message from it.  Furthermore, it can not accept the root directory
capability from the child filesystem and forward it to clients, as
they are potentially not trusted.

The latter problem can be solved the following way: The filesystem
knows about the server thread in the child filesystem.  It also
implements an authentication capability that represents the ability to
access the child filesystem.  This capability is also given to the
child filesystem at startup (or when it attaches itself to the parent
filesystem).  On client dir\_lookup, the parent filesystem can return
the server\_thread and the authentication capability to the client.
The client can use that to initiate a connection with the child
filesystem (by first building up a connection, then sending the
authentication capability from the parent filesystem, and receiving a
root directory capability in exchange).

\begin{comment}
  There is a race here.  If the child filesystem dies and the parent
  filesystem processes the task death notification and releases the
  task info cap for the child before the user acquires its own task
  info cap for the child, then an imposter might be able to pretend to
  be the child filesystem for the client.
  
  This race can only be avoided by a more complex protocol:
  
  Variant 1: The user has to acquire the task info cap for the child
  fs, and then it has to perform the lookup again.  If then the thread
  ID is for the task it got the task ID for in advance, it can go on.
  If not, it has to retry.  This is not so good because a directory
  lookup is usually an expensive operation.  However, it has the
  advantage of only slowing down the rare case.
  
  Variant 2: The client creates an empty reference container in the
  task server, which can then be used by the server to fill in a
  reference to the child's task ID.  However, the client has to create
  and destroy such a container for every filesystem where it excepts
  it could be redirected to another (that means: for all filesystems
  for which it does not use \verb/O_NOTRANS/).  This is quite an
  overhead to the common case.
\end{comment}

The actual creation of the child filesystem can be performed much like
a suid exec, just without any client to follow up with further
capabilities and startup info.  The only problem that remains is how
the parent filesystem can know which thread in the child filesystem
implements the initial handshake protocol for the clients to use.  The
only safe way here seems to be that the parent filesystem requires the
child to use the main thread for that, or that the parent filesystem
creates a second thread in the child at startup (passing its thread ID
in the startup data), requiring that this second thread is used.  In
either case the parent filesystem will know the thread ID in advance
because it created the thread in the first place.  This looks a bit
ugly, and violates good taste, so we might try to look for alternative
solutions.


\section{Debugging}

L4 does not support debugging.  So every task has to implement a debug
interface and implement debugging locally.  gdb needs to be changed to
make use of this interface.  How to perform the required
authentication, and how the debug thread is advertised to gdb, and how
the debug interface should look like, are all open questions.


\section{Scheduling}

The task server might implement an initial scheduler that just keeps
track of consumed CPU time, so we have some statistics.  Later, a
scheduler has to be written, that also can do SMP.  All of this is
still in the open.

There is no way to get at the ``system time'' in L4, it is assumed
that no time is spent in the kernel (which is mostly true).  So system
time will always be reported as 0.00, or 0.01.

\section{Device Drivers}

\subsection{Requirements}

  \begin{itemize}
  \item Performance: Speed is important!
  \item Portability: Framework should work on different architectures.
    
    Also: Useable in a not hurdisch environment with only
    small changes.

  \item Flexibility
  \item Convenient interfaces
  \item Consistency 
  \item Safety: driver failure should have as minimal system impact as
    possible.
  \end{itemize}

\subsection{Overview}

 The framework consists of: 
 \begin{itemize}
 \item Bus drivers
 \item Device drivers
 \item Service servers (plugin managers, $\omega_0$, rootserver)
 \end{itemize}

\subsubsection{Drivers and the filesystem}
  
  The device driver framework will only offer a physical device view.
  Ie. it will be a tree with devices as the leaves connected by
  various bus technologies.  Any logical view and naming persistence
  will have to be build on top of this (translator).

\subsubsection{Layer of the drivers}

  The device driver framework consists only of the lower level drivers
  and doesn't need to have a complicated scheme for access control.
  This is because it should be possible to share devices, e.g. for
  neighbour Hurd.  The authentication is done by installing a virtual
  driver in each OS/neighour Hurd.  The driver framework trusts these
  virtual drivers.  So it's possible for a non Hurdish system to use
  the driver framework just by implementing these virtual drivers.
  
  Only threads which have registered as trusted are allowed to access
  device drivers.  The check is simply done by checking the senders
  ID against a table of known threads.

\subsubsection{Address spaces}

  Drivers always reside in their own AS. The overhead for cross AS IPC
  is small enough to do so.

\subsubsection{Zero copying and DMA}
  
  It is assumed that there are no differences between physical memory
  pages. For example each physical memory page can be used for DMA
  transfers. Of course, older hardware like ISA devices can so not be
  supported. Who cares?
  
  With this assumption, the device driver framework can be given any
  physical memory page for DMA operation.  This physical memory page
  must be pinned down.
  
  If an application wants to send or receive data to/from a device
  driver it has to tell the virtual driver the page on which the
  operation has to be executed.  Since the application doesn't know
  the virtual-real memory mapping, it has to ask the physical memory
  manager for the real memory address of the page in question.  If the
  page is not directly mapped from the physical memory manager the
  application ask the mapper (another application which has mapped
  this memory region the first application) to resolve the mapping.
  This can be done recursively.  Normally, this resolving of mapping
  can be speed up using a cache services, since a small number of
  pages are reused very often.
  
  With the scheme, the drivers do not have to take special care of
  zero copying if there is only one virtual driver.  When there is
  more than one virtual driver pages have to copied for all other
  virtual drivers.

\subsubsection{Root bus driver}
  
  The root bus is the entrypoint to look up devices.
  
  XXX There should be iterators/visitors for operating on
  busses/devices.  (daniel)

\subsubsection{Physical versus logical device view}
  
  The device driver framework will only offer a physical device view.
  Ie. it will be a tree with devices as the leaves connected by
  various bus technologies.  Any logical view and naming persistence
  will have to be build on top of this (translator).

\subsubsection{Things for the future}

  \begin{itemize}
  \item Interaction with the task server (e.g. listings driver threads 
    with ps,etc.)
  \item Powermanagement
  \end{itemize}

\subsection{Bus Drivers}

A bus driver is responsible to manage the bus and provide access to
devices connected to it.  In practice it means a bus driver has to
perform the following tasks:

\begin{itemize}
\item Handle hotplug events
  
  Busses which do not support hotplugging, will treated as if there is
  1 insertion event for every device connected to it when the bus
  driver is started.  Drivers which don't support autoprobing of
  devices will probably have to read some configuration data from a
  file or if the driver is a needed for bootstrapping configuration
  can be given as argument on its stack.  In some cases the bus
  doesn't generate insertion/removal events, but can still support
  some form of hotplug functionality if the user tells the driver when
  a change to the bus configuration has happened (eg. SCSI).

\item Configure client device drivers
    
  The bus driver should start the appropriate client device driver
  translator when an insertion event is detected.  It should also
  provide the client device driver with all necessary configuration
  info, so it can access the device it needs.  This configuration data
  typically consists of the bus addresses of the device and possibly
  IRQ numbers or DMA channel ID's.  The device driver is loaded by the
  assotiatet plugin manager.

\item Provide access to devices
  
  This means the bus driver should be able to perform a bus
  transaction on behalf of a client device driver.  In some cases this
  involves sending a message and waiting for reply (eg. SCSI, USB,
  IEEE 1394, Fibre Channel,...).  The driver should provide
  send/receive message primitives in this case.  In other cases
  devices on the bus can be accessed by doing a memory accesses or by
  using special I/O instructions.  In this case the driver should
  provide mapping and unmapping primitives so a client device driver
  can get access to the memory range or is allowed to access the I/O
  addresses.  The client device driver should use a library, which is
  bus dependant, to access the device on the bus.  This library hides
  the platform specific details of accessing the bus.
  
  Furthermore the bus driver must also support rescans for hardware.
  It might be that not all drivers are found during bootstrapping and
  hence later on drivers could be loaded.  This is done by regenerate
  new attach notification sending to bus's plugin manager.  The plugin
  manager loads then if possible a new driver.  A probe funtion is not
  needed since all supported hardware can be identified by
  vendor/device identifactions (unlike ISA hardware).  For hardware
  busses which don't support such identifaction (ISA) only static
  configuration is possible (configuration scripts etc.)
\end{itemize}


\subsubsection{Plugin Manager}

  Each bus driver has a handle/reference to which insert/remove events
  are send.  The owner of the handle/refence must then take
  appropriate action like loading the drivers.  These actors are
  called plugin managers.

\subsubsection{Generic Bus Driver}

  Operations:
  \begin{itemize}
  \item notify (attach, detach)
  \item string enumerate
  \end{itemize}
  
  XXX Extract generic bus services from the PCI Bus Driver section
  which could be also be used other PCI related busses (ISA) be used.
  The name for this service is missleading, since a SCSI Bus Driver
  does not have anything in common with a PCI bus.  (daniel)

\subsubsection{ISA Bus Driver}
Inherits from:

\begin{itemize}
\item Generic Bus Driver
\end{itemize}

Operations:
\begin{itemize}
\item (none)
\end{itemize}

XXX The interface has not been defined up to now. (daniel)


\subsubsection{PCI Bus Driver}

Inherits from:
\begin{itemize}
\item Generic Bus Driver
\end{itemize}

Operations:
\begin{itemize}
\item map\_mmio: map a PCI BAR for MMIO
\item map\_io: map a PCI BAR for I/O
\item map\_mem: map a PCI BAR for memory
\item read\_mmio\_{8,16,32,64}: read from a MMIO register
\item write\_mmio\_{8,16,32,64}: write to a MMIO register
\item read\_io\_{8,16,32,64}: read from an IO register
\item write\_io\_{8,16,32,64}: write to an IO register
\item read\_config\_{8,16,32,?}: read from a PCI config register
\item write\_config\_{8,16,32,?}: write to a PCI config register
\item alloc\_dma\_mem(for non zero copying): allocate main memory useable for DMA
\item free\_dma\_mem  (for non zero copying): free main memory useable for DMA
\item prepare\_dma\_read: write back CPU cachelines for DMAable memory area
\item sync\_dma\_write: discard CPU cachelines for DMAable memory area
\item alloc\_consistent\_mem: allocate memory which is consistent between CPU 
  and device
\item free\_consistent\_mem: free memory which
  is consistent between CPU and device
\item get\_irq\_mapping (A,B,C,D): get the IRQ matching the INT(A,B,C,D) line
\end{itemize}

\subsection{Device Drivers}
\subsubsection{Classes}
\begin{itemize}
\item character: This the standard tty as known in the Unix environment.
\item block
\item human input: Keyboard, mouse, ...
\item packet switched network
\item circuit switched network
\item framebuffer
\item streaming audio
\item streaming video
\item solid state storage: flash memory
\end{itemize}

\subsubsection{Human input devices (HID) and the console}

The HIDs and the console are critical for user interaction with the
system.  Furthmore, the console should be working as soons as possible
to give feedback.  Log messages which are send to the console before
the hardware has been initialized should be buffered.

\subsubsection{Generic Device Driver}
Operations:
\begin{itemize}
\item init : prepare hardware for use
\item start : start normal operation
\item stop : stop normal operation
\item deinit : shutdown hardware
\item change\_irq\_peer : change peer thread to propagate irq message to.
\end{itemize}


\subsubsection{ISA Devices}
Inherits from:
\begin{itemize}
\item Generic Device Driver
\end{itemize}

Supported devices
\begin{itemize}
\item Keyboard (ps2)
\item serial port (mainly for debugging purposses)
\item parallel port
\end{itemize}

XXX interface definition for each device driver is missing. (daniel)


\subsubsection{PCI Devices}
Inherits from:
\begin{itemize}
\item Generic Device Driver
\end{itemize}
  
Supported devices:
\begin{itemize}
\item block devices
\item ...
\end{itemize}

XXX interface definition for each device driver is missing. (daniel)


\subsection{Resource Management}


\subsubsection{IRQ handling}

\paragraph{IRQ based interrupt vectors}

Some CPU architectures (eg 68k, IA32) can directly jump to an
interrupt vector depending on the IRQ number. This is typically the
case on CISC CPU's. In this case there is some priorization scheme. On
IA32 for example, the lowest IRQ number has the highest priority.
Sometimes the priorities are programmable.  Most RISC CPU's have only
a few interrupt vectors which are connected external IRQs. (typically
1 or 2). This means the IRQ handler should read a register in the
interrupt controller to determine which IRQ handler has to be
executed.  Sometimes the hardware assists here by providing a register
which indicates the highest priority interrupt according to some
(programmable) scheme.

\paragraph{IRQ acknowlegdement}

The IRQ acknowledgement is done in two steps. First inform the
hardware about the successful IRQ acceptance. Then inform the ISRs
about the IRQ event.

\paragraph{Edge versus level triggered IRQs}

Edge triggered IRQs typically don't need explicit acknowledgment by
the CPU at the device level. You can just acknowledge them at the
interrupt controller level.  Level triggered IRQs typically need to
explicitly acknowledged by the CPU at the device level. The CPU has to
read or write a register from the IRQ generating peripheral to make
the IRQ go away. If this is not done, the IRQ handler will be
reentered immediatly after it ended, effectively creating an endless
loop. Another way of preventing this would be to mask the IRQ.

\paragraph{Multiple interrupt controllers}

Some systems have multiple interrupt controllers in cascade. This is
for example the case on a PC, where you have 2 8259 interrupt
controllers. The second controller is connected to the IRQ 2 pin of
the first controller. It is also common in non PC systems which still
use some standard PC components such as a Super IO controller. In this
case the 2 8259's are connected to 1 pin of the primary interrupt
controller. Important for the software here is that you need to
acknowledge IRQ's at each controller. So to acknowledge an IRQ from
the second 8259 connected to the first 8259 connected to another
interrupt controller, you have to give an ACK command to each of those
controllers.  Another import fact is that on PC architecture the order
of the ACKs is important.

\paragraph{Shared IRQs}

Some systems have shared IRQs. In this case the IRQ handler has to
look at all devices using the same IRQ...

\paragraph{IRQ priorities}

All IRQs on L4 have priorities, so if an IRQ occurs any IRQ lower then
the first IRQ will be blocked until the first IRQ has been
acknowlegded.  ISR priorities must much the hardware priority (danger
of priority inversion).  Furthermore the IRQ acknowledgment order is
important.

The 8259 also supports a specific IRQ acknowledge iirc. But, this
scheme does not work in most level triggered IRQ environments. In
these environments you must acknowledge (or mask) the IRQ before
leaving the IRQ handler, otherwise the CPU will immediately reenter
the IRQ handler, effectively creating an endless loop. In this case L4
would have to mask the IRQ. The IRQ thread would have to unmask it
after acknowledgement and processing.

\paragraph{IRQ handling by L4/x86}

The L4 kernel does handle IRQ acknowlegdment. 

 
\subsubsection{$\omega_0$}

$\omega_0$ is a system-central IRQ-logic server. It runs in the
privileged AS space in order to be allowed rerouting IRQ IPC.

If an IRQ is shared between several devices, the drivers are daisy
chained and have to notify their peers if an IRQ IPC has arrived.

XXX For more detail see XXX URL missing

Operations:
\begin{itemize}
\item attach\_irq : attach an ISR thread to the IRQ 
\item detach\_irq : detach an ISR thread form the IRQ
\end{itemize}


\subsubsection{Memory}
If no physical memory pages are provided by the OS the device driver
framework alloces pages from the physical memory manager.  The device
driver framework has at no point of time to handle any virtual to
physical page mapping.


\subsection{Bootstrapping}

A simpleFS provides initial drivers for bootstraping.  The root bus
driver and simpleFS is loaded by grub as module.  It then signals for
loading new (bus) drivers.  As before if there is no driver avaible
for some reason for the device, the bus driver doesn't change the
device state and waits for a notifaction that there are new drivers
avaible. This simpleFS might be based on BSD libstand (library for
standalone applications).  simpleFS doesn't need to be writeable
either.


\subsubsection{Plugin Manager}
A Plugin manager handles driver loading for devices.  It searches for
driver in seach pathes (on filesystems).  It's possible to add new
search pathes later.  This allows the system to bootstrap with only
one search path (the simpleFS).  When the search path is changed, the
device tree will be scanned for devices which don't have a driver
loaded yet.  If a driver has become available, it will be loaded.


\subsection{Order of implementation}

\begin{enumerate}
\item rootserver, plugin server
\item root bus server
\item pci bus
\item isa bus
\item serial port  (isa bus)
\item console 
\end{enumerate}


\end{document}
