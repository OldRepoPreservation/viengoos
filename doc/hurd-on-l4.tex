\documentclass[9pt,a4paper]{extarticle}
%\usepackage{german}
%\usepackage[margin=2.5cm]{geometry}

\newenvironment{comment}{\footnotesize \begin{quote}}{\end{quote}}

\title{Porting the GNU Hurd to the L4 Micorkernel}
\author{Marcus Brinkmann, Peter De Schrijver, Daniel Wagner}
\date{August 2003}

\begin{document}
\maketitle


\section{Introduction}

The GNU Hurd is a multi-server operating system running on top of a
microkernel (currently Mach variants).  The core motivation of the
Hurd is the following:

\begin{quote}
  \emph{The operating system should enable its users to share the
    resources of the system without harming each other.}
\end{quote}

The focus is on the user, the system should try to allow the user to
do anything that is not harmful for other users.  Many operating
systems either restrict what the user can do to be more secure, while
others allow the user to do everything, but fail on protecting the
users from each other effectively.

The Hurd is designed to minimize the system code that the user is
required to use, while allowing the user to use, ignore or replace the
remaining system code, and this without harming other users.

So while the L4 microkernel tries to minimize the policy that the
kernel enforces on the software running on it, the Hurd tries to
minimize the policy that the operating system enforces on its users.
Furthermore, the Hurd also aims to provide a POSIX compatible general
purpose operating system.  However, this POSIX personality of the Hurd
is provided for convenience only, and to make the Hurd useful.  Other
personalities can be implemented and used by the users of the system
along with the POSIX personality.  This default personality of the
Hurd also provides some convenient features that allow the user to
extend the system so that all POSIX compatible programs can take
advantage of it.

These notes are a moving target in the effort to find the best
strategy to port the Hurd to the L4 microkernel.

\begin{comment}
  Remarks about the history of a certain feature and implementation
  details are set in a smaller font and separated from the main text,
  just like this paragraph.  Because this is work in progress, there
  are naturally a lot of such comments.
\end{comment}


\section{Booting}

A multiboot-compliant bootloader, for example GRUB, loads the loader
program \texttt{laden}, the kernel, $\sigma_0$, the rootserver and
further modules.  The loader is started, patches the kernel interface
page, and starts the kernel.  The kernel starts $\sigma_0$ and the
rootserver.  The rootserver has to deal with the other modules.


\subsection{The loader \texttt{laden}}

\texttt{laden} is a multiboot compliant kernel from the perspective of
GRUB.  It expects at least three modules.  The first module is the L4
kernel image, the second module is the $\sigma_0$ server image, and
the third module is the rootserver image.

\begin{comment}
  Later, the L4 kernel will support the optional UTCB paging server
  $\sigma_1$, which has to be treated like the other initial servers
  by \texttt{laden}.  A command line option to \texttt{laden} will
  allow the user to specify if the third module is the rootserver or
  $\sigma_1$.  If $\sigma_1$ is used, the rootserver is the fourth
  module in the list.
\end{comment}

\texttt{laden} copies (or moves) the three executable images to the
right location in memory, according to their respective ELF headers.
It also initializes the BSS section to zero.

\begin{comment}
  Laden has to deal with overlapping source and destination memory
  areas in an intelligent way.  It currently will detect such
  situations, but is not always able to find a solution, even if one
  exists.
\end{comment}

Then it searches for the kernel interface page (KIP) in the L4 kernel
image and modifies it in the following way:

\begin{itemize}
\item The memory descriptors are filled in according to the memory
  layout of the system.  On ia32, this information is -- at least
  partially -- provided by GRUB.

  \begin{comment}
    GRUB seems to omit information about the memory that is shared
    with the VGA card.  \texttt{laden} creates a special entry for
    that region, overriding any previous memory descriptor.
  \end{comment}
  
\item The start and end addresses and the entry point of the initial
  servers are filled in.

  \begin{comment}
    A future version of L4 should support adding information about the
    UTCB area of the initial rootserver as well.  Until then, the
    rootserver has no clean way to create a new thread (a hack is used
    by the rootserver to calculate the UTCB addresses for other
    threads).
  \end{comment}

\item The \verb/boot_info/ field is initialized.

  \begin{comment}
    The \verb/boot_info/ field is currently set to the GRUB
    \verb/multiboot_info/ structure.  This only works for the ia32
    architecture of course.  We might want to have a more architecture
    independent way to pass the information about further modules to
    the rootserver.  We also might want to gather the information
    provided by GRUB in a single page (if it isn't).
  \end{comment}
\end{itemize}


\subsection{The L4 kernel}

The L4 kernel initializes itself and then creates the address spaces
and threads for the initial servers $\sigma_0$ and the rootserver.  It
maps all physical memory idempotently into $sigma_0$, and sets the
pager of the rootserver thread to $sigma_0$.  Then it starts the
initial servers.


\subsection{The initial server $\sigma_0$}

$\sigma_0$ acts as the pager for the rootserver, answering page fault
messages by mapping the page at the fault address idempotently in the
rootserver.

\begin{comment}
  $\sigma_0$ can also be used directly by sending messages to it,
  according to the $sigma_0$ RPC protocol.  This is used by the kernel
  to allocate reserved memory, but can also be used by the user to
  explicitely allocate more memory than single pages indirectly via
  page faults.
\end{comment}

The thread ID of $\sigma_0$ is (\verb/UserBase, 1)/.

\begin{comment}
  We will write all thread IDs in the form (\verb/thread nr/,
  \verb/version/).
\end{comment}

Any fpage will only be provided to one thread.  $\sigma_0$ will return
an error if another thread attempts to map or manipulate an fpage that
has already been given to some other thread, even if both threads
reside in the same address space.


\subsection{The initial server $\sigma_1$}

$\sigma_1$ is intended to provide a paging service for UTCB memory.
This will allow orthogonal persistence to be implemented.  It is not
yet supported.

The thread ID of $\sigma_1$ is (\verb/UserBase + 1, 1)/.


\subsection{The rootserver}

The rootserver is the only task in the system which threads can
perform privileged system calls.  So the rootserver must provide
wrappers for the system calls to other unprivileged system tasks.

\begin{comment}
  For this, a simple authentication scheme is required.  The
  rootserver can keep a small, statically allocated table of threads
  which are granted access to the system call wrappers.  The caller
  could provide the index in the table for fast O(1) lookup instead
  linear search.  Threads with access could be allowed to add other
  threads or change existing table entries.  The same scheme can be
  used int the device driver framework.
  
  The rootserver should have one thread per CPU, and run at a high
  priority.
\end{comment}

The rootserver has the following initial state:

\begin{itemize}
\item Its thread ID is (\verb/UserBase + 2/, 1).

\item The priority is set to the 255, the maximum value.

\item The instruction pointer \verb/%eip/ is set to the entry point,
all other registers are undefined (including the stack pointer).

\item The pager is set to $\sigma_0$.
  
\item The exception handler set to \verb/nilthread/.
  
\item The scheduler is set to the rootserver thread itself.
\end{itemize}

So the first thing the rootserver has to do is to set up a simple
stack.

Then the rootserver should evaluate the \verb/boot_info/ field in the
KIP to find the information about the other modules.  It should parse
the information and create the desired initial tasks of the operating
system.  The Hurd uses a boot script syntax to allow to pass
information about other initial tasks and the root tasks to each
initial task in a generalized manner.

\begin{comment}
  The exact number and type of initial tasks necessary
  to boot the Hurd are not yet known.  Chances are that this list
  includes the task server, the physical memory server, the device
  servers, and the boot filesystem.
\end{comment}


\section{IPC}

The Hurd requires a capability system.  The current L4 specification
supports the notion of a redirector, that can be set for a task by the
privileged threads and forces all IPC through a different thread that
can then define the policy for IPC.

This adds one addition IPC to each RPC.  Furthermore, it makes
accounting the cost for managing capabilities difficult.  It also
keeps the IPC policy in system code which is imposed on the user.

The goal is to define and implement a capability system locally in
each task, and without requiring mutual trust.  

One difficulty is that in L4, IPC is always from thread to thread.
Thread identifiers are global and can be reused.  So programs must be
careful not to send any sensitive data to the wrong thread.

\subsection{IPC Implementation Roadmap}

\subsection{Threads and Tasks}
  
The Hurd will encode the task ID in the version part of the L4 thread
ID.  The version part can only be changed by the privileged system
code, so it is protected by the kernel.  This allows recipients of a
message to quickly determine the task from the sender's thread ID.

Task IDs will not be reused as long as there are still tasks that
might actively communicate with the (now destroyed) task.  Task info
capabilities provided by the task server can be used for that.  The
task info capability will also receive the task death notification (as
a normap capability death notification).  The task server will reuse a
task ID only when all task info capabilities for the task with that ID
have been released.

This of course can open a DoS attack.  Programs can attempt to acquire
task info capabilities and never release them.  Several strategies can
be applied to compensate that: The task server can automatically time
out task info capability references to dead tasks.  The proc server
can show dead task IDs with task info capability references as some
variant of zombie tasks, and provide a way to list all tasks
preventing the task ID from being reused, allowing the system
administrator to identify malicious or faulty users.  Task ID
references can be taken into account in quota restrictions, to
encourage a user to release them when they are not needed anymore (in
particular, a user holding a task ID reference to a dead task could be
punished with the same costs as for an additional normal task owned by
the user).  Another idea is to not allow any task to allocate more
task info capabilities than there are live tasks in the system, plus
some slack.  This provides a high incentive for tasks to release their
info caps (and if they get an error, they could block until their
notification system has processed the task death notification and
released the reference, and try again).

Access to task info capabilities can be open to everyone.  The above
strategies to prevent tasks from allocating too many of them for too
long work even if access to task info capabilities is given out
without any preconditions, and there is no real incentive other than
those above for a task to not pass on a task info capability to any
interested task anyway.  Allowing every task to create task info
capabilities for other tasks simplifies the protocols involved and
allows for some optimizations.


\subsection{Synchronous IPC}
  
The Hurd only needs synchronous IPC.  Asynchronous IPC is usually not
required.  An exception are notifications (see below).
  
There are possibly some places in the Hurd source code where
asynchronous IPC is assumed.  These must be replaced with different
strategies.  One example is the implementation of select() in the GNU
C library.
  
In other cases the Hurd receives the reply asynchronously from sending
the message.  This works fine in Mach, because send-once rights are
used as reply ports and Mach guarantees to deliver the reply message,
ignoring the kernel queue limit.  In L4, no messages are queued and
such places need to be rewritten in a different way (for example using
extra threads).

\subsection{Notifications}
  
Notifications to untrusted tasks happens frequently.  One case is
object death notifications, in particular task death notifications.
Other cases might be select() or notifications of changes to the
filesystem.
  
The console uses notifications to broadcast change events to the
console content, but it also uses shared memory to broadcast the
actual data, so not all notifications need to be received for
functional operation.  Still, at least one notification is queued by
Mach, and this is sufficient for the console to wakeup whenever
changes happened, even if the changes can not be processed
immediately.
  
From the servers point of view, notifications are simply messages with
a send and xfer timeout of 0 and without a receive phase.
  
For the client, however, there is only one way to ensure that it will
receive the notification: It must have the receiving thread in the
receive phase of an IPC.  While this thread is processing the
notification (even if it is only delegating), it might be preempted
and another (or the same) server might try to send a second
notification.
  
It is an open challenge how the client can ensure that it either
receives the notification or at least knows that it missed it, while
the server remains save from potential DoS attacks.  The usual
strategy, to give receivers of notifications a higher scheduling
priority than the sender, is not usable in a system with untrusted
receivers (like the Hurd).  The best strategy determined so far is to
have the servers retry to send the notification several times with
small delays inbetween.  This can increase the chance that a client is
able to receive the notification.  However, there is still the
question what a server can do if the client is not ready.
  
An alternative might be a global trusted notification server that runs
at a higher scheduling priority and records which servers have
notifications for which clients, and that can be used by clients to be
notified of pending notifications.  Then the clients can poll the
notifications from the servers.

The whole issue of notifications requires more thoughtful analysis.

\subsection{Capabilities}
  
Capabilities will be the building stones of the Hurd system.  Servers
will provide capabilities to clients.  Clients can invoke messages on
the capabilities, which are then processed by the server providing the
capability.  A capability will be normally associated with an object
at the server side (for example an opened file).

The low level interface will use client capability IDs that are local
to each client.  If a client gets the same capability through two
different ways at the same time, well-behaving servers will provide
the same local ID both times.  This allows a client to compare local
IDs numerically to establish identity within capabilities provided by
a single server..

Clients will be able to copy capabilities to other tasks.  This will
be possible without requiring mutual trust between the clients and the
server (the only trust requirements are that the clients trust the
server and that the sender of a capability trusts the receiver).
  
The straightforward protocol to move a capability from one client C1
to another client C2 is that the client C1 sends a request to the
server S to create a transitional object, a reference container
destined for client C2.  After receiving the identifier for this
object, client C1 sends the information about it to C2.  C2 can then
send a request to the server S to complete the transition, and reply
to C1 to allow it to synchronize with the completion of the operation
and destroy the transitional object.
  
There are some obvious and not-so-obvious properties of this basic
protocol.  For example, C1 can not hide references to objects in other
tasks, because the receiver has to explicitely accept the reference.
If C1 were to die before C2 can accept the reference (or if C2 does
reject to accept the handle), the server S would destroy the
transitional object.  On the other hand, C1 does not need to rely on
C2 to accept the reference, as it will always destroy it afterwards.
  
This basic protocol is not enough to provide secure capability
transfer on L4, though, as at any time a participant could die, and
there is the danger of another task reusing that participants task and
thread IDs.  Such an imposter can then gain access to capabilities it
would normally not allowed to get.  To prevent this, task info
capabilities have to be acquired by all participants to ensure that
the task IDs of the others are not reused in the whole process.  This
greatly increases the complexity of the protocol.
  
The exact syntax of such a protocol depend on the actual interfaces.
But here is a rough overview.  The starting condition is that C1 has a
capability implemented in S, and a capability implemented in C2.  C1
will send the capability implemented in S as part of a message invoked
on the capability implemented in C2.  Because C1 and S, as well as C1
and C2, are already communicating, C1 has task info capabilities for S
and C2, S has a task info capability for C1, and C2 has a task info
capability for S.

\begin{enumerate}
\item C1 sends a request to S to create a transitional object
  (reference container) destined for C2.
\item Before replying, S acquires a task info capability for C2 if it
  doesn't have any already.  It also must check if C1 is still alive
  after that (this can be done by the task server along with creating
  the task info capability), before entering the container into its
  data structures.  This prevents that an imposter (of C2) can acquire
  the capability by guessing the reference container ID before the
  server can receive and process the task death notifications for C1.
\item Then the server replies to C1 with the reference container ID.
  The task info capability will stay with the reference container, and
  both will be associated with C1.  If C1 dies now before C2 accepts
  the capability, both the reference and the task info capability will
  be destroyed.
\item C1 sends a request to C2 with the reference container ID and
  other necessary information (like the server thread ID).
\item C2 looks at the server thread ID, and if it wants to accept a
  capability implemented by this server (in other words: if it trusts
  that server), it acquires a task info capability for the server task
  if it doesn't have any already.  It then must check if C1 is still
  alive (this can be done by the task server along with creating the
  task info capability), because otherwise there might already be an
  imposter (of S).
\item Now C2 can send a request to S to accept the capability from C1.
\item The server will check that there is a reference container for C2
  provided by C1.  It will then install the reference as a proper
  reference for the capability owned by C2.  For this, it will also
  install the task info capability.  If now C1 dies, the reference
  container will be empty and C2 will keep its capability reference.
\item The server replies to C2, returning the capability ID for C2.
\item C2 can now return to C1, indicating success.
\item C1 can now destroy the transitional objectq, and optionally
  deallocate its own reference to the capability.
\end{enumerate}
  
Each step is necessary, and the order is peculiar.  If various things
go wrong, all behaving participants in this protocol can properly
clean up their state and resources without being harmed or tricked
into trusting a task they don't want to trust.

There will be other protocols, for example to return new capabilities
to the same server in a server reply (which is easy to do), and to
receive capabilities implemented by other servers from a server (which
can be done by creating empty reference containers in the client
before sending the request).


\section{Virtual Memory Management}

Traditionally, monolithical kernels, but even kernels like Mach,
provide a virtual memory management system in the kernel.  All paging
decisions are made by the kernel itself.  This requires good
heuristics.  Smart paging decisions are often not possible because the
kernel lacks the information about how the data is used.

In the Hurd, paging will be done locally in each task.  A physical
memory server provides a number of guaranteed physical pages to tasks.
It will also provide a number of excess pages (over-commit).  The task
might have to return any number of excess pages on short notice.  If
the task does not comply, all mappings are revoked (essentially
killing the task).

A problem arises when data has to be exchanged between a client and a
server, and the server wants to have control over the content of the
pages (for example, pass it on to other servers, like device drivers).
The client can not map the pages directly into the servers address
space, as it is not trusted.  Container objects created in the
physical memory server and mapped into the client and/or the servers
address space will provide the necessary security features to allow
this.  This can be used for DMA and zero-copying in the data exchange
between device drivers and (untrusted) user tasks.


\section{Task Management}

A task server will provide the ability to create and destroy tasks and
threads, nd get some basic information about them.  The task server
might also server as the initial scheduler for simple usage statistics
(cpu time of a process), which is not otherwise provided by L4.  Of
course, other information like creation time of a process will also be
provided.

A proc server (which is logically different but might be implemented
in the same process as the task server) will provide POSIX process
semantics for tasks.  Registration with the proc server is optional.

An accounting ID that can be set by the proc server and is inherited
at task creation allows to kill a group of (from proc's point of view)
unregistered tasks at once.  This is also useful to prevent left-over
of child processes that are incapable of running with exec() (see
below).  The accounting ID will usually be set to the PID of a process
as soon as it registers itself with proc.

If the last reference to a task control capability is released, the
task should be destroyed and the task server should release all task
control and info capabilities it held.  This should happen
recursively, of course.  However, it is important that the task
control capabilities are released before the info capabilities (so
that tasks for which this tasked had the only control capability,
which relied on this task to hold info capabilities for them, are
killed and not attackable by an imposter).  This is important for
tasks creating new tasks (which have to talk to other tasks, for
example their parent, before they get their own control capability),
or for proxy task servers (which hold the task control and info
capabilities for all tasks they proxy).

Other operations, like starting and stopping threads in a task, can
not be supported by the task server, but have to be implemented in
locally in each task because of the minimality of L4.


\subsection{Exec}

The exec() operation will be done locally in a task.  Traditionally,
exec() overlays the same task with a new process image, because
creating a new task and transferring the associated state is
expensive.  In L4, only the threads and virtual memory mappings are
actually kernel state associated with a task, and exactly those have
to be destroyed by exec() anyway.  There is a lot of Hurd specific
state associated with a task (capabilities, for example), but it is
difficult to preserve that.  There are security concerns, because
POSIX programs don't know about Hurd features like capabilities, so
inheriting all capabilities across exec() seems dangerous.  There are
also implementation obstacles, because only local threads can
manipulate the virtual memory mappings, and there is a lot of local
state that has to be kept somewhere between the time the old program
becomes defunct and the new binary image is installed and used (not to
speak of the actual program snippet that runs during the transition).

So a decision was made to always create a new task with exec(), and
move the desired state over from the current task to the new task.
This is a clean solution, because a new task will always start out
without any capabilities in servers, etc, and thus there is no need
for the old task to try to destroy all unneeded capabilities and other
local state before exec().  Also, in case the exec fails, the old
program can continue to run, even if the exec fails at a very late
point (there is no ``point of no return'' until the new task is
actually up and running).

For suid/sgid applications, the actual exec has to be done by the
filesystem.  However, the filesystem can not be bothered to also
transfer all the user state into the new task.  It can not even do
that, because it can not accept capabilities implemented by untrusted
servers from the user.  Also, the filesystem does not want to rely on
the new task to be cooperative, because it does not necessarily trust
the code.  (This actually depends on if users are allowed to set the
suid/sgid flag on their own programs.  If not, then it might be ok for
the filesystem to trust the program, but it is assumed that the Hurd
will not be so restrictive).  Here is how it can be done.  Only the
suid/sgid case is provided, the other one is naturally easier but
comparable.

\begin{enumerate}
\item The user creates a new task and a container with a single
  physical page, and makes the exec() call to the file capability,
  providing the task control capability.  Before that, it creates a
  task info capability from it for its own use.
\item The filesystem checks permission and then revokes all other
  users on the task control capability.  This will revoke the users
  access to the task, and will fail if the user did not provide a
  pristine task object.  (It is assumed that the filesystem should not
  create the task itself so the user can not use suid/sgid
  applications to escape from their quota restriction).
\item Then it revokes access to the provided physical page and writes
  a trusted startup code to it.
\item The filesystem will also prepare all capability transactions and
  write the required information (together with other useful
  information) in a stack on the physical page.
\item Then it creates a thread in the task, and starts it.  At
  pagefault, it will provide the physical page.
\item The startup code on the physical page completes the capability
  transfer.  It will also install a small pager that can install file
  mappings for this binary image.  Then it jumps to the entry point.
\item The filesystem in the meanwhile has done all it can do to help
  the task startup.  It will provide the content of the binary or
  script via paging or file reads, but that happens asynchronously,
  and as for any other task.  So the filesystem returns to the client.
\item The client can then send its untrusted information to the new
  task.  The new task got the client's thread ID from the filesystem
  (possibly provided by the client), and thus knows to which thread it
  should listen.  The new task will not trust this information
  ultimatively (ie, the new task will use the authentication, root
  directory and other capabilities it got from the filesystem), but it
  will accept all capabilities and make proper use of them.
\item Then the new task will send a message to proc to take over the
  old PID and other process state.  How this can be done best is still
  to be determined (likely the old task will provide a process control
  capability to the new task).  At that moment, the old task is
  desrtoyed by the proc server.
\end{enumerate}

This is a coarse and incomplete description, but it shows the general
idea.  The details will depend a lot on the actual implementation.


\section{Authentication}

The auth server gives out auth objects that contain zero or more of
effective user IDs, available user IDs, effective group IDs and
available group IDs.  New objects can be created from existing
objects, but only as subsets from the union of the IDs a user
possesses.  If an auth object has an effective or available user ID 0,
then arbitrary new auth objects can be created from that.

A passport can be created from an auth object that can be used by
everyone who possesses a handle to the passport object to verify the
IDs of the auth object that the passport was created from, and if the
auth object is owned by any particular task (normally the user
requesting the.

The auth server should always create new passport objects for
different tasks, even if the underlying auth object is the same, so
that a task having the passport capability can not spy on other tasks
unless they were given the passport object by that task.



\section{Unix Domain Sockets and Pipes}

In the Hurd on Mach, there was a global pflocal server that provided
unix domain sockets and pipes to all users.  This will not work very
well in the Hurd on L4, because for descriptor passing, read:
capability passing, the unix domain socket server needs to accept
capabilities in transit.  User capabilities are often implemented by
untrusted servers, though, and thus a global pflocal server running as
root can not accept them.

However, unix domain sockets and pipes can not be implemented locally
in the task.  An external task is needed to hold buffered data
capabilities in transit.  in theory, a new task could be used for
every pipe or unix domain socketpair.  However, in practice, one
server for each user would suffice and perform better.

This works, because access to Unix Domain Sockets is controlled via
the filesystem, and access to pipes is controlled via file
descriptors, usually by inheritance.  For example, if a fifo is
installed as a passive translator in the filesystem, the first user
accessing it will create a pipe in his pflocal server.  From then on,
an active translator must be installed in the node that redirects any
other users to the right pflocal server implementing this fifo.  This
is asymmetrical in that the first user to access a fifo will implement
it, and thus pay the costs for it.  But it doesn't seem to cause any
particular problems in implementing the POSIX semantics.

The GNU C library can contact ~/servers/socket/pflocal to implement
socketpair, or start a pflocal server for this task's exclusive use if
that node doesn't exist.

All this are optimizations: It should work to have one pflocal process
for each socketpair.  However, performance should be better with a
shared pflocal server, one per user.


\section{Filesystem Translators}

The Hurd has the ability to let users mount filesystems and other
servers providing a filesystem-like interface.  Such filesystem
servers are called translators.  In the Hurd on GNU Mach, the parent
filesystem would automatically start up such translators from passive
translator settings in the inode.  It would then block until the child
filesystem sends a message to its bootstrap port (provided by the
parent fs) with its root directory port.  This root directory port can
then be given to any client looking up the translated node.

There are several things wrong with this scheme, which becomes
apparent in the Hurd on L4.  The parent filesystem must be careful to
not block on creating the child filesystem task.  It must also be
careful to not block on receiving any acknowledgement or startup
message from it.  Furthermore, it can not accept the root directory
capability from the child filesystem and forward it to clients, as
they are potentially not trusted.

The latter problem can be solved the following way: The filesystem
knows about the server thread in the child filesystem.  It also
implements an authentication capability that represents the ability to
access the child filesystem.  This capability is also given to the
child filesystem at startup (or when it attaches itself to the parent
filesystem).  On client dir\_lookup, the parent filesystem can return
the server\_thread and the authentication capability to the client.
The client can use that to initiate a connection with the child
filesystem (by first building up a connection, then sending the
authentication capability from the parent filesystem, and receiving a
root directory capability in exchange).

The actual creation of the child filesystem can be performed much like
a suid exec, just without any client to follow up with further
capabilities and startup info.  The only problem that remains is how
the parent filesystem can know which thread in the child filesystem
implements the initial handshake protocol for the clients to use.  The
only safe way here seems to be that the parent filesystem requires the
child to use the main thread for that, or that the parent filesystem
creates a second thread in the child at startup (passing its thread ID
in the startup data), requiring that this second thread is used.  In
either case the parent filesystem will know the thread ID in advance
because it created the thread in the first place.  This looks a bit
ugly, and violates good taste, so we might try to look for alternative
solutions.


\section{Debugging}

L4 does not support debugging.  So every task has to implement a debug
interface and implement debugging locally.  gdb needs to be changed to
make use of this interface.  How to perform the required
authentication, and how the debug thread is advertised to gdb, and how
the debug interface should look like, are all open questions.


\section{Scheduling}

The task server might implement an initial scheduler that just keeps
track of consumed CPU time, so we have some statistics.  Later, a
scheduler has to be written, that also can do SMP.  All of this is
still in the open.

There is no way to get at the ``system time'' in L4, it is assumed
that no time is spent in the kernel (which is mostly true).  So system
time will always be reported as 0.00, or 0.01.

\section{Device Drivers}

\subsection{Requirements}

  \begin{itemize}
  \item Performance: Speed is important!
  \item Portability: Framework should work on different architectures.
    
    Also: Useable in a not hurdisch environment with only
    small changes.

  \item Flexibility
  \item Convenient interfaces
  \item Consistency 
  \item Safety: driver failure should have as minimal system impact as
    possible.
  \end{itemize}

\subsection{Overview}

 The framework consists of: 
 \begin{itemize}
 \item Bus drivers
 \item Device drivers
 \item Service servers (plugin managers, $\omega_0$, rootserver)
 \end{itemize}

\subsubsection{Drivers and the filesystem}
  
  The device driver framework will only offer a physical device view.
  Ie. it will be a tree with devices as the leaves connected by
  various bus technologies.  Any logical view and naming persistence
  will have to be build on top of this (translator).

\subsubsection{Layer of the drivers}

  The device driver framework consists only of the lower level drivers
  and doesn't need to have a complicated scheme for access control.
  This is because it should be possible to share devices, e.g. for
  neighbour Hurd.  The authentication is done by installing a virtual
  driver in each OS/neighour Hurd.  The driver framework trusts these
  virtual drivers.  So it's possible for a non Hurdish system to use
  the driver framework just by implementing these virtual drivers.
  
  Only threads which have registered as trusted are allowed to access
  device drivers.  The check is simply done by checking the senders
  ID against a table of known threads.

\subsubsection{Address spaces}

  Drivers always reside in their own AS. The overhead for cross AS IPC
  is small enough to do so.

\subsubsection{Zero copying and DMA}
  
  It is assumed that there are no differences between physical memory
  pages. For example each physical memory page can be used for DMA
  transfers. Of course, older hardware like ISA devices can so not be
  supported. Who cares?
  
  With this assumption, the device driver framework can be given any
  physical memory page for DMA operation.  This physical memory page
  must be pinned down.
  
  If an application wants to send or receive data to/from a device
  driver it has to tell the virtual driver the page on which the
  operation has to be executed.  Since the application doesn't know
  the virtual-real memory mapping, it has to ask the physical memory
  manager for the real memory address of the page in question.  If the
  page is not directly mapped from the physical memory manager the
  application ask the mapper (another application which has mapped
  this memory region the first application) to resolve the mapping.
  This can be done recursively.  Normally, this resolving of mapping
  can be speed up using a cache services, since a small number of
  pages are reused very often.
  
  With the scheme, the drivers do not have to take special care of
  zero copying if there is only one virtual driver.  When there is
  more than one virtual driver pages have to copied for all other
  virtual drivers.

\subsubsection{Root bus driver}
  
  The root bus is the entrypoint to look up devices.
  
  XXX There should be iterators/visitors for operating on
  busses/devices.  (daniel)

\subsubsection{Physical versus logical device view}
  
  The device driver framework will only offer a physical device view.
  Ie. it will be a tree with devices as the leaves connected by
  various bus technologies.  Any logical view and naming persistence
  will have to be build on top of this (translator).

\subsubsection{Things for the future}

  \begin{itemize}
  \item Interaction with the task server (e.g. listings driver threads 
    with ps,etc.)
  \item Powermanagement
  \end{itemize}

\subsection{Bus Drivers}

A bus driver is responsible to manage the bus and provide access to
devices connected to it.  In practice it means a bus driver has to
perform the following tasks:

\begin{itemize}
\item Handle hotplug events
  
  Busses which do not support hotplugging, will treated as if there is
  1 insertion event for every device connected to it when the bus
  driver is started.  Drivers which don't support autoprobing of
  devices will probably have to read some configuration data from a
  file or if the driver is a needed for bootstrapping configuration
  can be given as argument on its stack.  In some cases the bus
  doesn't generate insertion/removal events, but can still support
  some form of hotplug functionality if the user tells the driver when
  a change to the bus configuration has happened (eg. SCSI).

\item Configure client device drivers
    
  The bus driver should start the appropriate client device driver
  translator when an insertion event is detected.  It should also
  provide the client device driver with all necessary configuration
  info, so it can access the device it needs.  This configuration data
  typically consists of the bus addresses of the device and possibly
  IRQ numbers or DMA channel ID's.  The device driver is loaded by the
  assotiatet plugin manager.

\item Provide access to devices
  
  This means the bus driver should be able to perform a bus
  transaction on behalf of a client device driver.  In some cases this
  involves sending a message and waiting for reply (eg. SCSI, USB,
  IEEE 1394, Fibre Channel,...).  The driver should provide
  send/receive message primitives in this case.  In other cases
  devices on the bus can be accessed by doing a memory accesses or by
  using special I/O instructions.  In this case the driver should
  provide mapping and unmapping primitives so a client device driver
  can get access to the memory range or is allowed to access the I/O
  addresses.  The client device driver should use a library, which is
  bus dependant, to access the device on the bus.  This library hides
  the platform specific details of accessing the bus.
  
  Furthermore the bus driver must also support rescans for hardware.
  It might be that not all drivers are found during bootstrapping and
  hence later on drivers could be loaded.  This is done by regenerate
  new attach notification sending to bus's plugin manager.  The plugin
  manager loads then if possible a new driver.  A probe funtion is not
  needed since all supported hardware can be identified by
  vendor/device identifactions (unlike ISA hardware).  For hardware
  busses which don't support such identifaction (ISA) only static
  configuration is possible (configuration scripts etc.)
\end{itemize}


\subsubsection{Plugin Manager}

  Each bus driver has a handle/reference to which insert/remove events
  are send.  The owner of the handle/refence must then take
  appropriate action like loading the drivers.  These actors are
  called plugin managers.

\subsubsection{Generic Bus Driver}

  Operations:
  \begin{itemize}
  \item notify (attach, detach)
  \item string enumerate
  \end{itemize}
  
  XXX Extract generic bus services from the PCI Bus Driver section
  which could be also be used other PCI related busses (ISA) be used.
  The name for this service is missleading, since a SCSI Bus Driver
  does not have anything in common with a PCI bus.  (daniel)

\subsubsection{ISA Bus Driver}
Inherits from:

\begin{itemize}
\item Generic Bus Driver
\end{itemize}

Operations:
\begin{itemize}
\item (none)
\end{itemize}

XXX The interface has not been defined up to now. (daniel)


\subsubsection{PCI Bus Driver}

Inherits from:
\begin{itemize}
\item Generic Bus Driver
\end{itemize}

Operations:
\begin{itemize}
\item map\_mmio: map a PCI BAR for MMIO
\item map\_io: map a PCI BAR for I/O
\item map\_mem: map a PCI BAR for memory
\item read\_mmio\_{8,16,32,64}: read from a MMIO register
\item write\_mmio\_{8,16,32,64}: write to a MMIO register
\item read\_io\_{8,16,32,64}: read from an IO register
\item write\_io\_{8,16,32,64}: write to an IO register
\item read\_config\_{8,16,32,?}: read from a PCI config register
\item write\_config\_{8,16,32,?}: write to a PCI config register
\item alloc\_dma\_mem(for non zero copying): allocate main memory useable for DMA
\item free\_dma\_mem  (for non zero copying): free main memory useable for DMA
\item prepare\_dma\_read: write back CPU cachelines for DMAable memory area
\item sync\_dma\_write: discard CPU cachelines for DMAable memory area
\item alloc\_consistent\_mem: allocate memory which is consistent between CPU 
  and device
\item free\_consistent\_mem: free memory which
  is consistent between CPU and device
\item get\_irq\_mapping (A,B,C,D): get the IRQ matching the INT(A,B,C,D) line
\end{itemize}

\subsection{Device Drivers}
\subsubsection{Classes}
\begin{itemize}
\item character: This the standard tty as known in the Unix environment.
\item block
\item human input: Keyboard, mouse, ...
\item packet switched network
\item circuit switched network
\item framebuffer
\item streaming audio
\item streaming video
\item solid state storage: flash memory
\end{itemize}

\subsubsection{Human input devices (HID) and the console}

The HIDs and the console are critical for user interaction with the
system.  Furthmore, the console should be working as soons as possible
to give feedback.  Log messages which are send to the console before
the hardware has been initialized should be buffered.

\subsubsection{Generic Device Driver}
Operations:
\begin{itemize}
\item init : prepare hardware for use
\item start : start normal operation
\item stop : stop normal operation
\item deinit : shutdown hardware
\item change\_irq\_peer : change peer thread to propagate irq message to.
\end{itemize}


\subsubsection{ISA Devices}
Inherits from:
\begin{itemize}
\item Generic Device Driver
\end{itemize}

Supported devices
\begin{itemize}
\item Keyboard (ps2)
\item serial port (mainly for debugging purposses)
\item parallel port
\end{itemize}

XXX interface definition for each device driver is missing. (daniel)


\subsubsection{PCI Devices}
Inherits from:
\begin{itemize}
\item Generic Device Driver
\end{itemize}
  
Supported devices:
\begin{itemize}
\item block devices
\item ...
\end{itemize}

XXX interface definition for each device driver is missing. (daniel)


\subsection{Resource Management}


\subsubsection{IRQ handling}

\paragraph{IRQ based interrupt vectors}

Some CPU architectures (eg 68k, IA32) can directly jump to an
interrupt vector depending on the IRQ number. This is typically the
case on CISC CPU's. In this case there is some priorization scheme. On
IA32 for example, the lowest IRQ number has the highest priority.
Sometimes the priorities are programmable.  Most RISC CPU's have only
a few interrupt vectors which are connected external IRQs. (typically
1 or 2). This means the IRQ handler should read a register in the
interrupt controller to determine which IRQ handler has to be
executed.  Sometimes the hardware assists here by providing a register
which indicates the highest priority interrupt according to some
(programmable) scheme.

\paragraph{IRQ acknowlegdement}

The IRQ acknowledgement is done in two steps. First inform the
hardware about the successful IRQ acceptance. Then inform the ISRs
about the IRQ event.

\paragraph{Edge versus level triggered IRQs}

Edge triggered IRQs typically don't need explicit acknowledgment by
the CPU at the device level. You can just acknowledge them at the
interrupt controller level.  Level triggered IRQs typically need to
explicitly acknowledged by the CPU at the device level. The CPU has to
read or write a register from the IRQ generating peripheral to make
the IRQ go away. If this is not done, the IRQ handler will be
reentered immediatly after it ended, effectively creating an endless
loop. Another way of preventing this would be to mask the IRQ.

\paragraph{Multiple interrupt controllers}

Some systems have multiple interrupt controllers in cascade. This is
for example the case on a PC, where you have 2 8259 interrupt
controllers. The second controller is connected to the IRQ 2 pin of
the first controller. It is also common in non PC systems which still
use some standard PC components such as a Super IO controller. In this
case the 2 8259's are connected to 1 pin of the primary interrupt
controller. Important for the software here is that you need to
acknowledge IRQ's at each controller. So to acknowledge an IRQ from
the second 8259 connected to the first 8259 connected to another
interrupt controller, you have to give an ACK command to each of those
controllers.  Another import fact is that on PC architecture the order
of the ACKs is important.

\paragraph{Shared IRQs}

Some systems have shared IRQs. In this case the IRQ handler has to
look at all devices using the same IRQ...

\paragraph{IRQ priorities}

All IRQs on L4 have priorities, so if an IRQ occurs any IRQ lower then
the first IRQ will be blocked until the first IRQ has been
acknowlegded.  ISR priorities must much the hardware priority (danger
of priority inversion).  Furthermore the IRQ acknowledgment order is
important.

The 8259 also supports a specific IRQ acknowledge iirc. But, this
scheme does not work in most level triggered IRQ environments. In
these environments you must acknowledge (or mask) the IRQ before
leaving the IRQ handler, otherwise the CPU will immediately reenter
the IRQ handler, effectively creating an endless loop. In this case L4
would have to mask the IRQ. The IRQ thread would have to unmask it
after acknowledgement and processing.

\paragraph{IRQ handling by L4/x86}

The L4 kernel does handle IRQ acknowlegdment. 

 
\subsubsection{$\omega_0$}

$\omega_0$ is a system-central IRQ-logic server. It runs in the
privileged AS space in order to be allowed rerouting IRQ IPC.

If an IRQ is shared between several devices, the drivers are daisy
chained and have to notify their peers if an IRQ IPC has arrived.

XXX For more detail see XXX URL missing

Operations:
\begin{itemize}
\item attach\_irq : attach an ISR thread to the IRQ 
\item detach\_irq : detach an ISR thread form the IRQ
\end{itemize}


\subsubsection{Memory}
If no physical memory pages are provided by the OS the device driver
framework alloces pages from the physical memory manager.  The device
driver framework has at no point of time to handle any virtual to
physical page mapping.


\subsection{Bootstrapping}

A simpleFS provides initial drivers for bootstraping.  The root bus
driver and simpleFS is loaded by grub as module.  It then signals for
loading new (bus) drivers.  As before if there is no driver avaible
for some reason for the device, the bus driver doesn't change the
device state and waits for a notifaction that there are new drivers
avaible. This simpleFS might be based on BSD libstand (library for
standalone applications).  simpleFS doesn't need to be writeable
either.


\subsubsection{Plugin Manager}
A Plugin manager handles driver loading for devices.  It searches for
driver in seach pathes (on filesystems).  It's possible to add new
search pathes later.  This allows the system to bootstrap with only
one search path (the simpleFS).  When the search path is changed, the
device tree will be scanned for devices which don't have a driver
loaded yet.  If a driver has become available, it will be loaded.


\subsection{Order of implementation}

\begin{enumerate}
\item rootserver, plugin server
\item root bus server
\item pci bus
\item isa bus
\item serial port  (isa bus)
\item console 
\end{enumerate}


\end{document}
