\chapter{Virtual Memory Management}

\begin{quote}
\emph{The mind and memory are more sharply exercised in comprehending
another man's things than our own.}

\begin{flushright}
\emph{Timber} or \emph{Discoveries} by Ben Jonson
\end{flushright}
\end{quote}


\section{Introduction}

The goal of an operating system is simply, perhaps reductively,
stated: manage the available resources.  In other words, it is the
operating system's job to dictate the policy for obtaining resources
and to provide mechanisms to use them.  Most resources which the
operating system manages are sparse resources, for instance the CPUs,
the memory and the various peripherals including graphics cards and
hard drives.  Any given process, therefore, needs to compete with the
other processes in the system for some subset of the available
resources at any given time.  As can be imagined, the policy to access
and the mechanisms to use these resources determines many important
characteristics of the system.

A simple single user system may use a trivial first come first serve
policy for allocating resources, a device abstraction layer and no
protection domains.  Although this design may be very light-weight and
the thin access layer conducive to high speed, this design will only
work on a system where all programs can be trusted: a single malicious
or buggy program can potentially halt all others from making progress
simply by refusing to yield the CPU or allocating and not releasing
resources in a timely fashion.

The Hurd, like Unix, aims to provide strong protection domains thereby
preventing processes from accidentally or maliciously harming the rest
of the system.  Unix has shown that this can be done efficiently.  But
more than Unix, the Hurd desires to identify pieces of the system
which Unix placed in the kernel but which need not be there as they
could be done in user space and provide additional user flexibility.
Through our experience and analysis, we are convinced that one area is
much of the virtual memory system: tasks are often allocating as much
memory without regard---because Unix provides them with no mechanism
to do so---for the rest of the system.  But it is not a cooperative
model which we wish to embrace but a model which holds the users of
the resource responsible for it and when asked to release some of its
memory will or violate the social contract and face exile.  Not only
will this empower users but it will force them to make smarter
decisions.

\subsection{Learning from Unix}

Unix was designed as a multiuser timesharing system with protection
domains thereby permitting process separation, i.e. allowing different
users to concurrently run processes in the system and gain access to
resources in a controlled fashion such that any one process cannot
hurt or excessively starve any other.  Unix achieved this through a
monolithic kernel design wherein both policy and mechanism are
provided by the kernel.  Due to the limited hardware available at the
time and the state of Multics\footnote{Multics was seen as a system
which would never realize due to its overly ambitious feature set.},
Unix imposed a strong policy on how resources could be used: a program
could access files, however, lower level mechanism such as the file
system, the virtual file system, network protocol stacks and devices
drivers all existed in the kernel proper.  This approach made sense
for the extremely limited hardware that Unix was targeted for in the
1970s.  As hardware performance increased, however, a separation
between mechanism and policy never took place and today Unix-like
operating systems are in a very similar state to those available two
decades ago; certainly, the implementations have been vastly improved
and tuned, however, the fundamental design remains the same.

One of the most important of the policy/mechanism couplings in the
kernel is the virtual memory subsystem: every component in the system
needs memory for a variety of reasons and with different priorities.
The system must attempt to meet a given allocation criteria.  However,
as the kernel does not and cannot know how how a task will use its
memory except based on the use of page fault statistics is bound to
make sub-ideal eviction decisions.  It is in part through years of
fine tuning that Unix is able to perform as well as it does for the
general applications which fit its assumed statistical model.

\subsection{Learning from Mach}

The faults of Unix became clear through the use of Mach.  The
designers of Mach observed that there was too much mechanism in the
kernel and attempted to export the file systems, network stack and
much of the system API into user space servers.  They left a very
powerful VMM in the kernel with the device drivers and a novel IPC
system.  Our experience shows that the VMM although very flexible, is
unable to make smart paging decisions: because Unix was tied to so
many subsystems, it had a fair knowledge of how a lot of the memory in
the system was being used.  It could therefore make good guesses about
what memory could be evicted and not be needed in the near future.
Mach, however, did not have this advantage and relied strictly on page
fault statistics and access pattern detection for its page eviction
policy.

Based on this observation, it is imperitive that the page eviction
scheme have good knowledge about how pages are being used as it only
requires a few bad decisions to destroy performance.  Thus, a new
design can either choose to return to the monolithic design and add
even more knowledge to the kernel to increase performance or the page
eviction scheme can be remove from the kernel completely and placed in
user space and make all tasks self paged.

\subsection{Following the Hurd Philosophy}

As the Hurd aims, like Unix, to be a multiuser system for mutually
untrusted users, security is an absolute necessity.  But it is not the
object of the system to limit users excessively: as long as operations
can be done securely, they should be permitted.  It is based on this
philosophy that we have adopted a self paging design for the new Hurd
VMM: who knows better how a task will use its memory than the task
itself?  This is clear from the problems that have been encountered
with LRU, the basic page evition algorithm, by database developers,
language designers implementing garbage collectors and soft realtime
application developers such as multimedia developers: they all wrestle
with the underlying operating system's page eviction scheme.  By
putting the responsibility to page on tasks we think that tasks will
be forced to make smart decisions as they can only hurt themselves.

\section{Self Paging}

If memory was infinite and the only problem was worrying about one
program accessing the memory of another, memory allocation would be
trivial.  This is not, however, the case: memory is visibly finite and
a well designed system will exploit it all.  As memory is a system
resource, a system wide memory allocation policy must be established
which maximizes memory usage according to a given set of criteria.

In a typical Unix-like VMM, allocating memory (e.g. using
\function{sbrk} or \function{mmap}) does not allocate physical memory
but \keyword{virtual memory}.  In order to increase the amount of
memory available to users, the kernel uses a \keyword{backing store},
typically a hard disk, to temporarily free physical memory thereby
allowing other processes to make progress.  The sum of these two is
referred to as virtual memory.  The use of backing store ensures data
integrity when physical memory must be freed and application
transparency is required.  A variety of criteria are used to determine
which frames are \keyword{paged out}, however, most often some form of
a priority based least recently used, LRU, algorithm is applied.  Upon
\keyword{memory pressure}, the system steals pages from low priority
processes which have not been used recently or drain pages from an
internal cache.

This design has a major problem: the kernel has to evict the pages but
only the applications know which pages they really need in the near
term.  The kernel could ask the applications for this data, however,
it is unable to trust the applications as they could, for instance,
not respond, and the kernel would have to forcefully evict pages
anyway.  As such, the kernel relies on page fault statistics to make
projections about how the memory will be used, thus the LRU eviction
scheme.  An additional result of this scheme is that as applications
never know if mapped memory is in core, they are unable to make
guarantees about deadlines.

These problems are grounded in the way the Unix VMM allocates memory:
it does not allocate physical memory but virtual memory.  This is
illustated by the following scenario: when a process starts and begins
to use memory, the allocator will happily give it all of memory in the
system as long as no other process wants it.  What happens, however,
when a second memory hungry process starts is that the kernel has no
way to take back memory it allocated to the first process.  At this
point, it has two options: it can either return failure to the second
process or it can steal memory from the first process and send it to
backing store.

One way to solve these problems is to have the VMM allocate phsyical
memory and make applications completely self-paged.  Thus, the burden
of paging lies the application themselves.  When application request
memory, they no longer request virutal memory but physical memory.
Once the application has exhausted its available frames, it is its
responsibility to multiplex the available frames.  Thus, virtual
memory is done in the application itself.  It is important to note
that a standard manager or managers should be supplied by the
operating system.  This is important for implementing something like a
POSIX personality.  This should not, however, be hard coded: certain
application may greatly benefit by being able to control their own
eviction schemes.  At its most basic level, hints could be provided to
the manager by introducing extentions on basic function calls.  For
instance, \function{malloc} could take an extra parameter indicating
the class of data being allocated.  These class would provide hints
about the expected usage pattern and life time of the data.

\section{Bootstrap}

When the Hurd starts up, all physical memory is eventually transfered
to the physical memory server by the root server.  At this point, the
physical memory server will control all of the physical pages in the
system.

\section{Memory Allocation Policy}

\subsection{Guaranteed Frames and Extra Frames}

The physical memory server maintains a concept of \keyword{guaranteed
frames} and \keyword{extra frames}.  The former are virtual frames
that a given task is guaranteed to map in a very short amount of time.
Given this predicate, the total number of guaranteed frames can never
exceed the total number of physical frames in the system.  Extra
frames are frames which are given to clients who have reached their
guaranteed frame allocation limit.  The physical memory server may
request that a client relinquish a number of extant extra frames at
any time.  The client must return the frames to the physical memory
(i.e. free them) in a short amount of time.  The task should not
assume that it has enough time to send frames to backing store.  As
such, extra frames should only contain remanufacturable data
(i.e. cached data).  Should a task fail to return the frames in a
reasonable amount of time, it risks having all of its memory
dropped---not swapped out or saved in any way---and reclaimed by the
physical memory server.  Note that the physical memory server does not
know if a given frame is considered guaranteed or extra: it knows that
a given task has $G$ guaranteed frames and $G + E$ allocated frames,
and $E$ extra frames.  The distinction between guaranteed and extra
frames must be made by the task itself.  One strategy is to remember
which frames can be remanufactured (e.g. reread from disk or
recalculated) and internally promote them to guaranteed frames when
the frame becomes dirty being careful to never have less than $E$
clean frames in the task.  Given these semantics, guanteed frames
should not be thought of as wired (e.g. \function{mlock}ed in the
POSIX sense)---although they can have this property---but as frames
which the task itself must multiplex.  Thus the idea of self-paged
tasks.

Readers familiar with VMS will see striking similarities with the
self-paging and guaranteed frame paradigms.  This is not without
reason.  Yet, differences remain: VMS does not have extra frames and
the number of guaranteed frames is fixed at task creation time.
Frames returned to VMS (in order to allocate a new frame) are placed
in a dirty list (thus the actual multiplexing of frames is done in
VMS, not in user space) thereby simulating a two level backing store:
a fast memory backing store where frames are waylaid and swap, where
they are sent to when sufficient memory pressure forces them out.  It
is in this way that a given task may access more than its quota of
memory when there is low memory contention (e.g. if there are two
tasks each with 100 frames and there are 1000 frames in the system for
tasks, the remaining 800 are not dormant).  Our divergence from VMS is
motivated by the location of file systems and device drivers in the
Hurd: unlike in VMS, the file systems and device drivers are in user
space.  Thus, the caching that was being done by VMS cannot be done
intelligently by the physical memory server.

\subsection{An External Memory Policy Server}

The number of guaranteed frames that a given task has access to is not
determined by the physical memory server but by the \keyword{memory
policy server}.  This division means the physical memory server need
only concern itself with allocation mechanisms; all policy decisions
are delegated to the policy server provided by the underlying
operating system.  (An important implication is that although tailored
for Hurd specific needs, the physical memory server is essentially
separate from the Hurd and can be used by other operating systems
running on the L4 microkernel.)  It is the memory policy server's
responsibility to determine who gets how much memory.  This may be
calculated as a function of the user or looking in a file on disk for
e.g. quotas.  As can be seen this type of data acquisition could add
significant complexity to the physical memory server and require
blocking states (e.g. waiting for a read operation on file i/o) and
could create circular dependencies.  The default memory policy
server's mechanisms and policies will be discussed later.

The physical memory server and the memory policy server will contain a
shared buffer of tupples indexed by task id containing the number of
allocated frames, the number of guaranteed frame, and a boolean
indicating whether or not this task is eligible for guaranteed frames.
The guaranteed frame field and the extra frame predicate may only be
written to by the memory policy server.  The number of allocated frames
may only be written to by the physical memory server.  This scheme
means that no locking in required.  (On some architectures where a
read of a given field cannot be performed in a single operation, the
read may have to be done twice.)  The memory policy server must not
over commit the number of frames, i.e. the total number of guaranteed
frames must never exceed the number of frames avilable for allocation.

Until the memory policy server makes the intial contact with the
physical memory server, memory will be allocated on a first come first
serve basis.  The memory policy server shall use the following remote
procedure call to contact the physical memory server:

\begin{code}
error\_t pm\_get\_control (out hurd\_cap\_t control)
\end{code}

\noindent
This function will succeed the first time it is called and return a
control capability.  It will fail all subsequent times.  By using a
capability, the acquiring task may move or copy the capability to
another task.  This permits replacing the policy server on a live
system.  At this point, the physical memory server will begin
allocating memory according to the described protocol.  Note that the
inital buffer will be initialized with the current total allocations
while the guaranteed frames will be set to zero.  The memory policy
server must request the shared policy buffer as soon as possible and
adjust these values.

The shared policy buffer may be obtained from the physical memory
server by the policy by calling:

\begin{code}
error\_t pm\_get\_policy\_buffer (out l4\_map\_t buffer)
\end{code}

\noindent
The returned buffer is mapped with read and write access into the
policy memory server's address space.  It may need to be resized due
to the number of tasks in the system.  When this is the case, the
physical memory server shall unmap the buffer from the memory policy
server's address space and copy the buffer internally as required.
The memory policy server will fault on the memory region on its next
access and it may rerequest the buffer.  This call will succeed when
the sender is the memory policy server, it will fail otherwise.

\section{Containers}

Containers are the basic abstraction used for allocating, addressing
and sharing memory.  Conceptually, containers contain a set of
integers identifying \keyword{virtual frame}s in the physical memory
server.  A virtual frame references a physical frame but is not bound
to a particular physical frame (this allows the physical memory server
to move the contents of frames around).  Multiple physical frames may
reference the same physical frame in which case the memory is shared.
Sharing may be either real, e.g. System V shared memory, or logical,
e.g. copy on write.

When a virtual frame is allocated into a container, there may be no
physical frame associated with it.  The physical memory server
guarantees that when the contents of the virtual frame is accessed a
physical frame will be provided in a short amount of time
(cf. guaranteed virtual frames above).

Each virtual frame in a container counts against the container's
owner's total allocated frames.  Only the owner of a container may
allocate frames into a container.

Containers only hold virtual frames.  When the contents of a frame are
copied to backing store, no association between the data on the
backing store and the the frame identifier in the container is
maintained by the physical memory server.

When a task starts, it will allocate an initial contain and several
frames into it.  Typically, the total amount of memory used by an
application will exceed the total number of guaranteed frames.  When
the task reaches its maximum permitted allocation, it must reuse an
available frame.  Typically, the task will choose a victim page, unmap
any pages that point to the associated frame, swap the frame out, mark
the frame as swapped out and save the swap identifier in the mapping
database.  At this point, the task may reuse the frame.  This example
illustrates that imagining a virtual frame as bound to a page in a
task's address space for its entire lifetime is incorrect.  It should
also now be clear that when the data is eventually brought back into
memory from backing store, it may reside in a different virtual frame.

Containers are used for passing data between tasks.  Typically there
will be two tasks, a client and a server.  L4 provides a mechanism to
map pages from one address space to another.  This mechanism could be
used when a file is mapped into a task's address space, however, this
can present several problems.  If the server dies before the client,
the mappings in the client's address space will suddenly disappear.
Similarly, if the server is malicious, it may revoke the mappings at
some inconvenient (i.e. unrecoverable) time for the client causing it
to crash.  If a server allocates resources on behalf of the the client
it becomes impossible to do system wide resource accounting as many
servers are not trusted by the system.  All of these problems are
solved by containers.  When a client needs to obtain a memory mapping
from a server, it creates a container and adds to it container the
number of frames that the server will require for the operation.  It
then shares the container with the server and the server copies the
data into the frames.  It is important to understand that the server
does not ``fill'' the container: the number of frames remains constant
but the state of the bits changes.  When the server returns to the
client, the client unshares the container and is now able to map the
frames into its address space by contacting the physical memory
server.  Should the server die, the client remains uneffected as the
data is cached in the physical memory server.  The physical memory
server is also trusted thus if a task is malicious, it can only be
malicious during the initial copy of the data into the container,
i.e. before the client starts using the data.  Finally, as the
resources are allocated by the client via system servers, resource
accounting is possible.

\subsection{Creating Containers}

Applications are able allocate memory into containers.  Containers may
be created using:

\begin{code}
error\_t pm\_container\_create (out container\_t container)
\end{code}

Memory allocation does not allocate physical frames: if so, it would
be impossible to move memory around and memory would have to be
returned to the same spot after being swapped out and back in.
Containers are useful for grouping and then moving memory around.

How to get frames.  Type of frames (e.g. DMA) or fixed physical address.

Memory is not allocate until map time (and not always then,
e.g. logical copies).

\subsection{Mapping Memory}

The physical memory server guarantees that a mapping operation will
take a short amount of time: this is not guaranteed to happen
immediately as the virtual frames may only be allocated at this point
and they may have to be reaped from other tasks' extra frame
allocations.

The physical memory server may unmap pages at any time.  This allows
the contents of vitual frames to be moved between physical frames by
the physical memory server which permits page blocking (i.e. the
construction of superpages), the creation of DMAable memory areas or
other specific physical address and the creation of contiguous blocks
of memory (e.g. to defragment physical memory).  Tasks must be
prepared to reestablish a mapping with the physical memory server at
any time.

Mappings may be granted readonly evil if the a read/write mapping was
requested: the physical memory server will not grant a read/write
mapping if the frame is marked copy on write.  In order to get a
read/write mapping (and thus force the copy on write early), the task
must request add the enforced write flag when mapping.

\begin{code}
error\_t pm\_container\_map (in container\_t container, in frame\_t
start, in int nr\_frames, in int flags)
\end{code}

\noindent
Flags may is a bit wise or of: CONTAINER\_MAP\_READ,
CONTAINER\_MAP\_WRITE and CONTAINER\_MAP\_ENFORCE\_WRITE.

\subsection{Moving Data}

In a monolithic kernel, little data is exchanged between tasks.  In a
multiserver system, file systems live in their own tasks and thus
reading and writing involve servers.  Thus, powerful primatives for
moving memory around with the least number of physical copies,
i.e. using virtual copy mechanisms which preserve COW frames, etc.

It is important that an fs does not map from one task to another
directly: the client may not trust the source or the source may die,
etc.  Doing the mapping via the phys memory server means all trust
issues are resolved at the time of mapping and can be reported to the
user: the fs cannot pretend to be nice and then revoke mappings and
silently harm the client.

Data will be moved around using containers.  Describe how to read and
write.  Task -> FS -> Device drivers.  Locking memory.  Caching.

It is important that clients do the allocation for the memory which
they use: not the servers doing allocations on behalf on clients: in
the latter, there is no way to do resource tracking.

Discuss mmap: local function call.  RPC is done when a page is
faulted: do a read from the fs (into a container), then map the data
from the container into the AS as required.

MAP\_COPY sucks: fs must save all modified data.  What happens when a
100MB file is completely rewritten (or 1GB, etc)?  can we use upcalls?
If we do, the fs still needs to hold the data in the intern.  Can we
copy the file on disk and use that as backing store (think how
deleting an open file works).

Can a readonly private mapping once faulted be dropped or must we
promote it to anonymous memory and send it to swap fearing that the
underlying block might change between dropping it and rereading it
(e.g. by another task modifying the file)?

\section{Caching Store Accesses}

It need not be explained how caching accesses to stores can radically
improve the speed of the system.  In a monolithic kernel this cache is
added to by the readers, i.e. the device drivers, supplemented with
metadata from the file systems in the form of expected access patterns
based on the type of data and how the file was opened and managed by
the virtual memory manager.  In our design, this is impossible: each
component---each device driver, each file system and the physical
memory manager---all live in their own address spaces; additionally
there will rarely be mutual trust: the physical memory server may not
trust the file systems nor the ``device drivers'' (consider a network
block device).  A caching mechanism must be designed.

The purpose of caching is useful for multiple readers of a given
block.  Sometimes this is the same task, however, more often it is
multiple tasks.  Thus, having the caching scheme in each task is quite
difficult as tasks do not trust one another and furthermore, tasks can
die at any time thereby dropping their cache.  The logical place to
put the cache then is the common point of access, the file system.

An argument could be made that in reality, the common point of access
is the device driver: there can be multiple accessors of the same
store.  The question must be asked: what happens when the device
driver is made the cache point instead of the file system?  Logically,
a large tradeoff is made in terms of the ability to intelligently
decide what frame to keep in the cache.  The file system, for
instance, has meta-data about how a given frame may be used based on
how a file is opened and may realize that some frames need not be
placed in the cache because they will be used once and immediately
discarded.  This is true of the access patterns of multimedia
applications.  These types of hints may be gathered at file open time.
The class of data is another way the file system is able to predict
usage, for example, it understands the difference between
meta-data---inodes and directories---and file data.  A file system is
also able to anticipate file-level access patterns whereas a device
driver can only anticipate block-level access patterns, i.e. although
file data is sometimes sequential, it is often scattered across a
section of the disk due to fragmentation.  The primary way a the
device driver can really manage its cache is through historical data
in the form of previous accesses (which is itself even more limited as
the device driver is uninformed of cache hits in the file system
cache).  This type of data implies some form of LRU, least recently
used, eviction scheme.  It should now be clear that the file system
can make smarter decisions about what which blocks to evict due to its
ability to make predictions based on client hints and its greater
understanding of the data in the store.

If we resign ourselves to keeping the cache only in the file system,
then multiple users of a store will be penalized greatly: a block read
by one client will always be reread if another client requests the
same block: not only is the store accessed a second time, but twice as
much memory will be used as there is no way to share the frame and use
copy on write.  Is this penalty worth the added intelligence in the
file system?  An argument can be made that using just one caching
strategy is suboptimital when we could just have two: nothing stops
both the file system and the device driver from caching thereby
permitting the former to continue to maintain an intelligent cache and
the device driver to have its simple LRU cache.  This argument
overlooks several important implications of having the two caches.
First, complexity is being added to the device driver in the form of a
list of frames it has read and given out.  This increase in memory
usage has a secondary effect: if the data structures become large (as
it certainly will for large active stores), it will be impossible to
keep the device driver in question in a small address space (an
important optimization on architectures without tagged TLBs, table
look aside buffers).  Second, if both the file system and the device
driver keep a cache, when the file system has a cache miss, the device
driver then checks its cache before going to disk.  The device driver
will only ever have a cache hit if there are multiple readers: when
there is a single user of a store, the file system's cache and the
device driver's cache will be identical.  This begs the question: how
often will there be multiple users of a single store?  The answer
seems to be very rarely: assuming the common case that the store has
some type of file system on it, there can only be multiple users if
all users are readers (note that not even one can be a writer as this
implies cache consistency issues across different users of the store).
Since this is a very rare case, we argue based on the philosophy ``do
not optimize for rare cases'' that the overhead is greater than the
potential pay back from the optimization.  Having multiple caches
leads to a further problem: a frame is really not evicted from the
system until it is purged from all caches.  Thus if the file system
cache is smart and chooses the better frames to evict, the
cooresponding physical frames will not really be freed until the
device driver also drops its references to the frames.  Thus, the
effectiveness of the smarter caching algorithm is impeded by the
device driver's caching scheme.  Double caching must be avoided.

\subsection{Caching in the File System}

We have argued above that all block caching will be done at the file
system layer.  In this section, we detail how the caching will work.

The file system allocates extra frames as long as it can and adds all
eligible frames to the cache by logically copying them into a local
container (data which it reasons will be read once and then dropped
may not be considered eligible).  When the physical memory server
wants frames back, it chooses a victim with extra frames and asks for a
subset of them back.  If a task has $G$ guaranteed frames and $G + E$
frames allocated, the physical memory server can request up to $E$
frames back from the task.  We recall from the definition of the extra
frames that extra frames must be given back quickly (i.e. there is no
time to send them to swap).

Although a task chooses a frame to evict from its cache, it does not
mean that the frame will be reused immediately, in fact, it is
sometimes that case that the frame cannot be reused at all as another
task has a reference to the frame (in the form of a logical copy).  As
such, it would be nice to be able to get frames back that might still
be in the physical memory server.  The following mechanism is thus
provided: when a frame is returned to the physical memory server, the
reference to the frame is turned into a soft reference.  Only when the
frame is actually reused by the physical memory server are soft
references discarded.  A task is able to convert a soft reference back
to a hard reference by contacting the physical memory server and
asking for the frame back.  If this operation returns
\errno{ENOEXIST}, the frame has been reused and the frame must be
remanufactured (e.g. by retrieving it from backing store).  This
operation may also fail and return \errno{ENOMEM} if the task does not
have enough guaranteed frames and there are no extra frames available.

\begin{comment}
There is a problem here in the form of name space pollution: the task
doing the caching has to remember the mapping of blocks to container
identifiers in order to recover the soft reference but the task has no
way to know when the physical memory server expires a given soft
reference.  Thus, while the physical memory server may drop a frame,
the task will only ever know this when it tries to convert the soft
reference to a hard reference and fails (i.e. gets a cache miss).  For
frames which this is never done, the memorized mapping will never be
invalidated.  This may not be a problem if a block offset to container
id is used, however, if hashing is done or some other mapping of block
offsets to container identifiers is used, this will pollute the cache
container's name space.
\end{comment}

\subsection{Caching Interfaces}

The physical memory server will do an up call to a victim task
requesting a number of frames back.  The physical memory server may do
this at any time for any reason and it expects to receive the frames
back from the task within a short amount of time (the victim task
should not expect to be able to send the frames to backing store in
that amount of time).  The physical memory server will never request
guaranteed frames.  As such, this number will always be less than or
equal to the number of allocated frames minus the number of guaranteed
frames.

\begin{code}
void pm\_return\_frames (in int count);
\end{code}

The physical memory send this message to the task's memory control
thread.  The thread must always be ready to receive: the physical
memory server will never wait (thus, the thread must be in the
receiving state).  If the thread is not ready, the physical memory
server assumes that the task is misbehaving.  The physical memory
server does not wait for a reply, instead, the client must free the
frames using \function{pm\_release\_frames} as described above.

\section{The Memory Policy Server}

At task creation time, the task must negotiate a medium-term contract
for guaranteed frames and determine if it shall have access to extra
frames.  This may be renegotiated later.  It must be renegotiated when
the contract expires.  The policy server will give the task enough
time to send frames to swap before committing if the number of
guaranteed frames is reduced.

\section{Sending Data to Swap}

When a task reaches its guaranteed frame allocation, it must begin to
reuse its available virtual frames.  If the data is frames is precious
(i.e. not easliy constructed by e.g. a calculation or by rereading a
file) then the task will want to save the contents for when it is
needed in the future.  This can be done by sending a frame to backing
store.

\begin{code}
error\_t pm\_swap (in container\_t c, in container\_frame\_t frame, in int
count, out [] swap\_ids)
\end{code}

The swap server resides in (or is proxied by) the phsyical memory
server.  This allows the logical copies of frames to be preserved
across the swapped out period (i.e. logical copies are not lost when a
frame is sent to swap).  If this was not the case, then when a number
of tasks all with a reference to a given physical send the frame to
swap, the swap server would allocate and write N times as opposed to
once when all of the tasks eventually release any references to the
frame.

Frame may not be sent to swap immediately.  Instead, they are kept on
an inactive list allowing thereby allowing a task to recover the
contents of a frame before it is flushed to swap (that is to say, swap
operations are not synchronous).

Since there may be multiple references to a virtual frame, it is
recommended that \function{pm\_container\_orphan\_data} be called
before the frame is reused to prevent gratuitous copy on writes from
begin performed.  It also important to call this function if the frame
was being used for shared memory.

Swap quotas (put the policy in the memory policy server).

\section{Self Paging}

Tasks multiplex guaranteed frames.  Must manage their own memory.  How
to get data (e.g. extend malloc via the slab mechanism, extend fopen).

Multiplexing guaranteed frames: say the contents of a frame are sent
to swap in order to reuse the frame for something else.  The frame
itself must be cleared, i.e. disassocitated with any logical copies.
This is done using:

\begin{code}
error\_t pm\_release\_data (in pm\_container\_t container, in pm\_frame\_t[] frames)
\end{code}


% Traditionally, monolithical kernels, but even kernels like Mach,
% provide a virtual memory management system in the kernel.  All paging
% decisions are made by the kernel itself.  This requires good
% heuristics.  Smart paging decisions are often not possible because the
% kernel lacks the information about how the data is used.
% 
% In the Hurd, paging will be done locally in each task.  A physical
% memory server provides a number of guaranteed physical pages to tasks.
% It will also provide a number of excess pages (over-commit).  The task
% might have to return any number of excess pages on short notice.  If
% the task does not comply, all mappings are revoked (essentially
% killing the task).
% 
% A problem arises when data has to be exchanged between a client and a
% server, and the server wants to have control over the content of the
% pages (for example, pass it on to other servers, like device drivers).
% The client can not map the pages directly into the servers address
% space, as it is not trusted.  Container objects created in the
% physical memory server and mapped into the client and/or the servers
% address space will provide the necessary security features to allow
% this.  This can be used for DMA and zero-copying in the data exchange
% between device drivers and (untrusted) user tasks.
% 
% 
