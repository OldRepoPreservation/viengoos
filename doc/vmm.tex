\chapter{Virtual Memory Management}

\begin{quote}
\emph{The mind and memory are more sharply exercised in comprehending
another man's things than our own.}

\begin{flushright}
\emph{Timber} or \emph{Discoveries} by Ben Jonson
\end{flushright}
\end{quote}


\section{Introduction}

The goal of an operating system is simply, perhaps reductively,
stated: manage the available resources.  In other words, it is the
operating system's job to dictate the policy for obtaining resources
and to provide mechanisms to use them.  Most resources which the
operating system manages are sparse resources, for instance the CPUs,
the memory and the various peripherals including graphics cards and
hard drives.  Any given process, therefore, needs to compete with the
other processes in the system for some subset of the available
resources at any given time.  As can be imagined, the policy to access
and the mechanisms to use these resources determines many important
characteristics of the system.

A simple single user system may use a trivial first come first serve
policy for allocating resources, a device abstraction layer and no
protection domains.  Although this design may be very light-weight and
the thin access layer conducive to high speed, this design will only
work on a system where all programs can be trusted: a single malicious
or buggy program can potentially halt all others from making progress
simply by refusing to yield the CPU or allocating and not releasing
resources in a timely fashion.

The Hurd, like Unix, aims to provide strong protection domains thereby
preventing processes from accidentally or maliciously harming the rest
of the system.  Unix has shown that this can be done efficiently.  But
more than Unix, the Hurd desires to identify pieces of the system
which Unix placed in the kernel but which need not be there as they
could be done in user space and provide additional user flexibility.
Through our experience and analysis, we are convinced that one area is
much of the virtual memory system: tasks are often allocating as much
memory without regard---because Unix provides them with no mechanism
to do so---for the rest of the system.  But it is not a cooperative
model which we wish to embrace but a model which holds the users of
the resource responsible for it and when asked to release some of its
memory will or violate the social contract and face exile.  Not only
will this empower users but it will force them to make smarter
decisions.

\subsection{Learning from Unix}

Unix was designed as a multiuser timesharing system with protection
domains thereby permitting process separation, i.e. allowing different
users to concurrently run processes in the system and gain access to
resources in a controlled fashion such that any one process cannot
hurt or excessively starve any other.  Unix achieved this through a
monolithic kernel design wherein both policy and mechanism are
provided by the kernel.  Due to the limited hardware available at the
time and the state of Multics\footnote{Multics was seen as a system
which would never realize due to its overly ambitious feature set.},
Unix imposed a strong policy on how resources could be used: a program
could access files, however, lower level mechanism such as the file
system, the virtual file system, network protocol stacks and devices
drivers all existed in the kernel proper.  This approach made sense
for the extremely limited hardware that Unix was targeted for in the
1970s.  As hardware performance increased, however, a separation
between mechanism and policy never took place and today Unix-like
operating systems are in a very similar state to those available two
decades ago; certainly, the implementations have been vastly improved
and tuned, however, the fundamental design remains the same.

One of the most important of the policy/mechanism couplings in the
kernel is the virtual memory subsystem: every component in the system
needs memory for a variety of reasons and with different priorities.
The system must attempt to meet a given allocation criteria.  However,
as the kernel does not and cannot know how how a task will use its
memory except based on the use of page fault statistics is bound to
make sub-ideal eviction decisions.  It is in part through years of
fine tuning that Unix is able to perform as well as it does for the
general applications which fit its assumed statistical model.

\subsection{Learning from Mach}

The faults of Unix became clear through the use of Mach.  The
designers of Mach observed that there was too much mechanism in the
kernel and attempted to export the file systems, network stack and
much of the system API into user space servers.  They left a very
powerful VMM in the kernel with the device drivers and a novel IPC
system.  Our experience shows that the VMM although very flexible, is
unable to make smart paging decisions: because Unix was tied to so
many subsystems, it had a fair knowledge of how a lot of the memory in
the system was being used.  It could therefore make good guesses about
what memory could be evicted and not be needed in the near future.
Mach, however, did not have this advantage and relied strictly on page
fault statistics and access pattern detection for its page eviction
policy.

Based on this observation, it is imperitive that the page eviction
scheme have good knowledge about how pages are being used as it only
requires a few bad decisions to destroy performance.  Thus, a new
design can either choose to return to the monolithic design and add
even more knowledge to the kernel to increase performance or the page
eviction scheme can be remove from the kernel completely and placed in
user space and make all tasks self paged.

\subsection{Following the Hurd Philosophy}

As the Hurd aims, like Unix, to be a multiuser system for mutually
untrusted users, security is an absolute necessity.  But it is not the
object of the system to limit users excessively: as long as operations
can be done securely, they should be permitted.  It is based on this
philosophy that we have adopted a self paging design for the new Hurd
VMM: who knows better how a task will use its memory than the task
itself?  This is clear from the problems that have been encountered
with LRU, the basic page evition algorithm, by database developers,
language designers implementing garbage collectors and soft realtime
application developers such as multimedia developers: they all wrestle
with the underlying operating system's page eviction scheme.  By
putting the responsibility to page on tasks we think that tasks will
be forced to make smart decisions as they can only hurt themselves.

\section{Self Paging}

If memory was infinite and the only problem was worrying about one
program accessing the memory of another, memory allocation would be
trivial.  This is not, however, the case: memory is visibly finite and
a well designed system will exploit it all.  As memory is a system
resource, a system wide memory allocation policy must be established
which maximizes memory usage according to a given set of criteria.

In a typical Unix-like VMM, allocating memory (e.g. using
\function{sbrk} or \function{mmap}) does not allocate physical memory
but \keyword{virtual memory}.  In order to increase the amount of
memory available to users, the kernel uses a \keyword{backing store},
typically a hard disk, to temporarily free physical memory thereby
allowing other processes to make progress.  The sum of these two is
referred to as virtual memory.  The use of backing store ensures data
integrity when physical memory must be freed and application
transparency is required.  A variety of criteria are used to determine
which frames are \keyword{paged out}, however, most often some form of
a priority based least recently used, LRU, algorithm is applied.  Upon
\keyword{memory pressure}, the system steals pages from low priority
processes which have not been used recently or drain pages from an
internal cache.

This design has a major problem: the kernel has to evict the pages but
only the applications know which pages they really need in the near
term.  The kernel could ask the applications for this data, however,
it is unable to trust the applications as they could, for instance,
not respond, and the kernel would have to forcefully evict pages
anyway.  As such, the kernel relies on page fault statistics to make
projections about how the memory will be used, thus the LRU eviction
scheme.  An additional result of this scheme is that as applications
never know if mapped memory is in core, they are unable to make
guarantees about deadlines.

These problems are grounded in the way the Unix VMM allocates memory:
it does not allocate physical memory but virtual memory.  This is
illustated by the following scenario: when a process starts and begins
to use memory, the allocator will happily give it all of memory in the
system as long as no other process wants it.  What happens, however,
when a second memory hungry process starts is that the kernel has no
way to take back memory it allocated to the first process.  At this
point, it has two options: it can either return failure to the second
process or it can steal memory from the first process and send it to
backing store.

One way to solve these problems is to have the VMM allocate phsyical
memory and make applications completely self-paged.  Thus, the burden
of paging lies the application themselves.  When application request
memory, they no longer request virutal memory but physical memory.
Once the application has exhausted its available frames, it is its
responsibility to multiplex the available frames.  Thus, virtual
memory is done in the application itself.  It is important to note
that a standard manager or managers should be supplied by the
operating system.  This is important for implementing something like a
POSIX personality.  This should not, however, be hard coded: certain
application may greatly benefit by being able to control their own
eviction schemes.  At its most basic level, hints could be provided to
the manager by introducing extentions on basic function calls.  For
instance, \function{malloc} could take an extra parameter indicating
the class of data being allocated.  These class would provide hints
about the expected usage pattern and life time of the data.

\section{Bootstrap}

When the Hurd starts up, all physical memory is eventually transfered
to the physical memory server by the root server.  At this point, the
physical memory server will control all of the physical pages in the
system.

\section{Memory Allocation Policy}

\subsection{Guaranteed Frames and Extra Frames}

The physical memory server maintains a concept of \keyword{guaranteed
frames} and \keyword{extra frames}.  The former are virtual frames
that a given task is guaranteed to map in a very short amount of time.
Given this predicate, the total number of guaranteed frames can never
exceed the total number of physical frames in the system.  Extra
frames are frames which are given to clients who have reached their
guaranteed frame allocation limit.  The physical memory server may
request that a client relinquish a number of extant extra frames at
any time.  The client must return the frames to the physical memory
(i.e. free them) in a short amount of time.  The task should not
assume that it has enough time to send frames to backing store.  As
such, extra frames should only contain remanufacturable data
(i.e. cached data).  Should a task fail to return the frames in a
reasonable amount of time, it risks having all of its memory
dropped---not swapped out or saved in any way---and reclaimed by the
physical memory server.  Note that the physical memory server does not
know if a given frame is considered guaranteed or extra: it knows that
a given task has $G$ guaranteed frames and $G + E$ allocated frames,
and $E$ extra frames.  The distinction between guaranteed and extra
frames must be made by the task itself.  One strategy is to remember
which frames can be remanufactured (e.g. reread from disk or
recalculated) and internally promote them to guaranteed frames when
the frame becomes dirty being careful to never have less than $E$
clean frames in the task.  Given these semantics, guanteed frames
should not be thought of as wired (e.g. \function{mlock}ed in the
POSIX sense)---although they can have this property---but as frames
which the task itself must multiplex.  Thus the idea of self-paged
tasks.

Readers familiar with VMS will see striking similarities with the
self-paging and guaranteed frame paradigms.  This is not without
reason.  Yet, differences remain: VMS does not have extra frames and
the number of guaranteed frames is fixed at task creation time.
Frames returned to VMS (in order to allocate a new frame) are placed
in a dirty list (thus the actual multiplexing of frames is done in
VMS, not in user space) thereby simulating a two level backing store:
a fast memory backing store where frames are waylaid and swap, where
they are sent to when sufficient memory pressure forces them out.  It
is in this way that a given task may access more than its quota of
memory when there is low memory contention (e.g. if there are two
tasks each with 100 frames and there are 1000 frames in the system for
tasks, the remaining 800 are not dormant).  Our divergence from VMS is
motivated by the location of file systems and device drivers in the
Hurd: unlike in VMS, the file systems and device drivers are in user
space.  Thus, the caching that was being done by VMS cannot be done
intelligently by the physical memory server.

\subsection{An External Memory Policy Server}

The number of guaranteed frames that a given task has access to is not
determined by the physical memory server but by the \keyword{memory
policy server}.  This division means the physical memory server need
only concern itself with allocation mechanisms; all policy decisions
are delegated to the policy server provided by the underlying
operating system.  (An important implication is that although tailored
for Hurd specific needs, the physical memory server is essentially
separate from the Hurd and can be used by other operating systems
running on the L4 microkernel.)  It is the memory policy server's
responsibility to determine who gets how much memory.  This may be
calculated as a function of the user or looking in a file on disk for
e.g. quotas.  As can be seen this type of data acquisition could add
significant complexity to the physical memory server and require
blocking states (e.g. waiting for a read operation on file i/o) and
could create circular dependencies.  The default memory policy
server's mechanisms and policies will be discussed later.

The physical memory server and the memory policy server will contain a
shared buffer of tupples indexed by task id containing the number of
allocated frames, the number of guaranteed frame, and a boolean
indicating whether or not this task is eligible for guaranteed frames.
The guaranteed frame field and the extra frame predicate may only be
written to by the memory policy server.  The number of allocated frames
may only be written to by the physical memory server.  This scheme
means that no locking in required.  (On some architectures where a
read of a given field cannot be performed in a single operation, the
read may have to be done twice.)  The memory policy server must not
over commit the number of frames, i.e. the total number of guaranteed
frames must never exceed the number of frames avilable for allocation.

Until the memory policy server makes the intial contact with the
physical memory server, memory will be allocated on a first come first
serve basis.  The memory policy server shall use the following remote
procedure call to contact the physical memory server:

\begin{code}
error\_t pm\_get\_control (out hurd\_cap\_t control)
\end{code}

\noindent
This function will succeed the first time it is called and return a
control capability.  It will fail all subsequent times.  By using a
capability, the acquiring task may move or copy the capability to
another task.  This permits replacing the policy server on a live
system.  At this point, the physical memory server will begin
allocating memory according to the described protocol.  Note that the
inital buffer will be initialized with the current total allocations
while the guaranteed frames will be set to zero.  The memory policy
server must request the shared policy buffer as soon as possible and
adjust these values.

The shared policy buffer may be obtained from the physical memory
server by the policy by calling:

\begin{code}
error\_t pm\_get\_policy\_buffer (out l4\_map\_t buffer)
\end{code}

\noindent
The returned buffer is mapped with read and write access into the
policy memory server's address space.  It may need to be resized due
to the number of tasks in the system.  When this is the case, the
physical memory server shall unmap the buffer from the memory policy
server's address space and copy the buffer internally as required.
The memory policy server will fault on the memory region on its next
access and it may rerequest the buffer.  This call will succeed when
the sender is the memory policy server, it will fail otherwise.

\section{Containers}

In a monolithic kernel, other than through pipes, little data is
exchanged between tasks: all services are provided by the kernel, a
trusted entity which is able to directly access tasks' address space.
In a multiserver system, most data acquisitions come from user space
servers.  As such, powerful primatives for moving memory around is an
absolute necessity: physical copying must be kept to an absolute
minimum and there must be a way to use and preserve copy on write
pages.

Containers are the basic abstraction used for allocating, addressing
and sharing memory.  Conceptually, containers contain a set of
integers identifying \keyword{virtual frame}s in the physical memory
server.  A virtual frame references a physical frame but is not bound
to a particular physical frame (thereby allowing the physical memory
server to move the contents between physical frames for page blocking,
assembly of DMA arena and memory defragmentation).  Virtual frames are
thus the sharing mechanism for physical frames.  Although virtual
frames cannot be copied, their contents may be logically copied such
that a new virtual frame is created with the same underlying physical
frame.  Sharing may be either real, e.g. System V shared memory, or
logical, e.g. copy on write.

When a virtual frame is allocated into a container, there may be no
physical frame associated with it.  The physical memory server
guarantees that when the contents of the virtual frame is accessed a
physical frame will be provided in a short amount of time
(cf. guaranteed virtual frames above).

Each virtual frame in a container counts against the container's
owner's total allocated frames.  Only the owner of a container may
allocate frames into a container.

Containers only hold virtual frames.  When the contents of a frame are
copied to backing store, no association between the data on the
backing store and the the frame identifier in the container is
maintained by the physical memory server.

When a task starts, it will allocate an initial container and several
frames into it.  Typically, the total amount of memory used by an
application will exceed the total number of guaranteed frames.  When
the task reaches its maximum permitted allocation, it must reuse an
available frame.  Typically, the task will choose a victim page, unmap
any pages that point to the associated frame, swap the frame out, mark
the frame as swapped out and save the swap identifier in the mapping
database.  At this point, the task may reuse the frame.  This example
illustrates that imagining a virtual frame as bound to a page in a
task's address space for its entire lifetime is incorrect.  It should
also now be clear that when the data is eventually brought back into
memory from backing store, it may reside in a different virtual frame
(as well as a different physical frame).

Containers are used for passing data between tasks.  Typically there
will be two tasks, a client and a server.  L4 provides a mechanism to
map pages from one address space to another.  This mechanism could be
used to e.g. map a file into a client task's address space.  An
analysis reveals several problems with this approach.  If the server
dies before the client, the mappings in the client's address space
will suddenly disappear.  Similarly, if the server is malicious, it
may revoke the mappings at some inconvenient (i.e. unrecoverable) time
causing the client to crash or unable to inform the user of the
change.  Also, if a server allocates resources on behalf of the the
client it becomes impossible to do system wide resource accounting as
many servers are not trusted by the system.  All of these problems are
solved by containers.  When a client needs to read data from a server,
it creates a container, adds the number of frames that the server will
require for the operation to it and finally shares the container with
the server.  After sending a request to the server, the server copies
the data into the provided container.  It is important to understand
that the server does not ``fill'' the container: the number of frames
remains constant; the state of the bits changes.  When the server
returns to the client, the client revokes the share and is now able to
map the frames into its address space by contacting the physical
memory server.  Should the server die, the client remains uneffected
as the data lives in the physical memory server.  The physical memory
server is also trusted thus if a task is malicious, it can only be
malicious during the initial copy of the data into the container,
i.e. before the client starts using the data and thereby giving the
client the opportunity to report an inconsistencies to the caller.
Finally, as the resources are allocated by the client via system
servers, global resource accounting is possible.

\subsection{The Container Interface}

\paragraph{Creating Containers}

A container may be created using:

\begin{code}
error\_t pm\_container\_create (out container\_t container)
\end{code}

A container\_t is, for all intents and purposes, a hurd\_cap\_t.  If a
container is shared with another task, the second task may allocate
frames which count against the container's owner's total allocated
pages.  This must be used with care.

\paragraph{Sharing Containers}

To allow another task to access the contents of a container, the
container must be shared.  Clearly, it is not desirable to grant full
access to the container to the remote task: trust between a client and
a server must exist, however, that trust is typically limited in both
directions (neither the client trusts the server fully nor does the
server fully trust the client).  Since clients provide server with the
resources for the operation, servers need a guarantee that the client
will not touch the resources while it is in a critical section.
Horrific results can emerge if this happens during a DMA operation.
Likewise, clients need to have the ability to cancel an exant request
and reclaim shared resources if the server does not answer in a timely
manner thereby also preventing the server from being able to steal
resources.  In both of these cases, the physical memory server acts as
the trusted third party.  The physical memory server allows a server
to lock a container for a limited amount of time during which the
client may not access or destroy the resource.  At any other time, the
client can cancel the server's access to the shared resource.

To facility this, a second class capability is provided to access
containers.  Using this capability, clients may not allocate or
deallocate frames.

\begin{code}
error\_t pm\_container\_share (in container\_t container, in task\_t
remote, out container\_t weak\_ref)
\end{code}

\noindent
\variable{weak\_ref} can be passed to the sharee using the normal
capability passing protocol.

\paragraph{Allocating and Deallocating Memory}

Virtual frames may be allocated into a container using:

\begin{code}
error\_t pm\_container\_allocate (in container\_t container, in
frame\_t start, in out int count, in int flags)
\end{code}

\noindent
\variable{start} is the first frame identifier to use for the new
memory.  If \variable{count} is greater than one then frames will be
allocated in the subsequent $count - 1$ frame identifiers.  The number
of frames actually allocated is returned in \variable{count}.  If an
identifier already references a virtual frame, \errno{EEXIST} is
returned.  \variable{flags} is a bitwise or of:
\constant{CONT\_ALLOC\_PARTIAL}, \constant{CONT\_ALLOC\_SQUASH} and
\constant{CONT\_ALLOC\_EXTRA}.  If \constant{CONT\_ALLOC\_PARTIAL} is
set and the number of frames which can be allocated before a memory
allocation error occurs is greater than one but less than
\variable{count} then the maximum number of frames is allocated, count
is set to that number and the error is returned.  If
\constant{CONT\_ALLOC\_PARTIAL} is not set then partial allocations
will fail, count will be set to 0 and an error will be returned.  If
\constant{CONT\_ALLOC\_SQUASH} is set and a frame identifier already
references a frame, the virtual frame will be dropped and its contents
lost.  Using this flag is dangerous and be a sign of internal
inconsistencies in the task!  All virtual frames should be accounted
for by the task and deallocated explicitly.  If
\constant{CONT\_ALLOC\_EXTRA} is set then extra frames may be
allocated otherwise the physical memory server will only allocate up
to the guaranteed virtual frame limit.  This flag should only be used
by tasks able to handle the added complexity of the extra frame
protocol.  The contents of allocated frames is undefined.

% When obtaining data from a server (e.g. reading from a file), tasks
% will: create a container, fill it with anonymous memory and share the
% container with the server.  Since this is a very common operation, a
% short cut has been provided to which combines the three operations:

Deallocating memory is done using:

\begin{code}
error\_t pm\_container\_deallocate (in container\_t container, in
frame\_t start, in out int count, in int flags)
\end{code}

\noindent
The arguments have similar meaning as those in
\function{pm\_container\_allocate}.  \constant{CONT\_DEALLOC\_PARTIAL}
and \constant{CONT\_DEALLOC\_SQUASH} are similar to
\constant{CONT\_ALLOC\_PARTIAL} and \constant{CONT\_ALLOC\_SQUASH}
respectively.

\paragraph{Mapping Memory}

The physical memory server guarantees that a mapping operation takes a
short amount of time: no guarantee is made that this will happen
immediately as the underlying physical frames may have to be allocated
in which case the physical memory server may have to be reap physical
pages from other tasks' extra frame allocations.

The physical memory server may unmap pages at any time.  This allows
the physical memory server to fucntionally lock the contents of the
frame and move it to a new physical frame.  As such, tasks must be
prepared to reestablish a mapping with the physical memory server at
anytime.  The physical memory server is not a registry of mappings: it
is a cache.

Read-only mappings may be returned when read/write mapping are
requested: the physical memory server will never grant a read/write
mapping if the frame is marked copy on write.  In order to obtain a
read/write mapping (and thus force the copy on write), the task must
add the enforced write flag to the mapping request.

\begin{code}
error\_t pm\_container\_map (in container\_t container, in frame\_t
start, in int nr\_frames, in int flags)
\end{code}

\noindent
Flags may is a bitwise or of: \constant{CONT\_MAP\_READ},
\constant{CONT\_MAP\_WRITE} and \constant{CONT\_MAP\_FORCE\_WRITE}.
\constant{CONT\_MAP\_FORCE\_WRITE} will only be respected if
\constant{CONT\_MAP\_WRITE} is also set.

\paragraph{Doing It All At Once}

When reading to or writing data from a server, the task will normally:
allocate a new container, fill it with memory and share the container
with the server.  Since this is such a common operation, short cuts
are provided to reduce the required number of rpcs:

\begin{code}
error\_t pm\_container\_create\_with (out container\_t container, in
in int frame\_count, out container\_t weak\_ref)
\end{code}

\begin{code}
error\_t pm\_container\_create\_from (out container\_t container, in
container\_t source, in frame\_t start, in int count, out container\_t
weak\_ref)
\end{code}

\begin{code}
error\_t pm\_container\_create\_grather (out container\_t container,
in container\_t source, in frame\_t [] frames, out container\_t
weak\_ref)
\end{code}

\paragraph{Copying Data Into or Out of Containers}

It is possible to copy data into containers by mapping the frames in
question and using \function{memcpy}.  If this technique is used there
is no easy way to create logical copies (copy on write): an especially
important technique for sharing executable and shared library text.  A
family of functions are available which logically copies the contents
of one container to another:

\begin{code}
error\_t pm\_container\_copy (in container\_t src, in frame\_t
src\_start, in countainer\_t dest, in frame\_t dest\_start, in int
frame\_count, out frame\_t frame\_error)
\end{code}

\begin{code}
error\_t pm\_container\_copy\_scatter (in container\_t src, in
frame\_t src\_start, in countainer\_t dest, in frame\_t []
dest\_frames, out frame\_t frame\_error)
\end{code}

\begin{code}
error\_t pm\_container\_copy\_gather (in container\_t src, in frame\_t
[] src\_frames, in countainer\_t dest, in frame\_t dest\_start, out
frame\_t frame\_error)
\end{code}

\begin{code}
error\_t pm\_container\_copy\_scatter\_gather (in container\_t src, in
frame\_t [] src\_frames, in countainer\_t dest, in frame\_t []
dest\_frames, out frame\_t frame\_error)
\end{code}

If a frame does not exist in the source, ENOENT.  If a frame does not
exist in the destination, ENOMEM is returned.  In both cases, the
frame identifier causing the error is returned in
\variable{frame\_error}.

\paragraph{Locking Containers and Pinning Memory}

\paragraph{Finding Deallocate Memory}

\paragraph{Reusing frames}

release\_data

\subsection{Moving Data}

Data will be moved around using containers.  Describe how to read and
write.  Task -> FS -> Device drivers.  Locking memory.  Caching.

It is important that clients do the allocation for the memory which
they use: not the servers doing allocations on behalf on clients: in
the latter, there is no way to do resource tracking.

Discuss mmap: local function call.  RPC is done when a page is
faulted: do a read from the fs (into a container), then map the data
from the container into the AS as required.

MAP\_COPY sucks: fs must save all modified data.  What happens when a
100MB file is completely rewritten (or 1GB, etc)?  can we use upcalls?
If we do, the fs still needs to hold the data in the intern.  Can we
copy the file on disk and use that as backing store (think how
deleting an open file works).

Can a readonly private mapping once faulted be dropped or must we
promote it to anonymous memory and send it to swap fearing that the
underlying block might change between dropping it and rereading it
(e.g. by another task modifying the file)?

\section{Caching Store Accesses}

It need not be explained how caching accesses to stores can radically
improve the speed of the system.  In a monolithic kernel this cache is
added to by the readers, i.e. the device drivers, supplemented with
metadata from the file systems in the form of expected access patterns
based on the type of data and how the file was opened and managed by
the virtual memory manager.  In our design, this is impossible: each
component---each device driver, each file system and the physical
memory manager---all live in their own address spaces; additionally
there will rarely be mutual trust: the physical memory server may not
trust the file systems nor the ``device drivers'' (consider a network
block device).  A caching mechanism must be designed.

The purpose of caching is useful for multiple readers of a given
block.  Sometimes this is the same task, however, more often it is
multiple tasks.  Thus, having the caching scheme in each task is quite
difficult as tasks do not trust one another and furthermore, tasks can
die at any time thereby dropping their cache.  The logical place to
put the cache then is the common point of access, the file system.

An argument could be made that in reality, the common point of access
is the device driver: there can be multiple accessors of the same
store.  The question must be asked: what happens when the device
driver is made the cache point instead of the file system?  Logically,
a large tradeoff is made in terms of the ability to intelligently
decide what frame to keep in the cache.  The file system, for
instance, has meta-data about how a given frame may be used based on
how a file is opened and may realize that some frames need not be
placed in the cache because they will be used once and immediately
discarded.  This is true of the access patterns of multimedia
applications.  These types of hints may be gathered at file open time.
The class of data is another way the file system is able to predict
usage, for example, it understands the difference between
meta-data---inodes and directories---and file data.  A file system is
also able to anticipate file-level access patterns whereas a device
driver can only anticipate block-level access patterns, i.e. although
file data is sometimes sequential, it is often scattered across a
section of the disk due to fragmentation.  The primary way a the
device driver can really manage its cache is through historical data
in the form of previous accesses (which is itself even more limited as
the device driver is uninformed of cache hits in the file system
cache).  This type of data implies some form of LRU, least recently
used, eviction scheme.  It should now be clear that the file system
can make smarter decisions about what which blocks to evict due to its
ability to make predictions based on client hints and its greater
understanding of the data in the store.

If we resign ourselves to keeping the cache only in the file system,
then multiple users of a store will be penalized greatly: a block read
by one client will always be reread if another client requests the
same block: not only is the store accessed a second time, but twice as
much memory will be used as there is no way to share the frame and use
copy on write.  Is this penalty worth the added intelligence in the
file system?  An argument can be made that using just one caching
strategy is suboptimital when we could just have two: nothing stops
both the file system and the device driver from caching thereby
permitting the former to continue to maintain an intelligent cache and
the device driver to have its simple LRU cache.  This argument
overlooks several important implications of having the two caches.
First, complexity is being added to the device driver in the form of a
list of frames it has read and given out.  This increase in memory
usage has a secondary effect: if the data structures become large (as
it certainly will for large active stores), it will be impossible to
keep the device driver in question in a small address space (an
important optimization on architectures without tagged TLBs, table
look aside buffers).  Second, if both the file system and the device
driver keep a cache, when the file system has a cache miss, the device
driver then checks its cache before going to disk.  The device driver
will only ever have a cache hit if there are multiple readers: when
there is a single user of a store, the file system's cache and the
device driver's cache will be identical.  This begs the question: how
often will there be multiple users of a single store?  The answer
seems to be very rarely: assuming the common case that the store has
some type of file system on it, there can only be multiple users if
all users are readers (note that not even one can be a writer as this
implies cache consistency issues across different users of the store).
Since this is a very rare case, we argue based on the philosophy ``do
not optimize for rare cases'' that the overhead is greater than the
potential pay back from the optimization.  Having multiple caches
leads to a further problem: a frame is really not evicted from the
system until it is purged from all caches.  Thus if the file system
cache is smart and chooses the better frames to evict, the
cooresponding physical frames will not really be freed until the
device driver also drops its references to the frames.  Thus, the
effectiveness of the smarter caching algorithm is impeded by the
device driver's caching scheme.  Double caching must be avoided.

\subsection{Caching in the File System}

We have argued above that all block caching will be done at the file
system layer.  In this section, we detail how the caching will work.

The file system allocates extra frames as long as it can and adds all
eligible frames to the cache by logically copying them into a local
container (data which it reasons will be read once and then dropped
may not be considered eligible).  When the physical memory server
wants frames back, it chooses a victim with extra frames and asks for a
subset of them back.  If a task has $G$ guaranteed frames and $G + E$
frames allocated, the physical memory server can request up to $E$
frames back from the task.  We recall from the definition of the extra
frames that extra frames must be given back quickly (i.e. there is no
time to send them to swap).

Although a task chooses a frame to evict from its cache, it does not
mean that the frame will be reused immediately, in fact, it is
sometimes that case that the frame cannot be reused at all as another
task has a reference to the frame (in the form of a logical copy).  As
such, it would be nice to be able to get frames back that might still
be in the physical memory server.  The following mechanism is thus
provided: when a frame is returned to the physical memory server, the
reference to the frame is turned into a soft reference.  Only when the
frame is actually reused by the physical memory server are soft
references discarded.  A task is able to convert a soft reference back
to a hard reference by contacting the physical memory server and
asking for the frame back.  If this operation returns
\errno{ENOEXIST}, the frame has been reused and the frame must be
remanufactured (e.g. by retrieving it from backing store).  This
operation may also fail and return \errno{ENOMEM} if the task does not
have enough guaranteed frames and there are no extra frames available.

\begin{comment}
There is a problem here in the form of name space pollution: the task
doing the caching has to remember the mapping of blocks to container
identifiers in order to recover the soft reference but the task has no
way to know when the physical memory server expires a given soft
reference.  Thus, while the physical memory server may drop a frame,
the task will only ever know this when it tries to convert the soft
reference to a hard reference and fails (i.e. gets a cache miss).  For
frames which this is never done, the memorized mapping will never be
invalidated.  This may not be a problem if a block offset to container
id is used, however, if hashing is done or some other mapping of block
offsets to container identifiers is used, this will pollute the cache
container's name space.
\end{comment}

\subsection{Caching Interfaces}

The physical memory server will do an up call to a victim task
requesting a number of frames back.  The physical memory server may do
this at any time for any reason and it expects to receive the frames
back from the task within a short amount of time (the victim task
should not expect to be able to send the frames to backing store in
that amount of time).  The physical memory server will never request
guaranteed frames.  As such, this number will always be less than or
equal to the number of allocated frames minus the number of guaranteed
frames.

\begin{code}
void pm\_return\_frames (in int count);
\end{code}

The physical memory send this message to the task's memory control
thread.  The thread must always be ready to receive: the physical
memory server will never wait (thus, the thread must be in the
receiving state).  If the thread is not ready, the physical memory
server assumes that the task is misbehaving.  The physical memory
server does not wait for a reply, instead, the client must free the
frames using \function{pm\_release\_frames} as described above.

\section{The Memory Policy Server}

At task creation time, the task must negotiate a medium-term contract
for guaranteed frames and determine if it shall have access to extra
frames.  This may be renegotiated later.  It must be renegotiated when
the contract expires.  The policy server will give the task enough
time to send frames to swap before committing if the number of
guaranteed frames is reduced.

\section{Sending Data to Swap}

When a task reaches its guaranteed frame allocation, it must begin to
reuse its available virtual frames.  If the data is frames is precious
(i.e. not easliy constructed by e.g. a calculation or by rereading a
file) then the task will want to save the contents for when it is
needed in the future.  This can be done by sending a frame to backing
store.

\begin{code}
error\_t pm\_swap (in container\_t c, in container\_frame\_t frame, in int
count, out [] swap\_ids)
\end{code}

The swap server resides in (or is proxied by) the phsyical memory
server.  This allows the logical copies of frames to be preserved
across the swapped out period (i.e. logical copies are not lost when a
frame is sent to swap).  If this was not the case, then when a number
of tasks all with a reference to a given physical send the frame to
swap, the swap server would allocate and write N times as opposed to
once when all of the tasks eventually release any references to the
frame.

Frame may not be sent to swap immediately.  Instead, they are kept on
an inactive list allowing thereby allowing a task to recover the
contents of a frame before it is flushed to swap (that is to say, swap
operations are not synchronous).

Since there may be multiple references to a virtual frame, it is
recommended that \function{pm\_container\_orphan\_data} be called
before the frame is reused to prevent gratuitous copy on writes from
begin performed.  It also important to call this function if the frame
was being used for shared memory.

Swap quotas (put the policy in the memory policy server).

\section{Self Paging}

As already explained, tasks are self-paged.  The default
implementation provided with the hurd has each thread in a task set
its pager (i.e. its fault handler) to a common pager thread in the
same address space.  This thread maintains a mapping database which
associates virtual addresses with either a frame of memory in a
container or information on how to retrieve the data, e.g. from swap
or a file server.

Normally, there is a single primary container for virtual frames that
is created at start up.  A task may choose to use more containers and
generally will for short periods of time (for instance, for reading to
and writing from servers).  The pager must always be able to handle
multiple containers.  When using additional containers, frames need to
be added to them to be shared with the server.  The pager must provide
a mechanism to allow the caller to steal guaranteed frames for this
purpose and return them upon deallocation of the container.

\subsection{The Pager}

The pager itself may require a fair amount of memory for its database
and all of the code and supporting libraries.  This presents a
problem: if the pager handles page faults, who will handle its faults?
One of two solutions are possible: either all of the text and data
must be wired into memory (thereby reducing the number of frames
available for multiplexing application memory) or the pager is itself
paged.  The default self-pager implementation uses the latter option:
the pager, now referred to as the primary pager, is backed by a final
pager.  The final pager only maps the pagers text and data thus it has
a significantly smaller memory footprint.  Care must be taken to be
sure that the primary pager does not accidently allocate memory from
common memory pools: in the very least it needs its own private
\function{malloc} arena.  As the primary pager will call, for
instance, routines to manipulate capabilities, this text must be
backed by the final pager.  Other code can also, however, makes calls
to the capability library.  This means that the primary pager must
also have a copy of these mappings in its database.

The purpose of the final pager is to allow the data and some of the
text of the primary pager to be swapped.  As such, the final pager
must be able to at least read data from file servers and retrieve data
from backing store.  This may imply significant overlap of the text
for the primary and final pagers.  In some case, however, it may be
useful to have a second implementation of a function only for the
final pager which is optimized for size or avoids making calls to
certain libraries.

\subsubsection{Managing Mappings}

Mappings are normally made via calls to \function{mmap}.  Unlike in
Unix, this is not a system trap: instead it is almost always
implemented locally.  \function{mmap} must associate a region with
either anonymous memory or with a file on disk.  This is only a matter
of creating a few entries in the mapping database so that faults will
brings the data in lazily.

Rather than have the caller manipulate the mapping database directly,
instead, a local ipc sent to the primary pager.  If there is only ever
a single thread which manipulates the mapping database, there will be
locking requirements.  If the pager thread is busy, then the local ipc
call blocks in the kernel.

It is not always useful to fault memory in lazily: when a task has
received data from a server, it will normally be in a container from
where it must be consumed.  The task will generally map the container
into memory and then proceed to use or at least copy the data to some
other location.  Clearly, the faulting the pages in is a waste.  As
such, the pager should provide a mechanism which allows the caller to
not only establish a mapping from a container but also to map the
pages immediately in the address space.

\subsection{Reusing Virtual Frames}

Multiplexing frames: say the contents of a frame are sent to swap in
order to reuse the frame for something else.  The frame itself must be
cleared, i.e. disassocitated with any logical copies.  This is done
using:

\begin{code}
error\_t pm\_release\_data (in pm\_container\_t container, in pm\_frame\_t[] frames)
\end{code}

\subsection{Taking Advantage of Self-Paging}

extend malloc via e.g. the slab mechanism, extend fopen (how a file is
used).

% Traditionally, monolithical kernels, but even kernels like Mach,
% provide a virtual memory management system in the kernel.  All paging
% decisions are made by the kernel itself.  This requires good
% heuristics.  Smart paging decisions are often not possible because the
% kernel lacks the information about how the data is used.
% 
% In the Hurd, paging will be done locally in each task.  A physical
% memory server provides a number of guaranteed physical pages to tasks.
% It will also provide a number of excess pages (over-commit).  The task
% might have to return any number of excess pages on short notice.  If
% the task does not comply, all mappings are revoked (essentially
% killing the task).
% 
% A problem arises when data has to be exchanged between a client and a
% server, and the server wants to have control over the content of the
% pages (for example, pass it on to other servers, like device drivers).
% The client can not map the pages directly into the servers address
% space, as it is not trusted.  Container objects created in the
% physical memory server and mapped into the client and/or the servers
% address space will provide the necessary security features to allow
% this.  This can be used for DMA and zero-copying in the data exchange
% between device drivers and (untrusted) user tasks.
% 
% 
