\chapter{Virtual Memory Management}

\begin{quote}
\emph{The mind and memory are more sharply exercised in comprehending
another man's things than our own.}

\begin{flushright}
\emph{Timber} or \emph{Discoveries} by Ben Jonson
\end{flushright}
\end{quote}


\section{Introduction}

The goal of an operating system is simply, perhaps reductively,
stated: manage the available resources.  In other words, it is the
operating system's job to dictate the policy for obtaining resources
and to provide mechanisms to use them.  Most resources which the
operating system manages are sparse resources, for instance the CPUs,
the memory and the various peripherals including graphics cards and
hard drives.  Any given process, therefore, needs to compete with the
other processes in the system for some subset of the available
resources at any given time.  As can be imagined, the policy to access
and the mechanisms to use these resources determines many important
characteristics of the system.

A simple single user system may use a trivial first come first serve
policy for allocating resources, a device abstraction layer and no
protection domains.  Although this design may be very light-weight and
the thin access layer conducive to high speed, this design will only
work on a system where all programs can be trusted: a single malicious
or buggy program can potentially halt all others from making progress
simply by refusing to yield the CPU or allocating and not releasing
resources in a timely fashion.

The Hurd, like Unix, aims to provide strong protection domains thereby
preventing processes from accidentally or maliciously harming the rest
of the system.  Unix has shown that this can be done efficiently.  But
more than Unix, the Hurd desires to identify pieces of the system
which Unix placed in the kernel but which need not be there as they
could be done in user space and provide additional user flexibility.
Through our experience and analysis, we are convinced that one area is
much of the virtual memory system: tasks are often allocating as much
memory without regard---because Unix provides them with no mechanism
to do so---for the rest of the system.  But it is not a cooperative
model which we wish to embrace but a model which holds the users of
the resource responsible for it and when asked to release some of its
memory will or violate the social contract and face exile.  Not only
will this empower users but it will force them to make smarter
decisions.

\subsection{Learning from Unix}

Unix was designed as a multiuser timesharing system with protection
domains thereby permitting process separation, i.e. allowing different
users to concurrently run processes in the system and gain access to
resources in a controlled fashion such that any one process cannot
hurt or excessively starve any other.  Unix achieved this through a
monolithic kernel design wherein both policy and mechanism are
provided by the kernel.  Due to the limited hardware available at the
time and the state of Multics\footnote{Multics was seen as a system
which would never realize due to its overly ambitious feature set.},
Unix imposed a strong policy on how resources could be used: a program
could access files, however, lower level mechanism such as the file
system, the virtual file system, network protocol stacks and devices
drivers all existed in the kernel proper.  This approach made sense
for the extremely limited hardware that Unix was targeted for in the
1970s.  As hardware performance increased, however, a separation
between mechanism and policy never took place and today Unix-like
operating systems are in a very similar state to those available two
decades ago; certainly, the implementations have been vastly improved
and tuned, however, the fundamental design remains the same.

One of the most important of the policy/mechanism couplings in the
kernel is the virtual memory subsystem: every component in the system
needs memory for a variety of reasons and with different priorities.
The system must attempt to meet a given allocation criteria.  However,
as the kernel does not and cannot know how how a task will use its
memory except based on the use of page fault statistics is bound to
make sub-ideal eviction decisions.  It is in part through years of
fine tuning that Unix is able to perform as well as it does for the
general applications which fit its assumed statistical model.

\subsection{Learning from Mach}

The faults of Unix became clear through the use of Mach.  The
designers of Mach observed that there was too much mechanism in the
kernel and attempted to export the file systems, network stack and
much of the system API into user space servers.  They left a very
powerful VMM in the kernel with the device drivers and a novel IPC
system.  Our experience shows that the VMM although very flexible, is
unable to make smart paging decisions: because Unix was tied to so
many subsystems, it had a fair knowledge of how a lot of the memory in
the system was being used.  It could therefore make good guesses about
what memory could be evicted and not be needed in the near future.
Mach, however, did not have this advantage and relied strictly on page
fault statistics and access pattern detection for its page eviction
policy.

Based on this observation, it is imperitive that the page eviction
scheme have good knowledge about how pages are being used as it only
requires a few bad decisions to destroy performance.  Thus, a new
design can either choose to return to the monolithic design and add
even more knowledge to the kernel to increase performance or the page
eviction scheme can be remove from the kernel completely and placed in
user space and make all tasks self paged.

\subsection{Following the Hurd Philosophy}

As the Hurd aims, like Unix, to be a multiuser system for mutually
untrusted users, security is an absolute necessity.  But it is not the
object of the system to limit users excessively: as long as operations
can be done securely, they should be permitted.  It is based on this
philosophy that we have adopted a self paging design for the new Hurd
VMM: who knows better how a task will use its memory than the task
itself?  This is clear from the problems that have been encountered
with LRU, the basic page evition algorithm, by database developers,
language designers implementing garbage collectors and soft realtime
application developers such as multimedia developers: they all wrestle
with the underlying operating system's page eviction scheme.  By
putting the responsibility to page on tasks we think that tasks will
be forced to make smart decisions as they can only hurt themselves.

\section{Memory Allocation}

If memory was infinite and the only problem was worrying about one
program accessing the memory of another, memory allocation would be
trivial.  This is not, however, the case: memory is visibly finite and
a well designed system will exploit it all.  As memory is a system
resource, a system wide memory allocation policy must be established
which maximizes memory usage according to a given set of criteria.

In a typical Unix-like VMM, allocating memory (e.g. using
\function{sbrk} or \function{mmap}) does not allocate physical memory
but \keyword{virtual memory}.  In order to increase the amount of
memory available to users, the kernel uses a \keyword{backing store},
typically a hard disk, to temporarily free physical memory thereby
allowing other processes to make progress.  The sum of these two is
referred to as virtual memory.  The use of backing store ensures data
integrity when physical memory must be freed and application
transparency is required.  A variety of criteria are used to determine
which frames are \keyword{paged out}, however, most often some form of
a priority based least recently used, LRU, algorithm is applied.  Upon
\keyword{memory pressure}, the system steals pages from low priority
processes which have not been used recently or drain pages from an
internal cache.

This design has a major problem: the kernel has to evict the pages but
only the applications know which pages they really need in the near
term.  The kernel could ask the applications for this data, however,
it is unable to trust the applications as they could, for instance,
not respond, and the kernel would have to forcefully evict pages
anyway.  As such, the kernel relies on page fault statistics to make
projections about how the memory will be used, thus the LRU eviction
scheme.  An additional result of this scheme is that as applications
never know if mapped memory is in core, they are unable to make
guarantees about deadlines.

These problems are grounded in the way the Unix VMM allocates memory:
it does not allocate physical memory but virtual memory.  This is
illustated by the following scenario: when a process starts and begins
to use memory, the allocator will happily give it all of memory in the
system as long as no other process wants it.  What happens, however,
when a second memory hungry process starts is that the kernel has no
way to take back memory it allocated to the first process.  At this
point, it has two options: it can either return failure to the second
process or it can steal memory from the first process and send it to
backing store.

One way to solve these problems is to have the VMM allocate phsyical
memory and make applications completely self-paged.  Thus, the burden
of paging lies the application themselves.  When application request
memory, they no longer request virutal memory but physical memory.
Once the application has exhausted its available frames, it is its
responsibility to multiplex the available frames.  Thus, virtual
memory is done in the application itself.  It is important to note
that a standard manager or managers should be supplied by the
operating system.  This is important for implementing something like a
POSIX personality.  This should not, however, be hard coded: certain
application may greatly benefit by being able to control their own
eviction schemes.  At its most basic level, hints could be provided to
the manager by introducing extentions on basic function calls.  For
instance, \function{malloc} could take an extra parameter indicating
the class of data being allocated.  These class would provide hints
about the expected usage pattern and life time of the data.

\subsection{Bootstrap}

When the Hurd starts up, all physical memory is eventually transfered
to the physical memory server by the root server.  At this point, the
physical memory server will control all of the physical pages in the
system.

\subsection{Allocation Policy}

\subsubsection{Guaranteed Pages and Extra Pages}

The physical memory server maintains a concept of \keyword{guaranteed
pages} and \keyword{extra pages}.  The former are pages that a given
task is guaranteed to map in a very short amount of time.  Given this
predicate, the total number of guaranteed pages can never exceed the
total number of frames in the system.  Extra pages are pages which are
given to clients who have reached their guaranteed page allocation
limit.  The physical memory server may request that a client
relinquish a number of extant extra pages at any time.  The client
must return the pages to the physical memory (i.e. free them) in a
short amount of time.  The task should not assume that it has enough
time to send pages to backing store.  As such, extra pages should only
contain remanufacturable data (i.e. cached data).  Should a task fail
to return the pages in a reasonable amount of time, it risks having
all of its memory dropped---not swapped out or saved in anyway---and
reclaimed by the physical memory server.  Note that the physical
memory server does not know if a given page is considered guaranteed
or extra: it knows that a given task has $G$ guaranteed pages and $G +
E$ allocated pages, it has $E$ extra pages.  The distinction between
guaranteed and extra pages must be made by the task itself.  One
strategy is to remember which pages can be remanufactured (e.g. reread
from disk or recalculated) and promote them to guaranteed pages when
the page becomes dirty being careful to never have less than $E$ clean
pages in the task.  Given these semantics, guanteed pages should not
be thought of as wired (e.g. \fuction{mlock}ed in the POSIX
sense)---although they can have this property---but as frames which
the task itself must multiplex.  Thus the idea of self-paged tasks.

Readers familiar with VMS will see striking similarities with the
self-paging and guaranteed page paradigms.  This is not without
reason.  Yet, differences remain: VMS does not have extra pages and
the number of guaranteed pages is fixed at task creation time.  Pages
returned to VMS (in order to allocate a new page) are placed in a
dirty list (thus the actual multiplexing of frames is done in VMS, not
in user space) thereby simulating a two level backing store: a fast
memory backing store where pages are waylaid and swap, where they are
sent when memory pressure forces out.  It is in this way that a given
task may get at more than its quota of memory when there is low memory
contention.  Our divergence from VMS is motivated by the location of
file systems and device drivers in the Hurd: unlike in VMS, the file
systems and device drivers are in user space.  Thus, the caching that
was being done by VMS cannot be done intelligently by the physical
memory server.

\subsubsection{An External Memory Policy Server}

The number of guaranteed pages that a given task has access to is not
determined by the physical memory server but by the \keyword{memory
policy server}.  This division allows the physical memory server to
concern itself primarily with the allocation mechanisms and delegate
all the policy decisions to the underlying operating system.  (An
important implication is that although tailored for Hurd specific
needs, the physical memory server is completely separate from the Hurd
and can be used by other operating systems running on the L4
microkernel.)  It is the memory policy server's responsibility to
determine who gets how much memory.  This may be determined as a
function of the user or looking in a file on disk for e.g. quotas.  As
can be seen this type of data acquisition could add significant
complexity to the physical memory server and require blocking states
(e.g. waiting for a read operation on file i/o) and could create
circular dependencies.  The default memory policy server's mechanisms
and policies will be discussed later.

The physical memory server and the memory policy server will contain a
shared buffer of tupples indexed by task id containing the number of
allocated pages, the number of guaranteed page, and a boolean
indicating whether or not this task is eligible for guaranteed pages.
The guaranteed page field and the extra page predicate may only be
written to by the memory policy server.  The number of allocated pages
may only be written to by the physical memory server.  This scheme
means that no locking in required.  (On some architectures where a
read of a given field cannot be performed in a single operation, the
read may have to be done twice.)  The memory policy server must not
over commit the number of frames, i.e. the total number of guaranteed
pages must never exceed the number of frames avilable for allocation.

Until the memory policy server makes the intial contact with the
physical memory server, memory will be allocated on a first come first
serve basis.  The memory policy server shall use the following remote
procedure call to contact the physical memory server:

\begin{code}
error\_t pm\_get\_control (out hurd\_cap\_t control)
\end{code}

\noindent
This function will succeed the first time it is called and return a
control capability.  It will fail all subsequent times.  By using a
capability, the acquiring task may move or copy the capability to
another task.  This permits replacing the policy server on a live
system.  At this point, the physical memory server will begin
allocating memory according to the previously described protocol.
Note that the inital buffer will be initialized with the current total
allocations but the guaranteed pages will be set to zero.  The memory
policy server must request the shared policy buffer as soon as
possible and adjust these values.

The shared policy buffer may be obtained from the physical memory
server by the policy by calling:

\begin{code}
error\_t pm\_get\_policy\_buffer (out l4\_map\_t buffer)
\end{code}

\noindent
The returned buffer is mapped with read and write access into the
policy memory server's address space.  It may need to be resized due
to the number of tasks in the system.  When this is the case, the
physical memory server shall unmap the buffer from the memory policy
server's address space and copy the buffer internally as required.
The memory policy server will fault on the memory region on its next
access and it may rerequest the buffer.  This call will succeed when
the sender is the memory policy server, it will fail otherwise.

\subsection{Allocation Mechanisms}

Applications are able allocate memory into containers.  Containers may
be created using:

\begin{code}
error\_t pm\_container\_create (out container\_t container)
\end{code}

Memory allocation does not allocate physical frames: if so, it would
be impossible to move memory around and memory would have to be
returned to the same spot after being swapped out and back in.
Containers are useful for grouping and then moving memory around.

How to get pages.  Type of pages (e.g. DMA) or fixed physical address.

Memory is not allocate until map time (and not always then,
e.g. logical copies).

\section{Mapping Memory from Containers}

A map will occur in a short amount of time: this is not guaranteed to
happen immediately as the pages are only allocated at this point.
They may currently be in the form of extra pages for another task.

Pages may be unmapped at any time by the physical memory server.  This
allows pages to be moved in memory by the physical memory server
thereby permitting page blocking (i.e. construct superpages), opening
up DMAable memory areas or other specific physical address, and to
create contiguous blocks of memory (i.e. to defragment the memory).
The task must always be read to reestablish a mapping.

Mapping can be made readable.  A task may request that a mapping be
read/write, however, the physical memory server may not grant a
read/write mapping if the page is copy on write, COW.  In this case, a
read mapping will be returned.  In order to get a read/write mapping,
the task must request add the enforced flag.

\section{Moving Data}

In a monolithic kernel, little data is exchanged between tasks.  In a
multiserver system, file systems live in their own tasks and thus
reading and writing involve servers.  Thus, powerful primatives for
moving memory around with the least number of physical copies,
i.e. using virtual copy mechanisms which preserve COW pages, etc.

It is important that an fs does not map from one task to another
directly: the client may not trust the source or the source may die,
etc.  Doing the mapping via the phys memory server means all trust
issues are resolved at the time of mapping and can be reported to the
user: the fs cannot pretend to be nice and then revoke mappings and
silently harm the client.

Data will be moved around using containers.  Describe how to read and
write.  Task -> FS -> Device drivers.  Locking memory.  Caching.

It is important that clients do the allocation for the memory which
they use: not the servers doing allocations on behalf on clients: in
the latter, there is no way to do resource tracking.

Discuss mmap: local function call.  RPC is done when a page is
faulted: do a read from the fs (into a container), then map the data
from the container into the AS as required.

MAP_COPY sucks: fs must save all modified data.  What happens when a
100MB file is completely rewritten (or 1GB, etc)?  can we use upcalls?
If we do, the fs still needs to hold the data in the intern.  Can we
copy the file on disk and use that as backing store (think how
deleting an open file works).

Can a readonly private mapping once faulted be dropped or must we
promote it to anonymous memory and send it to swap fearing that the
underlying block might change between dropping it and rereading it
(e.g. by another task modifying the file)?

\section{Caching Store Accesses}

It need not be explained how caching accesses to stores can radically
improve the speed of the system.  In a monolithic kernel this cache is
added to by the readers, i.e. the device drivers, supplemented with
metadata from the file systems in the form of expected access patterns
based on the type of data and how the file was opened and managed by
the virtual memory manager.  In our design, this is impossible: each
component---each device driver, each file system and the physical
memory manager---all live in their own address spaces; additionally
there will rarely be mutual trust: the physical memory server may not
trust the file systems nor the ``device drivers'' (consider a network
block device).  A caching mechanism must be designed.

The purpose of caching is useful for multiple readers of a given
block.  Sometimes this is the same task, however, more often it is
multiple tasks.  Thus, having the caching scheme in each task is quite
difficult as tasks do not trust one another and furthermore, tasks can
die at any time thereby dropping their cache.  The logical place to
put the cache then is the common point of access, the file system.

An argument could be made that in reality, the common point of access
is the device driver: there can be multiple accessors of the same
store.  The question must be asked: what happens when the device
driver is made the cache point instead of the file system?  Logically,
a large tradeoff is made in terms of the ability to intelligently
decide what pages to keep in the cache.  The file system, for
instance, has meta-data about how a given page may be used based on
how a file is opened and may realize that some pages should not be
placed in the cache because they will be used once and immediately
discarded.  This is true of the access patterns of multimedia
applications.  These types of hints may be gathered at file open time.
The class of data is another way the file system is able to predict
usage, for example, it understands the difference between
meta-data---inodes and directories---and file data.  A file system is
also able to anticipate file-level access patterns whereas a device
driver can only anticipate block-level access patterns, i.e. although
file data is sometimes sequential, it is often scattered across a
section of the disk due to fragmentation.  The primary way a the
device driver can really manage its cache is through historical data
in the form of previous accesses (which is itself even more limited as
the device driver is uninformed of cache hits in the file system
cache).  This type of data implies some form of LRU, least recently
used, eviction scheme.  It should now be clear that the file system
can make smarter decisions about what which blocks to evict due to its
ability to make predictions based on client hints and its greater
understanding of the data in the store.

If we resign ourselves to keeping the cache only in the file system,
then multiple users of a store will be penalized greatly: a block read
by one client will always be reread if another client requests the
same block: not only is the store accessed a second time, but twice as
much memory will be used as there is no way to share the page and use
copy on write.  Is this penalty worth the added intelligence in the
file system?  An argument can be made that using just one caching
strategy is suboptimital when we could just have two: nothing stops
both the file system and the device driver from caching thereby
permitting the former to continue to maintain an intelligent cache and
the device driver to have its simple LRU cache.  This argument
overlooks several important implications of having the two caches.
First, complexity is being added to the device driver in the form of a
list of pages it has read and given out.  This increase in memory
usage has a secondary effect: if the data structures become large (as
it certainly will for large active stores), it will be impossible to
keep the device driver in question in a small address space (an
important optimization on architectures without tagged TLBs, table
look aside buffers).  Second, if both the file system and the device
driver keep a cache, when the file system has a cache miss, the device
driver then checks its cache before going to disk.  The device driver
will only ever have a cache hit if there are multiple readers: when
there is a single user of a store, the file system's cache and the
device driver's cache will be identical.  This begs the question: how
often will there be multiple users of a single store?  The answer
seems to be very rarely: assuming the common case that the store has
some type of file system on it, there can only be multiple users if
all users are readers (note that not even one can be a writer as this
implies cache consistency issues across different users of the store).
Since this is a very rare case, we argue based on the philosophy ``do
not optimize for rare cases'' that the overhead is greater than the
potential pay back from the optimization.  Having multiple caches
leads to a further problem: a page is really not evicted from the
system until it is purged from all caches.  Thus if the file system
cache is smart and chooses the better pages to evict, the
cooresponding frames will not really be freed until the device driver
also drops its references to the pages.  Thus, the effectiveness of
the smarter caching algorithm is impeded by the device driver's
caching scheme.  Double caching must be avoided at all costs.

\subsection{Caching in the File System}

We have argued above that all block caching will be done at the file
system layer.  In this section, we detail how the caching will work.

The file system allocates extra pages as long as it can and adds all
eligible pages to the cache by logically copying them into a local
container (pages which it reasons will be read once and then dropped
may not be considered eligible).  When the physical memory server
wants pages back, it chooses a victim with extra pages and asks for a
subset of them back.  If a task has $G$ guaranteed pages and $G + E$
pages allocated, the physical memory server can request up to $E$
pages back from the task.  We recall from the definition of the extra
pages that extra pages must be given back quickly (i.e. there is no
time to send them to swap).

Although a task chooses a page to evict from its cache, it does not
mean that the page will be reused immediately, in fact, it is
sometimes that case that the page cannot be reused at all as another
task has have a reference to the page (in the form of a logical copy).
As such, it would be nice to be able to get pages back that might
still be in the physical memory server.  The following mechanism is
thus provided: when a page is returned to the physical memory server,
the reference to the page is turned into a soft reference.  Only when
the page is actually reused by the physical memory server are soft
references discarded.  A task is able to convert a soft reference back
to a hard reference by contacting the physical memory server and
asking for the page back.  If this operation return \errno{ENOEXIST},
the page has been reused and the page must be remanufactured (e.g. by
retrieving it from backing store).  This operation may also fail and
return \errno{ENOMEM} if the task does not have enough guaranteed
pages and there are no extra pages available.

\begin{comment}
There is a problem here in the form of name space pollution: the task
doing the caching has to remember the mapping of blocks to container
identifiers in order to recover the soft reference but the task has no
way to know when the physical memory server expires a given soft
reference.  Thus, while the physical memory server may drop a page,
the task will only ever know this when it tries to convert the soft
reference to a hard reference and fails (i.e. gets a cache miss).  For
pages which this is never done, the memorized mapping will never be
invalidated.  This may not be a problem if a block offset to container
id is used, however, if hashing is done or some other mapping of block
offsets to container identifiers is used, this will pollute the cache
container's name space.
\end{comment}

\subsection{Caching Interfaces}

The physical memory server will do an up call to a victim task
requesting a number of pages back.  The physical memory server may do
this at any time for any reason and it expects to receive the pages
back from the task within a short amount of time (the victim task
should not expect to be able to send the pages to backing store in
that amount of time).  The physical memory server will never request
guaranteed pages.  As such, this number will always be less than or
equal to the number of allocated pages minus the number of guaranteed
pages.

\begin{code}
void pm_return_pages (in int count);
\end{code}

The physical memory send this message to the task's memory control
thread.  The thread must always be ready to receive: the physical
memory server will never wait (thus, the thread must be in the
receiving state).  If the thread is not ready, the physical memory
server assumes that the task is misbehaving.  The physical memory
server does not wait for a reply, instead, the client must free the
pages using \function{pm_release_pages} as described above.

\section{The Memory Policy Server}

At task creation time, the task must negotiate a medium-term contract
for guaranteed pages and determine if it shall have access to extra
pages.  This may be renegotiated later.  It must be renegotiated when
the contract expires.  The policy server will give the task enough
time to send pages to swap before committing if the number of
guaranteed pages is reduced.

\section{Sending Data to Swap}

Data must be sent to swap.  The swap server must be in the phsyical
memory server in order to preserve logical copies in swap (if not, X
tasks swap a page to the swap server thus X writes/reads to swap
instead of 1 when all tasks release their references to the page).

Swap quotas (put the policy in the memory policy server).

Memory kept on an inactive list thus allowing recover before a page is
flushed to swap (i.e. a swap operation is not synchronous).

\section{Self Paging}

Tasks multiplex guaranteed pages.  Must manage their own memory.  How
to get data (e.g. extend malloc via the slab mechanism, extend fopen).


% Traditionally, monolithical kernels, but even kernels like Mach,
% provide a virtual memory management system in the kernel.  All paging
% decisions are made by the kernel itself.  This requires good
% heuristics.  Smart paging decisions are often not possible because the
% kernel lacks the information about how the data is used.
% 
% In the Hurd, paging will be done locally in each task.  A physical
% memory server provides a number of guaranteed physical pages to tasks.
% It will also provide a number of excess pages (over-commit).  The task
% might have to return any number of excess pages on short notice.  If
% the task does not comply, all mappings are revoked (essentially
% killing the task).
% 
% A problem arises when data has to be exchanged between a client and a
% server, and the server wants to have control over the content of the
% pages (for example, pass it on to other servers, like device drivers).
% The client can not map the pages directly into the servers address
% space, as it is not trusted.  Container objects created in the
% physical memory server and mapped into the client and/or the servers
% address space will provide the necessary security features to allow
% this.  This can be used for DMA and zero-copying in the data exchange
% between device drivers and (untrusted) user tasks.
% 
% 
