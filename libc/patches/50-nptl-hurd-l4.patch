#! /bin/sh
patch -p1 -f $* < $0
exit $?

This frobs nptl.

diff -x CVS -rupN libc/nptl/allocatestack.c libc/nptl/allocatestack.c
--- libc/nptl/allocatestack.c	2005-01-23 18:39:28.000000000 +0100
+++ libc/nptl/allocatestack.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,947 +0,0 @@
-/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <assert.h>
-#include <errno.h>
-#include <signal.h>
-#include <stdint.h>
-#include <string.h>
-#include <unistd.h>
-#include <sys/mman.h>
-#include <sys/param.h>
-#include <dl-sysdep.h>
-#include <tls.h>
-#include <lowlevellock.h>
-
-
-#ifndef NEED_SEPARATE_REGISTER_STACK
-
-/* Most architectures have exactly one stack pointer.  Some have more.  */
-# define STACK_VARIABLES void *stackaddr
-
-/* How to pass the values to the 'create_thread' function.  */
-# define STACK_VARIABLES_ARGS stackaddr
-
-/* How to declare function which gets there parameters.  */
-# define STACK_VARIABLES_PARMS void *stackaddr
-
-/* How to declare allocate_stack.  */
-# define ALLOCATE_STACK_PARMS void **stack
-
-/* This is how the function is called.  We do it this way to allow
-   other variants of the function to have more parameters.  */
-# define ALLOCATE_STACK(attr, pd) allocate_stack (attr, pd, &stackaddr)
-
-#else
-
-/* We need two stacks.  The kernel will place them but we have to tell
-   the kernel about the size of the reserved address space.  */
-# define STACK_VARIABLES void *stackaddr; size_t stacksize
-
-/* How to pass the values to the 'create_thread' function.  */
-# define STACK_VARIABLES_ARGS stackaddr, stacksize
-
-/* How to declare function which gets there parameters.  */
-# define STACK_VARIABLES_PARMS void *stackaddr, size_t stacksize
-
-/* How to declare allocate_stack.  */
-# define ALLOCATE_STACK_PARMS void **stack, size_t *stacksize
-
-/* This is how the function is called.  We do it this way to allow
-   other variants of the function to have more parameters.  */
-# define ALLOCATE_STACK(attr, pd) \
-  allocate_stack (attr, pd, &stackaddr, &stacksize)
-
-#endif
-
-
-/* Default alignment of stack.  */
-#ifndef STACK_ALIGN
-# define STACK_ALIGN __alignof__ (long double)
-#endif
-
-/* Default value for minimal stack size after allocating thread
-   descriptor and guard.  */
-#ifndef MINIMAL_REST_STACK
-# define MINIMAL_REST_STACK	4096
-#endif
-
-
-/* Let the architecture add some flags to the mmap() call used to
-   allocate stacks.  */
-#ifndef ARCH_MAP_FLAGS
-# define ARCH_MAP_FLAGS 0
-#endif
-
-/* This yields the pointer that TLS support code calls the thread pointer.  */
-#if TLS_TCB_AT_TP
-# define TLS_TPADJ(pd) (pd)
-#elif TLS_DTV_AT_TP
-# define TLS_TPADJ(pd) ((struct pthread *)((char *) (pd) + TLS_PRE_TCB_SIZE))
-#endif
-
-/* Cache handling for not-yet free stacks.  */
-
-/* Maximum size in kB of cache.  */
-static size_t stack_cache_maxsize = 40 * 1024 * 1024; /* 40MiBi by default.  */
-static size_t stack_cache_actsize;
-
-/* Mutex protecting this variable.  */
-static lll_lock_t stack_cache_lock = LLL_LOCK_INITIALIZER;
-
-/* List of queued stack frames.  */
-static LIST_HEAD (stack_cache);
-
-/* List of the stacks in use.  */
-static LIST_HEAD (stack_used);
-
-/* List of the threads with user provided stacks in use.  No need to
-   initialize this, since it's done in __pthread_initialize_minimal.  */
-list_t __stack_user __attribute__ ((nocommon));
-hidden_data_def (__stack_user)
-
-#if COLORING_INCREMENT != 0
-/* Number of threads created.  */
-static unsigned int nptl_ncreated;
-#endif
-
-
-/* Check whether the stack is still used or not.  */
-#define FREE_P(descr) ((descr)->tid <= 0)
-
-
-/* We create a double linked list of all cache entries.  Double linked
-   because this allows removing entries from the end.  */
-
-
-/* Get a stack frame from the cache.  We have to match by size since
-   some blocks might be too small or far too large.  */
-static struct pthread *
-get_cached_stack (size_t *sizep, void **memp)
-{
-  size_t size = *sizep;
-  struct pthread *result = NULL;
-  list_t *entry;
-
-  lll_lock (stack_cache_lock);
-
-  /* Search the cache for a matching entry.  We search for the
-     smallest stack which has at least the required size.  Note that
-     in normal situations the size of all allocated stacks is the
-     same.  As the very least there are only a few different sizes.
-     Therefore this loop will exit early most of the time with an
-     exact match.  */
-  list_for_each (entry, &stack_cache)
-    {
-      struct pthread *curr;
-
-      curr = list_entry (entry, struct pthread, list);
-      if (FREE_P (curr) && curr->stackblock_size >= size)
-	{
-	  if (curr->stackblock_size == size)
-	    {
-	      result = curr;
-	      break;
-	    }
-
-	  if (result == NULL
-	      || result->stackblock_size > curr->stackblock_size)
-	    result = curr;
-	}
-    }
-
-  if (__builtin_expect (result == NULL, 0)
-      /* Make sure the size difference is not too excessive.  In that
-	 case we do not use the block.  */
-      || __builtin_expect (result->stackblock_size > 4 * size, 0))
-    {
-      /* Release the lock.  */
-      lll_unlock (stack_cache_lock);
-
-      return NULL;
-    }
-
-  /* Dequeue the entry.  */
-  list_del (&result->list);
-
-  /* And add to the list of stacks in use.  */
-  list_add (&result->list, &stack_used);
-
-  /* And decrease the cache size.  */
-  stack_cache_actsize -= result->stackblock_size;
-
-  /* Release the lock early.  */
-  lll_unlock (stack_cache_lock);
-
-  /* Report size and location of the stack to the caller.  */
-  *sizep = result->stackblock_size;
-  *memp = result->stackblock;
-
-  /* Cancellation handling is back to the default.  */
-  result->cancelhandling = 0;
-  result->cleanup = NULL;
-
-  /* No pending event.  */
-  result->nextevent = NULL;
-
-  /* Clear the DTV.  */
-  dtv_t *dtv = GET_DTV (TLS_TPADJ (result));
-  memset (dtv, '\0', (dtv[-1].counter + 1) * sizeof (dtv_t));
-
-  /* Re-initialize the TLS.  */
-  _dl_allocate_tls_init (TLS_TPADJ (result));
-
-  return result;
-}
-
-
-/* Add a stack frame which is not used anymore to the stack.  Must be
-   called with the cache lock held.  */
-static inline void
-__attribute ((always_inline))
-queue_stack (struct pthread *stack)
-{
-  /* We unconditionally add the stack to the list.  The memory may
-     still be in use but it will not be reused until the kernel marks
-     the stack as not used anymore.  */
-  list_add (&stack->list, &stack_cache);
-
-  stack_cache_actsize += stack->stackblock_size;
-  if (__builtin_expect (stack_cache_actsize > stack_cache_maxsize, 0))
-    {
-      /* We reduce the size of the cache.  Remove the last entries
-	 until the size is below the limit.  */
-      list_t *entry;
-      list_t *prev;
-
-      /* Search from the end of the list.  */
-      list_for_each_prev_safe (entry, prev, &stack_cache)
-	{
-	  struct pthread *curr;
-
-	  curr = list_entry (entry, struct pthread, list);
-	  if (FREE_P (curr))
-	    {
-	      /* Unlink the block.  */
-	      list_del (entry);
-
-	      /* Account for the freed memory.  */
-	      stack_cache_actsize -= curr->stackblock_size;
-
-	      /* Free the memory associated with the ELF TLS.  */
-	      _dl_deallocate_tls (TLS_TPADJ (curr), false);
-
-	      /* Remove this block.  This should never fail.  If it
-		 does something is really wrong.  */
-	      if (munmap (curr->stackblock, curr->stackblock_size) != 0)
-		abort ();
-
-	      /* Maybe we have freed enough.  */
-	      if (stack_cache_actsize <= stack_cache_maxsize)
-		break;
-	    }
-	}
-    }
-}
-
-
-static int
-internal_function
-change_stack_perm (struct pthread *pd
-#ifdef NEED_SEPARATE_REGISTER_STACK
-		   , size_t pagemask
-#endif
-		   )
-{
-#ifdef NEED_SEPARATE_REGISTER_STACK
-  void *stack = (pd->stackblock
-		 + (((((pd->stackblock_size - pd->guardsize) / 2)
-		      & pagemask) + pd->guardsize) & pagemask));
-  size_t len = pd->stackblock + pd->stackblock_size - stack;
-#else
-  void *stack = pd->stackblock + pd->guardsize;
-  size_t len = pd->stackblock_size - pd->guardsize;
-#endif
-  if (mprotect (stack, len, PROT_READ | PROT_WRITE | PROT_EXEC) != 0)
-    return errno;
-
-  return 0;
-}
-
-
-static int
-allocate_stack (const struct pthread_attr *attr, struct pthread **pdp,
-		ALLOCATE_STACK_PARMS)
-{
-  struct pthread *pd;
-  size_t size;
-  size_t pagesize_m1 = __getpagesize () - 1;
-  void *stacktop;
-
-  assert (attr != NULL);
-  assert (powerof2 (pagesize_m1 + 1));
-  assert (TCB_ALIGNMENT >= STACK_ALIGN);
-
-  /* Get the stack size from the attribute if it is set.  Otherwise we
-     use the default we determined at start time.  */
-  size = attr->stacksize ?: __default_stacksize;
-
-  /* Get memory for the stack.  */
-  if (__builtin_expect (attr->flags & ATTR_FLAG_STACKADDR, 0))
-    {
-      uintptr_t adj;
-
-      /* If the user also specified the size of the stack make sure it
-	 is large enough.  */
-      if (attr->stacksize != 0
-	  && attr->stacksize < (__static_tls_size + MINIMAL_REST_STACK))
-	return EINVAL;
-
-      /* Adjust stack size for alignment of the TLS block.  */
-#if TLS_TCB_AT_TP
-      adj = ((uintptr_t) attr->stackaddr - TLS_TCB_SIZE)
-	    & __static_tls_align_m1;
-      assert (size > adj + TLS_TCB_SIZE);
-#elif TLS_DTV_AT_TP
-      adj = ((uintptr_t) attr->stackaddr - __static_tls_size)
-	    & __static_tls_align_m1;
-      assert (size > adj);
-#endif
-
-      /* The user provided some memory.  Let's hope it matches the
-	 size...  We do not allocate guard pages if the user provided
-	 the stack.  It is the user's responsibility to do this if it
-	 is wanted.  */
-#if TLS_TCB_AT_TP
-      pd = (struct pthread *) ((uintptr_t) attr->stackaddr
-			       - TLS_TCB_SIZE - adj);
-#elif TLS_DTV_AT_TP
-      pd = (struct pthread *) (((uintptr_t) attr->stackaddr
-			        - __static_tls_size - adj)
-			       - TLS_PRE_TCB_SIZE);
-#endif
-
-      /* The user provided stack memory needs to be cleared.  */
-      memset (pd, '\0', sizeof (struct pthread));
-
-      /* The first TSD block is included in the TCB.  */
-      pd->specific[0] = pd->specific_1stblock;
-
-      /* Remember the stack-related values.  */
-      pd->stackblock = (char *) attr->stackaddr - size;
-      pd->stackblock_size = size;
-
-      /* This is a user-provided stack.  It will not be queued in the
-	 stack cache nor will the memory (except the TLS memory) be freed.  */
-      pd->user_stack = true;
-
-      /* This is at least the second thread.  */
-      pd->header.multiple_threads = 1;
-#ifndef TLS_MULTIPLE_THREADS_IN_TCB
-      __pthread_multiple_threads = *__libc_multiple_threads_ptr = 1;
-#endif
-
-#ifdef NEED_DL_SYSINFO
-      /* Copy the sysinfo value from the parent.  */
-      THREAD_SYSINFO(pd) = THREAD_SELF_SYSINFO;
-#endif
-
-      /* The process ID is also the same as that of the caller.  */
-      pd->pid = THREAD_GETMEM (THREAD_SELF, pid);
-
-      /* Allocate the DTV for this thread.  */
-      if (_dl_allocate_tls (TLS_TPADJ (pd)) == NULL)
-	{
-	  /* Something went wrong.  */
-	  assert (errno == ENOMEM);
-	  return EAGAIN;
-	}
-
-
-      /* Prepare to modify global data.  */
-      lll_lock (stack_cache_lock);
-
-      /* And add to the list of stacks in use.  */
-      list_add (&pd->list, &__stack_user);
-
-      lll_unlock (stack_cache_lock);
-    }
-  else
-    {
-      /* Allocate some anonymous memory.  If possible use the cache.  */
-      size_t guardsize;
-      size_t reqsize;
-      void *mem;
-      const int prot = (PROT_READ | PROT_WRITE
-			| ((GL(dl_stack_flags) & PF_X) ? PROT_EXEC : 0));
-
-#if COLORING_INCREMENT != 0
-      /* Add one more page for stack coloring.  Don't do it for stacks
-	 with 16 times pagesize or larger.  This might just cause
-	 unnecessary misalignment.  */
-      if (size <= 16 * pagesize_m1)
-	size += pagesize_m1 + 1;
-#endif
-
-      /* Adjust the stack size for alignment.  */
-      size &= ~__static_tls_align_m1;
-      assert (size != 0);
-
-      /* Make sure the size of the stack is enough for the guard and
-	 eventually the thread descriptor.  */
-      guardsize = (attr->guardsize + pagesize_m1) & ~pagesize_m1;
-      if (__builtin_expect (size < (guardsize + __static_tls_size
-				    + MINIMAL_REST_STACK + pagesize_m1 + 1),
-			    0))
-	/* The stack is too small (or the guard too large).  */
-	return EINVAL;
-
-      /* Try to get a stack from the cache.  */
-      reqsize = size;
-      pd = get_cached_stack (&size, &mem);
-      if (pd == NULL)
-	{
-	  /* To avoid aliasing effects on a larger scale than pages we
-	     adjust the allocated stack size if necessary.  This way
-	     allocations directly following each other will not have
-	     aliasing problems.  */
-#if MULTI_PAGE_ALIASING != 0
-	  if ((size % MULTI_PAGE_ALIASING) == 0)
-	    size += pagesize_m1 + 1;
-#endif
-
-	  mem = mmap (NULL, size, prot,
-		      MAP_PRIVATE | MAP_ANONYMOUS | ARCH_MAP_FLAGS, -1, 0);
-
-	  if (__builtin_expect (mem == MAP_FAILED, 0))
-	    {
-#ifdef ARCH_RETRY_MMAP
-	      mem = ARCH_RETRY_MMAP (size);
-	      if (__builtin_expect (mem == MAP_FAILED, 0))
-#endif
-		return errno;
-	    }
-
-	  /* SIZE is guaranteed to be greater than zero.
-	     So we can never get a null pointer back from mmap.  */
-	  assert (mem != NULL);
-
-#if COLORING_INCREMENT != 0
-	  /* Atomically increment NCREATED.  */
-	  unsigned int ncreated = atomic_increment_val (&nptl_ncreated);
-
-	  /* We chose the offset for coloring by incrementing it for
-	     every new thread by a fixed amount.  The offset used
-	     module the page size.  Even if coloring would be better
-	     relative to higher alignment values it makes no sense to
-	     do it since the mmap() interface does not allow us to
-	     specify any alignment for the returned memory block.  */
-	  size_t coloring = (ncreated * COLORING_INCREMENT) & pagesize_m1;
-
-	  /* Make sure the coloring offsets does not disturb the alignment
-	     of the TCB and static TLS block.  */
-	  if (__builtin_expect ((coloring & __static_tls_align_m1) != 0, 0))
-	    coloring = (((coloring + __static_tls_align_m1)
-			 & ~(__static_tls_align_m1))
-			& ~pagesize_m1);
-#else
-	  /* Unless specified we do not make any adjustments.  */
-# define coloring 0
-#endif
-
-	  /* Place the thread descriptor at the end of the stack.  */
-#if TLS_TCB_AT_TP
-	  pd = (struct pthread *) ((char *) mem + size - coloring) - 1;
-#elif TLS_DTV_AT_TP
-	  pd = (struct pthread *) ((((uintptr_t) mem + size - coloring
-				    - __static_tls_size)
-				    & ~__static_tls_align_m1)
-				   - TLS_PRE_TCB_SIZE);
-#endif
-
-	  /* Remember the stack-related values.  */
-	  pd->stackblock = mem;
-	  pd->stackblock_size = size;
-
-	  /* We allocated the first block thread-specific data array.
-	     This address will not change for the lifetime of this
-	     descriptor.  */
-	  pd->specific[0] = pd->specific_1stblock;
-
-	  /* This is at least the second thread.  */
-	  pd->header.multiple_threads = 1;
-#ifndef TLS_MULTIPLE_THREADS_IN_TCB
-	  __pthread_multiple_threads = *__libc_multiple_threads_ptr = 1;
-#endif
-
-#ifdef NEED_DL_SYSINFO
-	  /* Copy the sysinfo value from the parent.  */
-	  THREAD_SYSINFO(pd) = THREAD_SELF_SYSINFO;
-#endif
-
-	  /* The process ID is also the same as that of the caller.  */
-	  pd->pid = THREAD_GETMEM (THREAD_SELF, pid);
-
-	  /* Allocate the DTV for this thread.  */
-	  if (_dl_allocate_tls (TLS_TPADJ (pd)) == NULL)
-	    {
-	      /* Something went wrong.  */
-	      assert (errno == ENOMEM);
-
-	      /* Free the stack memory we just allocated.  */
-	      (void) munmap (mem, size);
-
-	      return EAGAIN;
-	    }
-
-
-	  /* Prepare to modify global data.  */
-	  lll_lock (stack_cache_lock);
-
-	  /* And add to the list of stacks in use.  */
-	  list_add (&pd->list, &stack_used);
-
-	  lll_unlock (stack_cache_lock);
-
-
-	  /* There might have been a race.  Another thread might have
-	     caused the stacks to get exec permission while this new
-	     stack was prepared.  Detect if this was possible and
-	     change the permission if necessary.  */
-	  if (__builtin_expect ((GL(dl_stack_flags) & PF_X) != 0
-				&& (prot & PROT_EXEC) == 0, 0))
-	    {
-	      int err = change_stack_perm (pd
-#ifdef NEED_SEPARATE_REGISTER_STACK
-					   , ~pagesize_m1
-#endif
-					   );
-	      if (err != 0)
-		{
-		  /* Free the stack memory we just allocated.  */
-		  (void) munmap (mem, size);
-
-		  return err;
-		}
-	    }
-
-
-	  /* Note that all of the stack and the thread descriptor is
-	     zeroed.  This means we do not have to initialize fields
-	     with initial value zero.  This is specifically true for
-	     the 'tid' field which is always set back to zero once the
-	     stack is not used anymore and for the 'guardsize' field
-	     which will be read next.  */
-	}
-
-      /* Create or resize the guard area if necessary.  */
-      if (__builtin_expect (guardsize > pd->guardsize, 0))
-	{
-#ifdef NEED_SEPARATE_REGISTER_STACK
-	  char *guard = mem + (((size - guardsize) / 2) & ~pagesize_m1);
-#else
-	  char *guard = mem;
-#endif
-	  if (mprotect (guard, guardsize, PROT_NONE) != 0)
-	    {
-	      int err;
-	    mprot_error:
-	      err = errno;
-
-	      lll_lock (stack_cache_lock);
-
-	      /* Remove the thread from the list.  */
-	      list_del (&pd->list);
-
-	      lll_unlock (stack_cache_lock);
-
-	      /* Get rid of the TLS block we allocated.  */
-	      _dl_deallocate_tls (TLS_TPADJ (pd), false);
-
-	      /* Free the stack memory regardless of whether the size
-		 of the cache is over the limit or not.  If this piece
-		 of memory caused problems we better do not use it
-		 anymore.  Uh, and we ignore possible errors.  There
-		 is nothing we could do.  */
-	      (void) munmap (mem, size);
-
-	      return err;
-	    }
-
-	  pd->guardsize = guardsize;
-	}
-      else if (__builtin_expect (pd->guardsize - guardsize > size - reqsize,
-				 0))
-	{
-	  /* The old guard area is too large.  */
-
-#ifdef NEED_SEPARATE_REGISTER_STACK
-	  char *guard = mem + (((size - guardsize) / 2) & ~pagesize_m1);
-	  char *oldguard = mem + (((size - pd->guardsize) / 2) & ~pagesize_m1);
-
-	  if (oldguard < guard
-	      && mprotect (oldguard, guard - oldguard, prot) != 0)
-	    goto mprot_error;
-
-	  if (mprotect (guard + guardsize,
-			oldguard + pd->guardsize - guard - guardsize,
-			prot) != 0)
-	    goto mprot_error;
-#else
-	  if (mprotect ((char *) mem + guardsize, pd->guardsize - guardsize,
-			prot) != 0)
-	    goto mprot_error;
-#endif
-
-	  pd->guardsize = guardsize;
-	}
-      /* The pthread_getattr_np() calls need to get passed the size
-	 requested in the attribute, regardless of how large the
-	 actually used guardsize is.  */
-      pd->reported_guardsize = guardsize;
-    }
-
-  /* Initialize the lock.  We have to do this unconditionally since the
-     stillborn thread could be canceled while the lock is taken.  */
-  pd->lock = LLL_LOCK_INITIALIZER;
-
-  /* We place the thread descriptor at the end of the stack.  */
-  *pdp = pd;
-
-#if TLS_TCB_AT_TP
-  /* The stack begins before the TCB and the static TLS block.  */
-  stacktop = ((char *) (pd + 1) - __static_tls_size);
-#elif TLS_DTV_AT_TP
-  stacktop = (char *) (pd - 1);
-#endif
-
-#ifdef NEED_SEPARATE_REGISTER_STACK
-  *stack = pd->stackblock;
-  *stacksize = stacktop - *stack;
-#else
-  *stack = stacktop;
-#endif
-
-  return 0;
-}
-
-
-void
-internal_function
-__deallocate_stack (struct pthread *pd)
-{
-  lll_lock (stack_cache_lock);
-
-  /* Remove the thread from the list of threads with user defined
-     stacks.  */
-  list_del (&pd->list);
-
-  /* Not much to do.  Just free the mmap()ed memory.  Note that we do
-     not reset the 'used' flag in the 'tid' field.  This is done by
-     the kernel.  If no thread has been created yet this field is
-     still zero.  */
-  if (__builtin_expect (! pd->user_stack, 1))
-    (void) queue_stack (pd);
-  else
-    /* Free the memory associated with the ELF TLS.  */
-    _dl_deallocate_tls (TLS_TPADJ (pd), false);
-
-  lll_unlock (stack_cache_lock);
-}
-
-
-int
-internal_function
-__make_stacks_executable (void **stack_endp)
-{
-  /* First the main thread's stack.  */
-  int err = _dl_make_stack_executable (stack_endp);
-  if (err != 0)
-    return err;
-
-#ifdef NEED_SEPARATE_REGISTER_STACK
-  const size_t pagemask = ~(__getpagesize () - 1);
-#endif
-
-  lll_lock (stack_cache_lock);
-
-  list_t *runp;
-  list_for_each (runp, &stack_used)
-    {
-      err = change_stack_perm (list_entry (runp, struct pthread, list)
-#ifdef NEED_SEPARATE_REGISTER_STACK
-			       , pagemask
-#endif
-			       );
-      if (err != 0)
-	break;
-    }
-
-  /* Also change the permission for the currently unused stacks.  This
-     might be wasted time but better spend it here than adding a check
-     in the fast path.  */
-  if (err == 0)
-    list_for_each (runp, &stack_cache)
-      {
-	err = change_stack_perm (list_entry (runp, struct pthread, list)
-#ifdef NEED_SEPARATE_REGISTER_STACK
-				 , pagemask
-#endif
-				 );
-	if (err != 0)
-	  break;
-      }
-
-  lll_unlock (stack_cache_lock);
-
-  return err;
-}
-
-
-/* In case of a fork() call the memory allocation in the child will be
-   the same but only one thread is running.  All stacks except that of
-   the one running thread are not used anymore.  We have to recycle
-   them.  */
-void
-__reclaim_stacks (void)
-{
-  struct pthread *self = (struct pthread *) THREAD_SELF;
-
-  /* No locking necessary.  The caller is the only stack in use.  */
-
-  /* Mark all stacks except the still running one as free.  */
-  list_t *runp;
-  list_for_each (runp, &stack_used)
-    {
-      struct pthread *curp;
-
-      curp = list_entry (runp, struct pthread, list);
-      if (curp != self)
-	{
-	  /* This marks the stack as free.  */
-	  curp->tid = 0;
-
-	  /* The PID field must be initialized for the new process.  */
-	  curp->pid = self->pid;
-
-	  /* Account for the size of the stack.  */
-	  stack_cache_actsize += curp->stackblock_size;
-	}
-    }
-
-  /* Add the stack of all running threads to the cache.  */
-  list_splice (&stack_used, &stack_cache);
-
-  /* Remove the entry for the current thread to from the cache list
-     and add it to the list of running threads.  Which of the two
-     lists is decided by the user_stack flag.  */
-  list_del (&self->list);
-
-  /* Re-initialize the lists for all the threads.  */
-  INIT_LIST_HEAD (&stack_used);
-  INIT_LIST_HEAD (&__stack_user);
-
-  if (__builtin_expect (THREAD_GETMEM (self, user_stack), 0))
-    list_add (&self->list, &__stack_user);
-  else
-    list_add (&self->list, &stack_used);
-
-  /* There is one thread running.  */
-  __nptl_nthreads = 1;
-
-  /* Initialize the lock.  */
-  stack_cache_lock = LLL_LOCK_INITIALIZER;
-}
-
-
-#if HP_TIMING_AVAIL
-# undef __find_thread_by_id
-/* Find a thread given the thread ID.  */
-attribute_hidden
-struct pthread *
-__find_thread_by_id (pid_t tid)
-{
-  struct pthread *result = NULL;
-
-  lll_lock (stack_cache_lock);
-
-  /* Iterate over the list with system-allocated threads first.  */
-  list_t *runp;
-  list_for_each (runp, &stack_used)
-    {
-      struct pthread *curp;
-
-      curp = list_entry (runp, struct pthread, list);
-
-      if (curp->tid == tid)
-	{
-	  result = curp;
-	  goto out;
-	}
-    }
-
-  /* Now the list with threads using user-allocated stacks.  */
-  list_for_each (runp, &__stack_user)
-    {
-      struct pthread *curp;
-
-      curp = list_entry (runp, struct pthread, list);
-
-      if (curp->tid == tid)
-	{
-	  result = curp;
-	  goto out;
-	}
-    }
-
- out:
-  lll_unlock (stack_cache_lock);
-
-  return result;
-}
-#endif
-
-int
-attribute_hidden
-__nptl_setxid (struct xid_command *cmdp)
-{
-  int result;
-  lll_lock (stack_cache_lock);
-
-  __xidcmd = cmdp;
-  cmdp->cntr = 0;
-
-  INTERNAL_SYSCALL_DECL (err);
-
-  struct pthread *self = THREAD_SELF;
-
-  /* Iterate over the list with system-allocated threads first.  */
-  list_t *runp;
-  list_for_each (runp, &stack_used)
-    {
-      struct pthread *t = list_entry (runp, struct pthread, list);
-      if (t != self)
-	{
-	  int val;
-#if __ASSUME_TGKILL
-	  val = INTERNAL_SYSCALL (tgkill, err, 3,
-				  THREAD_GETMEM (THREAD_SELF, pid),
-				  t->tid, SIGSETXID);
-#else
-# ifdef __NR_tgkill
-	  val = INTERNAL_SYSCALL (tgkill, err, 3,
-				  THREAD_GETMEM (THREAD_SELF, pid),
-				  t->tid, SIGSETXID);
-	  if (INTERNAL_SYSCALL_ERROR_P (val, err)
-	      && INTERNAL_SYSCALL_ERRNO (val, err) == ENOSYS)
-# endif
-	    val = INTERNAL_SYSCALL (tkill, err, 2, t->tid, SIGSETXID);
-#endif
-
-	  if (!INTERNAL_SYSCALL_ERROR_P (val, err))
-	    atomic_increment (&cmdp->cntr);
-	}
-    }
-
-  /* Now the list with threads using user-allocated stacks.  */
-  list_for_each (runp, &__stack_user)
-    {
-      struct pthread *t = list_entry (runp, struct pthread, list);
-      if (t != self)
-	{
-	  int val;
-#if __ASSUME_TGKILL
-	  val = INTERNAL_SYSCALL (tgkill, err, 3,
-				  THREAD_GETMEM (THREAD_SELF, pid),
-				  t->tid, SIGSETXID);
-#else
-# ifdef __NR_tgkill
-	  val = INTERNAL_SYSCALL (tgkill, err, 3,
-				  THREAD_GETMEM (THREAD_SELF, pid),
-				  t->tid, SIGSETXID);
-	  if (INTERNAL_SYSCALL_ERROR_P (val, err)
-	      && INTERNAL_SYSCALL_ERRNO (val, err) == ENOSYS)
-# endif
-	    val = INTERNAL_SYSCALL (tkill, err, 2, t->tid, SIGSETXID);
-#endif
-
-	  if (!INTERNAL_SYSCALL_ERROR_P (val, err))
-	    atomic_increment (&cmdp->cntr);
-	}
-    }
-
-  int cur = cmdp->cntr;
-  while (cur != 0)
-    {
-      lll_futex_wait (&cmdp->cntr, cur);
-      cur = cmdp->cntr;
-    }
-
-  /* This must be last, otherwise the current thread might not have
-     permissions to send SIGSETXID syscall to the other threads.  */
-  result = INTERNAL_SYSCALL_NCS (cmdp->syscall_no, err, 3,
-				 cmdp->id[0], cmdp->id[1], cmdp->id[2]);
-  if (INTERNAL_SYSCALL_ERROR_P (result, err))
-    {
-      __set_errno (INTERNAL_SYSCALL_ERRNO (result, err));
-      result = -1;
-    }
-
-  lll_unlock (stack_cache_lock);
-  return result;
-}
-
-static inline void __attribute__((always_inline))
-init_one_static_tls (struct pthread *curp, struct link_map *map)
-{
-  dtv_t *dtv = GET_DTV (TLS_TPADJ (curp));
-# if TLS_TCB_AT_TP
-  void *dest = (char *) curp - map->l_tls_offset;
-# elif TLS_DTV_AT_TP
-  void *dest = (char *) curp + map->l_tls_offset + TLS_PRE_TCB_SIZE;
-# else
-#  error "Either TLS_TCB_AT_TP or TLS_DTV_AT_TP must be defined"
-# endif
-
-  /* Fill in the DTV slot so that a later LD/GD access will find it.  */
-  dtv[map->l_tls_modid].pointer.val = dest;
-  dtv[map->l_tls_modid].pointer.is_static = true;
-
-  /* Initialize the memory.  */
-  memset (__mempcpy (dest, map->l_tls_initimage, map->l_tls_initimage_size),
-	  '\0', map->l_tls_blocksize - map->l_tls_initimage_size);
-}
-
-void
-attribute_hidden
-__pthread_init_static_tls (struct link_map *map)
-{
-  lll_lock (stack_cache_lock);
-
-  /* Iterate over the list with system-allocated threads first.  */
-  list_t *runp;
-  list_for_each (runp, &stack_used)
-    init_one_static_tls (list_entry (runp, struct pthread, list), map);
-
-  /* Now the list with threads using user-allocated stacks.  */
-  list_for_each (runp, &__stack_user)
-    init_one_static_tls (list_entry (runp, struct pthread, list), map);
-
-  lll_unlock (stack_cache_lock);
-}
diff -x CVS -rupN libc/nptl/init.c libc/nptl/init.c
--- libc/nptl/init.c	2005-01-23 18:39:28.000000000 +0100
+++ libc/nptl/init.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,345 +0,0 @@
-/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <assert.h>
-#include <limits.h>
-#include <signal.h>
-#include <stdlib.h>
-#include <unistd.h>
-#include <sys/param.h>
-#include <sys/resource.h>
-#include <pthreadP.h>
-#include <atomic.h>
-#include <ldsodefs.h>
-#include <tls.h>
-#include <fork.h>
-#include <version.h>
-#include <shlib-compat.h>
-#include <smp.h>
-#include <lowlevellock.h>
-
-
-#ifndef __NR_set_tid_address
-/* XXX For the time being...  Once we can rely on the kernel headers
-   having the definition remove these lines.  */
-#if defined __s390__
-# define __NR_set_tid_address	252
-#elif defined __ia64__
-# define __NR_set_tid_address	1233
-#elif defined __i386__
-# define __NR_set_tid_address	258
-#elif defined __x86_64__
-# define __NR_set_tid_address	218
-#elif defined __powerpc__
-# define __NR_set_tid_address	232
-#elif defined __sparc__
-# define __NR_set_tid_address	166
-#else
-# error "define __NR_set_tid_address"
-#endif
-#endif
-
-
-/* Size and alignment of static TLS block.  */
-size_t __static_tls_size;
-size_t __static_tls_align_m1;
-
-/* Version of the library, used in libthread_db to detect mismatches.  */
-static const char nptl_version[] __attribute_used__ = VERSION;
-
-
-#if defined USE_TLS && !defined SHARED
-extern void __libc_setup_tls (size_t tcbsize, size_t tcbalign);
-#endif
-
-
-#ifdef SHARED
-static const struct pthread_functions pthread_functions =
-  {
-    .ptr_pthread_attr_destroy = __pthread_attr_destroy,
-# if SHLIB_COMPAT(libpthread, GLIBC_2_0, GLIBC_2_1)
-    .ptr___pthread_attr_init_2_0 = __pthread_attr_init_2_0,
-# endif
-    .ptr___pthread_attr_init_2_1 = __pthread_attr_init_2_1,
-    .ptr_pthread_attr_getdetachstate = __pthread_attr_getdetachstate,
-    .ptr_pthread_attr_setdetachstate = __pthread_attr_setdetachstate,
-    .ptr_pthread_attr_getinheritsched = __pthread_attr_getinheritsched,
-    .ptr_pthread_attr_setinheritsched = __pthread_attr_setinheritsched,
-    .ptr_pthread_attr_getschedparam = __pthread_attr_getschedparam,
-    .ptr_pthread_attr_setschedparam = __pthread_attr_setschedparam,
-    .ptr_pthread_attr_getschedpolicy = __pthread_attr_getschedpolicy,
-    .ptr_pthread_attr_setschedpolicy = __pthread_attr_setschedpolicy,
-    .ptr_pthread_attr_getscope = __pthread_attr_getscope,
-    .ptr_pthread_attr_setscope = __pthread_attr_setscope,
-    .ptr_pthread_condattr_destroy = __pthread_condattr_destroy,
-    .ptr_pthread_condattr_init = __pthread_condattr_init,
-    .ptr___pthread_cond_broadcast = __pthread_cond_broadcast,
-    .ptr___pthread_cond_destroy = __pthread_cond_destroy,
-    .ptr___pthread_cond_init = __pthread_cond_init,
-    .ptr___pthread_cond_signal = __pthread_cond_signal,
-    .ptr___pthread_cond_wait = __pthread_cond_wait,
-    .ptr___pthread_cond_timedwait = __pthread_cond_timedwait,
-# if SHLIB_COMPAT(libpthread, GLIBC_2_0, GLIBC_2_3_2)
-    .ptr___pthread_cond_broadcast_2_0 = __pthread_cond_broadcast_2_0,
-    .ptr___pthread_cond_destroy_2_0 = __pthread_cond_destroy_2_0,
-    .ptr___pthread_cond_init_2_0 = __pthread_cond_init_2_0,
-    .ptr___pthread_cond_signal_2_0 = __pthread_cond_signal_2_0,
-    .ptr___pthread_cond_wait_2_0 = __pthread_cond_wait_2_0,
-    .ptr___pthread_cond_timedwait_2_0 = __pthread_cond_timedwait_2_0,
-# endif
-    .ptr_pthread_equal = __pthread_equal,
-    .ptr___pthread_exit = __pthread_exit,
-    .ptr_pthread_getschedparam = __pthread_getschedparam,
-    .ptr_pthread_setschedparam = __pthread_setschedparam,
-    .ptr_pthread_mutex_destroy = INTUSE(__pthread_mutex_destroy),
-    .ptr_pthread_mutex_init = INTUSE(__pthread_mutex_init),
-    .ptr_pthread_mutex_lock = INTUSE(__pthread_mutex_lock),
-    .ptr_pthread_mutex_unlock = INTUSE(__pthread_mutex_unlock),
-    .ptr_pthread_self = __pthread_self,
-    .ptr_pthread_setcancelstate = __pthread_setcancelstate,
-    .ptr_pthread_setcanceltype = __pthread_setcanceltype,
-    .ptr___pthread_cleanup_upto = __pthread_cleanup_upto,
-    .ptr___pthread_once = __pthread_once_internal,
-    .ptr___pthread_rwlock_rdlock = __pthread_rwlock_rdlock_internal,
-    .ptr___pthread_rwlock_wrlock = __pthread_rwlock_wrlock_internal,
-    .ptr___pthread_rwlock_unlock = __pthread_rwlock_unlock_internal,
-    .ptr___pthread_key_create = __pthread_key_create_internal,
-    .ptr___pthread_getspecific = __pthread_getspecific_internal,
-    .ptr___pthread_setspecific = __pthread_setspecific_internal,
-    .ptr__pthread_cleanup_push_defer = __pthread_cleanup_push_defer,
-    .ptr__pthread_cleanup_pop_restore = __pthread_cleanup_pop_restore,
-    .ptr_nthreads = &__nptl_nthreads,
-    .ptr___pthread_unwind = &__pthread_unwind,
-    .ptr__nptl_deallocate_tsd = __nptl_deallocate_tsd,
-    .ptr__nptl_setxid = __nptl_setxid
-  };
-# define ptr_pthread_functions &pthread_functions
-#else
-# define ptr_pthread_functions NULL
-#endif
-
-
-/* For asynchronous cancellation we use a signal.  This is the handler.  */
-static void
-sigcancel_handler (int sig, siginfo_t *si, void *ctx)
-{
-  /* Safety check.  It would be possible to call this function for
-     other signals and send a signal from another process.  This is not
-     correct and might even be a security problem.  Try to catch as
-     many incorrect invocations as possible.  */
-  if (sig != SIGCANCEL
-#ifdef __ASSUME_CORRECT_SI_PID
-      /* Kernels before 2.5.75 stored the thread ID and not the process
-	 ID in si_pid so we skip this test.  */
-      || si->si_pid != THREAD_GETMEM (THREAD_SELF, pid)
-#endif
-      || si->si_code != SI_TKILL)
-    return;
-
-  struct pthread *self = THREAD_SELF;
-
-  int oldval = THREAD_GETMEM (self, cancelhandling);
-  while (1)
-    {
-      /* We are canceled now.  When canceled by another thread this flag
-	 is already set but if the signal is directly send (internally or
-	 from another process) is has to be done here.  */
-      int newval = oldval | CANCELING_BITMASK | CANCELED_BITMASK;
-
-      if (oldval == newval || (oldval & EXITING_BITMASK) != 0)
-	/* Already canceled or exiting.  */
-	break;
-
-      int curval = THREAD_ATOMIC_CMPXCHG_VAL (self, cancelhandling, newval,
-					      oldval);
-      if (curval == oldval)
-	{
-	  /* Set the return value.  */
-	  THREAD_SETMEM (self, result, PTHREAD_CANCELED);
-
-	  /* Make sure asynchronous cancellation is still enabled.  */
-	  if ((newval & CANCELTYPE_BITMASK) != 0)
-	    /* Run the registered destructors and terminate the thread.  */
-	    __do_cancel ();
-
-	  break;
-	}
-
-      oldval = curval;
-    }
-}
-
-
-struct xid_command *__xidcmd attribute_hidden;
-
-/* For asynchronous cancellation we use a signal.  This is the handler.  */
-static void
-sighandler_setxid (int sig, siginfo_t *si, void *ctx)
-{
-  /* Safety check.  It would be possible to call this function for
-     other signals and send a signal from another process.  This is not
-     correct and might even be a security problem.  Try to catch as
-     many incorrect invocations as possible.  */
-  if (sig != SIGSETXID
-#ifdef __ASSUME_CORRECT_SI_PID
-      /* Kernels before 2.5.75 stored the thread ID and not the process
-	 ID in si_pid so we skip this test.  */
-      || si->si_pid != THREAD_GETMEM (THREAD_SELF, pid)
-#endif
-      || si->si_code != SI_TKILL)
-    return;
-
-  INTERNAL_SYSCALL_DECL (err);
-  INTERNAL_SYSCALL_NCS (__xidcmd->syscall_no, err, 3, __xidcmd->id[0],
-			__xidcmd->id[1], __xidcmd->id[2]);
-
-  if (atomic_decrement_val (&__xidcmd->cntr) == 0)
-    lll_futex_wake (&__xidcmd->cntr, 1);
-}
-
-
-/* When using __thread for this, we do it in libc so as not
-   to give libpthread its own TLS segment just for this.  */
-extern void **__libc_dl_error_tsd (void) __attribute__ ((const));
-
-
-void
-__pthread_initialize_minimal_internal (void)
-{
-#ifndef SHARED
-  /* Unlike in the dynamically linked case the dynamic linker has not
-     taken care of initializing the TLS data structures.  */
-  __libc_setup_tls (TLS_TCB_SIZE, TLS_TCB_ALIGN);
-
-  /* We must prevent gcc from being clever and move any of the
-     following code ahead of the __libc_setup_tls call.  This function
-     will initialize the thread register which is subsequently
-     used.  */
-  __asm __volatile ("");
-#endif
-
-  /* Minimal initialization of the thread descriptor.  */
-  struct pthread *pd = THREAD_SELF;
-  INTERNAL_SYSCALL_DECL (err);
-  pd->pid = pd->tid = INTERNAL_SYSCALL (set_tid_address, err, 1, &pd->tid);
-  THREAD_SETMEM (pd, specific[0], &pd->specific_1stblock[0]);
-  THREAD_SETMEM (pd, user_stack, true);
-  if (LLL_LOCK_INITIALIZER != 0)
-    THREAD_SETMEM (pd, lock, LLL_LOCK_INITIALIZER);
-#if HP_TIMING_AVAIL
-  THREAD_SETMEM (pd, cpuclock_offset, GL(dl_cpuclock_offset));
-#endif
-
-  /* Set initial thread's stack block from 0 up to __libc_stack_end.
-     It will be bigger than it actually is, but for unwind.c/pt-longjmp.c
-     purposes this is good enough.  */
-  THREAD_SETMEM (pd, stackblock_size, (size_t) __libc_stack_end);
-
-  /* Initialize the list of all running threads with the main thread.  */
-  INIT_LIST_HEAD (&__stack_user);
-  list_add (&pd->list, &__stack_user);
-
-
-  /* Install the cancellation signal handler.  If for some reason we
-     cannot install the handler we do not abort.  Maybe we should, but
-     it is only asynchronous cancellation which is affected.  */
-  struct sigaction sa;
-  sa.sa_sigaction = sigcancel_handler;
-  sa.sa_flags = SA_SIGINFO;
-  __sigemptyset (&sa.sa_mask);
-
-  (void) __libc_sigaction (SIGCANCEL, &sa, NULL);
-
-  /* Install the handle to change the threads' uid/gid.  */
-  sa.sa_sigaction = sighandler_setxid;
-  sa.sa_flags = SA_SIGINFO | SA_RESTART;
-
-  (void) __libc_sigaction (SIGSETXID, &sa, NULL);
-
-  /* The parent process might have left the signals blocked.  Just in
-     case, unblock it.  We reuse the signal mask in the sigaction
-     structure.  It is already cleared.  */
-  __sigaddset (&sa.sa_mask, SIGCANCEL);
-  __sigaddset (&sa.sa_mask, SIGSETXID);
-  (void) INTERNAL_SYSCALL (rt_sigprocmask, err, 4, SIG_UNBLOCK, &sa.sa_mask,
-			   NULL, _NSIG / 8);
-
-
-  /* Determine the default allowed stack size.  This is the size used
-     in case the user does not specify one.  */
-  struct rlimit limit;
-  if (getrlimit (RLIMIT_STACK, &limit) != 0
-      || limit.rlim_cur == RLIM_INFINITY)
-    /* The system limit is not usable.  Use an architecture-specific
-       default.  */
-    __default_stacksize = ARCH_STACK_DEFAULT_SIZE;
-  else if (limit.rlim_cur < PTHREAD_STACK_MIN)
-    /* The system limit is unusably small.
-       Use the minimal size acceptable.  */
-    __default_stacksize = PTHREAD_STACK_MIN;
-  else
-    {
-      /* Round the resource limit up to page size.  */
-      const uintptr_t pagesz = __sysconf (_SC_PAGESIZE);
-      __default_stacksize = (limit.rlim_cur + pagesz - 1) & -pagesz;
-    }
-
-  /* Get the size of the static and alignment requirements for the TLS
-     block.  */
-  size_t static_tls_align;
-  _dl_get_tls_static_info (&__static_tls_size, &static_tls_align);
-
-  /* Make sure the size takes all the alignments into account.  */
-  if (STACK_ALIGN > static_tls_align)
-    static_tls_align = STACK_ALIGN;
-  __static_tls_align_m1 = static_tls_align - 1;
-
-  __static_tls_size = roundup (__static_tls_size, static_tls_align);
-
-#ifdef SHARED
-  /* Transfer the old value from the dynamic linker's internal location.  */
-  *__libc_dl_error_tsd () = *(*GL(dl_error_catch_tsd)) ();
-  GL(dl_error_catch_tsd) = &__libc_dl_error_tsd;
-
-  /* Make __rtld_lock_{,un}lock_recursive use pthread_mutex_{,un}lock,
-     keep the lock count from the ld.so implementation.  */
-  GL(dl_rtld_lock_recursive) = (void *) INTUSE (__pthread_mutex_lock);
-  GL(dl_rtld_unlock_recursive) = (void *) INTUSE (__pthread_mutex_unlock);
-  unsigned int rtld_lock_count = GL(dl_load_lock).mutex.__data.__count;
-  GL(dl_load_lock).mutex.__data.__count = 0;
-  while (rtld_lock_count-- > 0)
-    INTUSE (__pthread_mutex_lock) (&GL(dl_load_lock).mutex);
-
-  GL(dl_make_stack_executable_hook) = &__make_stacks_executable;
-#endif
-
-  GL(dl_init_static_tls) = &__pthread_init_static_tls;
-
-  /* Register the fork generation counter with the libc.  */
-#ifndef TLS_MULTIPLE_THREADS_IN_TCB
-  __libc_multiple_threads_ptr =
-#endif
-    __libc_pthread_init (&__fork_generation, __reclaim_stacks,
-			 ptr_pthread_functions);
-
-  /* Determine whether the machine is SMP or not.  */
-  __is_smp = is_smp_system ();
-}
-strong_alias (__pthread_initialize_minimal_internal,
-	      __pthread_initialize_minimal)
diff -x CVS -rupN libc/nptl/Makefile libc/nptl/Makefile
--- libc/nptl/Makefile	2005-01-23 18:39:28.000000000 +0100
+++ libc/nptl/Makefile	2005-01-23 19:35:29.000000000 +0100
@@ -101,10 +101,9 @@ libpthread-routines = init vars events v
 		      pt-longjmp pt-cleanup\
 		      cancellation \
 		      lowlevellock \
-		      pt-vfork \
 		      ptw-write ptw-read ptw-close ptw-fcntl ptw-accept \
 		      ptw-connect ptw-recv ptw-recvfrom ptw-recvmsg ptw-send \
-		      ptw-sendmsg ptw-sendto ptw-fsync ptw-lseek ptw-llseek \
+		      ptw-sendmsg ptw-sendto ptw-fsync ptw-lseek \
 		      ptw-msync ptw-nanosleep ptw-open ptw-open64 ptw-pause \
 		      ptw-pread ptw-pread64 ptw-pwrite ptw-pwrite64 \
 		      ptw-tcdrain ptw-wait ptw-waitpid ptw-msgrcv ptw-msgsnd \
@@ -188,7 +187,6 @@ CFLAGS-pt-system.c = -fexceptions
 # Don't generate deps for calls with no sources.  See sysdeps/unix/Makefile.
 omit-deps = $(unix-syscalls:%=ptw-%)
 
-
 tests = tst-attr1 tst-attr2 tst-attr3 \
 	tst-mutex1 tst-mutex2 tst-mutex3 tst-mutex4 tst-mutex5 tst-mutex6 \
 	tst-mutex7 tst-mutex8 tst-mutex9 tst-mutex5a tst-mutex7a \
@@ -341,7 +339,7 @@ tests-static += tst-locale1 tst-locale2
 xtests-static += tst-setuid1-static
 endif
 # These tests are linked with libc before libpthread
-tests-reverse += tst-cancel5 tst-cancel23 tst-vfork1x tst-vfork2x
+tests-reverse += tst-cancel5 tst-cancel23
 
 include ../Rules
 
diff -x CVS -rupN libc/nptl/pthread_cancel.c libc/nptl/pthread_cancel.c
--- libc/nptl/pthread_cancel.c	2005-01-23 18:39:28.000000000 +0100
+++ libc/nptl/pthread_cancel.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,104 +0,0 @@
-/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <errno.h>
-#include <signal.h>
-#include "pthreadP.h"
-#include "atomic.h"
-#include <sysdep.h>
-#include <kernel-features.h>
-
-
-int
-pthread_cancel (th)
-     pthread_t th;
-{
-  volatile struct pthread *pd = (volatile struct pthread *) th;
-
-  /* Make sure the descriptor is valid.  */
-  if (INVALID_TD_P (pd))
-    /* Not a valid thread handle.  */
-    return ESRCH;
-
-#ifdef SHARED
-  pthread_cancel_init ();
-#endif
-  int result = 0;
-  int oldval;
-  int newval;
-  do
-    {
-      oldval = pd->cancelhandling;
-      newval = oldval | CANCELING_BITMASK | CANCELED_BITMASK;
-
-      /* Avoid doing unnecessary work.  The atomic operation can
-	 potentially be expensive if the bug has to be locked and
-	 remote cache lines have to be invalidated.  */
-      if (oldval == newval)
-	break;
-
-      /* If the cancellation is handled asynchronously just send a
-	 signal.  We avoid this if possible since it's more
-	 expensive.  */
-      if (CANCEL_ENABLED_AND_CANCELED_AND_ASYNCHRONOUS (newval))
-	{
-	  /* Mark the cancellation as "in progress".  */
-	  atomic_bit_set (&pd->cancelhandling, CANCELING_BIT);
-
-	  /* The cancellation handler will take care of marking the
-	     thread as canceled.  */
-	  INTERNAL_SYSCALL_DECL (err);
-
-	  /* One comment: The PID field in the TCB can temporarily be
-	     changed (in fork).  But this must not affect this code
-	     here.  Since this function would have to be called while
-	     the thread is executing fork, it would have to happen in
-	     a signal handler.  But this is no allowed, pthread_cancel
-	     is not guaranteed to be async-safe.  */
-	  int val;
-#if __ASSUME_TGKILL
-	  val = INTERNAL_SYSCALL (tgkill, err, 3,
-				  THREAD_GETMEM (THREAD_SELF, pid), pd->tid,
-				  SIGCANCEL);
-#else
-# ifdef __NR_tgkill
-	  val = INTERNAL_SYSCALL (tgkill, err, 3,
-				  THREAD_GETMEM (THREAD_SELF, pid), pd->tid,
-				  SIGCANCEL);
-	  if (INTERNAL_SYSCALL_ERROR_P (val, err)
-	      && INTERNAL_SYSCALL_ERRNO (val, err) == ENOSYS)
-# endif
-	    val = INTERNAL_SYSCALL (tkill, err, 2, pd->tid, SIGCANCEL);
-#endif
-
-	  if (INTERNAL_SYSCALL_ERROR_P (val, err))
-	    result = INTERNAL_SYSCALL_ERRNO (val, err);
-
-	  break;
-	}
-    }
-  /* Mark the thread as canceled.  This has to be done
-     atomically since other bits could be modified as well.  */
-  while (atomic_compare_and_exchange_bool_acq (&pd->cancelhandling, newval,
-					       oldval));
-
-  return result;
-}
-
-PTHREAD_STATIC_FN_REQUIRE (pthread_create)
diff -x CVS -rupN libc/nptl/pthread_condattr_setclock.c libc/nptl/pthread_condattr_setclock.c
--- libc/nptl/pthread_condattr_setclock.c	2004-10-10 12:41:04.000000000 +0200
+++ libc/nptl/pthread_condattr_setclock.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,72 +0,0 @@
-/* Copyright (C) 2003, 2004 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2003.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <assert.h>
-#include <errno.h>
-#include <stdbool.h>
-#include <time.h>
-#include <sysdep.h>
-#include "pthreadP.h"
-#include <kernel-features.h>
-
-
-int
-pthread_condattr_setclock (attr, clock_id)
-     pthread_condattr_t *attr;
-     clockid_t clock_id;
-{
-  /* Only a few clocks are allowed.  CLOCK_REALTIME is always allowed.
-     CLOCK_MONOTONIC only if the kernel has the necessary support.  */
-  if (clock_id == CLOCK_MONOTONIC)
-    {
-#ifndef __ASSUME_POSIX_TIMERS
-# ifdef __NR_clock_getres
-      /* Check whether the clock is available.  */
-      static int avail;
-
-      if (avail == 0)
-	{
-	  struct timespec ts;
-
-	  INTERNAL_SYSCALL_DECL (err);
-	  int val;
-	  val = INTERNAL_SYSCALL (clock_getres, err, 2, CLOCK_MONOTONIC, &ts);
-	  avail = INTERNAL_SYSCALL_ERROR_P (val, err) ? -1 : 1;
-	}
-
-      if (avail < 0)
-# endif
-	/* Not available.  */
-	return EINVAL;
-#endif
-    }
-  else if (clock_id != CLOCK_REALTIME)
-    /* If more clocks are allowed some day the storing of the clock ID
-       in the pthread_cond_t structure needs to be adjusted.  */
-    return EINVAL;
-
-  /* Make sure the value fits in the bits we reserved.  */
-  assert (clock_id < (1 << COND_CLOCK_BITS));
-
-  int *valuep = &((struct pthread_condattr *) attr)->value;
-
-  *valuep = (*valuep & ~(1 << (COND_CLOCK_BITS + 1)) & ~1) | (clock_id << 1);
-
-  return 0;
-}
diff -x CVS -rupN libc/nptl/pthread_cond_init.c libc/nptl/pthread_cond_init.c
--- libc/nptl/pthread_cond_init.c	2004-10-10 12:41:04.000000000 +0200
+++ libc/nptl/pthread_cond_init.c	2005-01-23 19:35:29.000000000 +0100
@@ -30,7 +30,7 @@ __pthread_cond_init (cond, cond_attr)
      Conditional variables are always usable in multiple processes.  */
   struct pthread_condattr *icond_attr = (struct pthread_condattr *) cond_attr;
 
-  cond->__data.__lock = LLL_MUTEX_LOCK_INITIALIZER;
+  cond->__data.__lock = LLL_LOCK_INITIALIZER;
   cond->__data.__futex = 0;
   cond->__data.__nwaiters = (icond_attr != NULL
 			     && ((icond_attr->value & (COND_CLOCK_BITS << 1))
diff -x CVS -rupN libc/nptl/pthread_create.c libc/nptl/pthread_create.c
--- libc/nptl/pthread_create.c	2005-01-23 18:39:28.000000000 +0100
+++ libc/nptl/pthread_create.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,522 +0,0 @@
-/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <errno.h>
-#include <stdbool.h>
-#include <stdlib.h>
-#include <string.h>
-#include "pthreadP.h"
-#include <hp-timing.h>
-#include <ldsodefs.h>
-#include <atomic.h>
-#include <libc-internal.h>
-#include <resolv.h>
-
-#include <shlib-compat.h>
-
-
-/* Local function to start thread and handle cleanup.  */
-static int start_thread (void *arg);
-
-
-/* Nozero if debugging mode is enabled.  */
-int __pthread_debug;
-
-/* Globally enabled events.  */
-static td_thr_events_t __nptl_threads_events;
-
-/* Pointer to descriptor with the last event.  */
-static struct pthread *__nptl_last_event;
-
-/* Number of threads running.  */
-unsigned int __nptl_nthreads = 1;
-
-
-/* Code to allocate and deallocate a stack.  */
-#include "allocatestack.c"
-
-/* Code to create the thread.  */
-#include "createthread.c"
-
-
-struct pthread *
-internal_function
-__find_in_stack_list (pd)
-     struct pthread *pd;
-{
-  list_t *entry;
-  struct pthread *result = NULL;
-
-  lll_lock (stack_cache_lock);
-
-  list_for_each (entry, &stack_used)
-    {
-      struct pthread *curp;
-
-      curp = list_entry (entry, struct pthread, list);
-      if (curp == pd)
-	{
-	  result = curp;
-	  break;
-	}
-    }
-
-  if (result == NULL)
-    list_for_each (entry, &__stack_user)
-      {
-	struct pthread *curp;
-
-	curp = list_entry (entry, struct pthread, list);
-	if (curp == pd)
-	  {
-	    result = curp;
-	    break;
-	  }
-      }
-
-  lll_unlock (stack_cache_lock);
-
-  return result;
-}
-
-
-/* Deallocate POSIX thread-local-storage.  */
-void
-attribute_hidden
-__nptl_deallocate_tsd (void)
-{
-  struct pthread *self = THREAD_SELF;
-
-  /* Maybe no data was ever allocated.  This happens often so we have
-     a flag for this.  */
-  if (THREAD_GETMEM (self, specific_used))
-    {
-      size_t round;
-      size_t cnt;
-
-      round = 0;
-      do
-	{
-	  size_t idx;
-
-	  /* So far no new nonzero data entry.  */
-	  THREAD_SETMEM (self, specific_used, false);
-
-	  for (cnt = idx = 0; cnt < PTHREAD_KEY_1STLEVEL_SIZE; ++cnt)
-	    {
-	      struct pthread_key_data *level2;
-
-	      level2 = THREAD_GETMEM_NC (self, specific, cnt);
-
-	      if (level2 != NULL)
-		{
-		  size_t inner;
-
-		  for (inner = 0; inner < PTHREAD_KEY_2NDLEVEL_SIZE;
-		       ++inner, ++idx)
-		    {
-		      void *data = level2[inner].data;
-
-		      if (data != NULL)
-			{
-			  /* Always clear the data.  */
-			  level2[inner].data = NULL;
-
-			  /* Make sure the data corresponds to a valid
-			     key.  This test fails if the key was
-			     deallocated and also if it was
-			     re-allocated.  It is the user's
-			     responsibility to free the memory in this
-			     case.  */
-			  if (level2[inner].seq
-			      == __pthread_keys[idx].seq
-			      /* It is not necessary to register a destructor
-				 function.  */
-			      && __pthread_keys[idx].destr != NULL)
-			    /* Call the user-provided destructor.  */
-			    __pthread_keys[idx].destr (data);
-			}
-		    }
-		}
-	      else
-		idx += PTHREAD_KEY_1STLEVEL_SIZE;
-	    }
-
-	  if (THREAD_GETMEM (self, specific_used) == 0)
-	    /* No data has been modified.  */
-	    goto just_free;
-	}
-      /* We only repeat the process a fixed number of times.  */
-      while (__builtin_expect (++round < PTHREAD_DESTRUCTOR_ITERATIONS, 0));
-
-      /* Just clear the memory of the first block for reuse.  */
-      memset (&THREAD_SELF->specific_1stblock, '\0',
-	      sizeof (self->specific_1stblock));
-
-    just_free:
-      /* Free the memory for the other blocks.  */
-      for (cnt = 1; cnt < PTHREAD_KEY_1STLEVEL_SIZE; ++cnt)
-	{
-	  struct pthread_key_data *level2;
-
-	  level2 = THREAD_GETMEM_NC (self, specific, cnt);
-	  if (level2 != NULL)
-	    {
-	      /* The first block is allocated as part of the thread
-		 descriptor.  */
-	      free (level2);
-	      THREAD_SETMEM_NC (self, specific, cnt, NULL);
-	    }
-	}
-
-      THREAD_SETMEM (self, specific_used, false);
-    }
-}
-
-
-/* Deallocate a thread's stack after optionally making sure the thread
-   descriptor is still valid.  */
-void
-internal_function
-__free_tcb (struct pthread *pd)
-{
-  /* The thread is exiting now.  */
-  if (__builtin_expect (atomic_bit_test_set (&pd->cancelhandling,
-					     TERMINATED_BIT) == 0, 1))
-    {
-      /* Remove the descriptor from the list.  */
-      if (DEBUGGING_P && __find_in_stack_list (pd) == NULL)
-	/* Something is really wrong.  The descriptor for a still
-	   running thread is gone.  */
-	abort ();
-
-      /* Queue the stack memory block for reuse and exit the process.  The
-	 kernel will signal via writing to the address returned by
-	 QUEUE-STACK when the stack is available.  */
-      __deallocate_stack (pd);
-    }
-}
-
-
-static int
-start_thread (void *arg)
-{
-  struct pthread *pd = (struct pthread *) arg;
-
-#if HP_TIMING_AVAIL
-  /* Remember the time when the thread was started.  */
-  hp_timing_t now;
-  HP_TIMING_NOW (now);
-  THREAD_SETMEM (pd, cpuclock_offset, now);
-#endif
-
-  /* Initialize resolver state pointer.  */
-  __resp = &pd->res;
-
-  /* This is where the try/finally block should be created.  For
-     compilers without that support we do use setjmp.  */
-  struct pthread_unwind_buf unwind_buf;
-
-  /* No previous handlers.  */
-  unwind_buf.priv.data.prev = NULL;
-  unwind_buf.priv.data.cleanup = NULL;
-
-  int not_first_call;
-  not_first_call = setjmp ((struct __jmp_buf_tag *) unwind_buf.cancel_jmp_buf);
-  if (__builtin_expect (! not_first_call, 1))
-    {
-      /* Store the new cleanup handler info.  */
-      THREAD_SETMEM (pd, cleanup_jmp_buf, &unwind_buf);
-
-      if (__builtin_expect (pd->stopped_start, 0))
-	{
-	  int oldtype = CANCEL_ASYNC ();
-
-	  /* Get the lock the parent locked to force synchronization.  */
-	  lll_lock (pd->lock);
-	  /* And give it up right away.  */
-	  lll_unlock (pd->lock);
-
-	  CANCEL_RESET (oldtype);
-	}
-
-      /* Run the code the user provided.  */
-#ifdef CALL_THREAD_FCT
-      THREAD_SETMEM (pd, result, CALL_THREAD_FCT (pd));
-#else
-      THREAD_SETMEM (pd, result, pd->start_routine (pd->arg));
-#endif
-    }
-
-  /* Run the destructor for the thread-local data.  */
-  __nptl_deallocate_tsd ();
-
-  /* Clean up any state libc stored in thread-local variables.  */
-  __libc_thread_freeres ();
-
-  /* If this is the last thread we terminate the process now.  We
-     do not notify the debugger, it might just irritate it if there
-     is no thread left.  */
-  if (__builtin_expect (atomic_decrement_and_test (&__nptl_nthreads), 0))
-    /* This was the last thread.  */
-    exit (0);
-
-  /* Report the death of the thread if this is wanted.  */
-  if (__builtin_expect (pd->report_events, 0))
-    {
-      /* See whether TD_DEATH is in any of the mask.  */
-      const int idx = __td_eventword (TD_DEATH);
-      const uint32_t mask = __td_eventmask (TD_DEATH);
-
-      if ((mask & (__nptl_threads_events.event_bits[idx]
-		   | pd->eventbuf.eventmask.event_bits[idx])) != 0)
-	{
-	  /* Yep, we have to signal the death.  Add the descriptor to
-	     the list but only if it is not already on it.  */
-	  if (pd->nextevent == NULL)
-	    {
-	      pd->eventbuf.eventnum = TD_DEATH;
-	      pd->eventbuf.eventdata = pd;
-
-	      do
-		pd->nextevent = __nptl_last_event;
-	      while (atomic_compare_and_exchange_bool_acq (&__nptl_last_event,
-							   pd, pd->nextevent));
-	    }
-
-	  /* Now call the function to signal the event.  */
-	  __nptl_death_event ();
-	}
-    }
-
-  /* The thread is exiting now.  Don't set this bit until after we've hit
-     the event-reporting breakpoint, so that td_thr_get_info on us while at
-     the breakpoint reports TD_THR_RUN state rather than TD_THR_ZOMBIE.  */
-  atomic_bit_set (&pd->cancelhandling, EXITING_BIT);
-
-  /* If the thread is detached free the TCB.  */
-  if (IS_DETACHED (pd))
-    /* Free the TCB.  */
-    __free_tcb (pd);
-
-  /* We cannot call '_exit' here.  '_exit' will terminate the process.
-
-     The 'exit' implementation in the kernel will signal when the
-     process is really dead since 'clone' got passed the CLONE_CLEARTID
-     flag.  The 'tid' field in the TCB will be set to zero.
-
-     The exit code is zero since in case all threads exit by calling
-     'pthread_exit' the exit status must be 0 (zero).  */
-  __exit_thread_inline (0);
-
-  /* NOTREACHED */
-  return 0;
-}
-
-
-/* Default thread attributes for the case when the user does not
-   provide any.  */
-static const struct pthread_attr default_attr =
-  {
-    /* Just some value > 0 which gets rounded to the nearest page size.  */
-    .guardsize = 1,
-  };
-
-
-int
-__pthread_create_2_1 (newthread, attr, start_routine, arg)
-     pthread_t *newthread;
-     const pthread_attr_t *attr;
-     void *(*start_routine) (void *);
-     void *arg;
-{
-  STACK_VARIABLES;
-  const struct pthread_attr *iattr;
-  struct pthread *pd;
-  int err;
-
-  iattr = (struct pthread_attr *) attr;
-  if (iattr == NULL)
-    /* Is this the best idea?  On NUMA machines this could mean
-       accessing far-away memory.  */
-    iattr = &default_attr;
-
-  err = ALLOCATE_STACK (iattr, &pd);
-  if (__builtin_expect (err != 0, 0))
-    /* Something went wrong.  Maybe a parameter of the attributes is
-       invalid or we could not allocate memory.  */
-    return err;
-
-
-  /* Initialize the TCB.  All initializations with zero should be
-     performed in 'get_cached_stack'.  This way we avoid doing this if
-     the stack freshly allocated with 'mmap'.  */
-
-#ifdef TLS_TCB_AT_TP
-  /* Reference to the TCB itself.  */
-  pd->header.self = pd;
-
-  /* Self-reference for TLS.  */
-  pd->header.tcb = pd;
-#endif
-
-  /* Store the address of the start routine and the parameter.  Since
-     we do not start the function directly the stillborn thread will
-     get the information from its thread descriptor.  */
-  pd->start_routine = start_routine;
-  pd->arg = arg;
-
-  /* Copy the thread attribute flags.  */
-  struct pthread *self = THREAD_SELF;
-  pd->flags = ((iattr->flags & ~(ATTR_FLAG_SCHED_SET | ATTR_FLAG_POLICY_SET))
-	       | (self->flags & (ATTR_FLAG_SCHED_SET | ATTR_FLAG_POLICY_SET)));
-
-  /* Initialize the field for the ID of the thread which is waiting
-     for us.  This is a self-reference in case the thread is created
-     detached.  */
-  pd->joinid = iattr->flags & ATTR_FLAG_DETACHSTATE ? pd : NULL;
-
-  /* The debug events are inherited from the parent.  */
-  pd->eventbuf = self->eventbuf;
-
-
-  /* Copy the parent's scheduling parameters.  The flags will say what
-     is valid and what is not.  */
-  pd->schedpolicy = self->schedpolicy;
-  pd->schedparam = self->schedparam;
-
-  /* Determine scheduling parameters for the thread.  */
-  if (attr != NULL
-      && __builtin_expect ((iattr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0, 0)
-      && (iattr->flags & (ATTR_FLAG_SCHED_SET | ATTR_FLAG_POLICY_SET)) != 0)
-    {
-      INTERNAL_SYSCALL_DECL (err);
-
-      /* Use the scheduling parameters the user provided.  */
-      if (iattr->flags & ATTR_FLAG_POLICY_SET)
-	pd->schedpolicy = iattr->schedpolicy;
-      else if ((pd->flags & ATTR_FLAG_POLICY_SET) == 0)
-	{
-	  pd->schedpolicy = INTERNAL_SYSCALL (sched_getscheduler, err, 1, 0);
-	  pd->flags |= ATTR_FLAG_POLICY_SET;
-	}
-
-      if (iattr->flags & ATTR_FLAG_SCHED_SET)
-	memcpy (&pd->schedparam, &iattr->schedparam,
-		sizeof (struct sched_param));
-      else if ((pd->flags & ATTR_FLAG_SCHED_SET) == 0)
-	{
-	  INTERNAL_SYSCALL (sched_getparam, err, 2, 0, &pd->schedparam);
-	  pd->flags |= ATTR_FLAG_SCHED_SET;
-	}
-
-      /* Check for valid priorities.  */
-      int minprio = INTERNAL_SYSCALL (sched_get_priority_min, err, 1,
-				      iattr->schedpolicy);
-      int maxprio = INTERNAL_SYSCALL (sched_get_priority_max, err, 1,
-				      iattr->schedpolicy);
-      if (pd->schedparam.sched_priority < minprio
-	  || pd->schedparam.sched_priority > maxprio)
-	{
-	  err = EINVAL;
-	  goto errout;
-	}
-    }
-
-  /* Pass the descriptor to the caller.  */
-  *newthread = (pthread_t) pd;
-
-  /* Remember whether the thread is detached or not.  In case of an
-     error we have to free the stacks of non-detached stillborn
-     threads.  */
-  bool is_detached = IS_DETACHED (pd);
-
-  /* Start the thread.  */
-  err = create_thread (pd, iattr, STACK_VARIABLES_ARGS);
-  if (err != 0)
-    {
-      /* Something went wrong.  Free the resources.  */
-      if (!is_detached)
-	{
-	errout:
-	  __deallocate_stack (pd);
-	}
-      return err;
-    }
-
-  return 0;
-}
-versioned_symbol (libpthread, __pthread_create_2_1, pthread_create, GLIBC_2_1);
-
-
-#if SHLIB_COMPAT(libpthread, GLIBC_2_0, GLIBC_2_1)
-int
-__pthread_create_2_0 (newthread, attr, start_routine, arg)
-     pthread_t *newthread;
-     const pthread_attr_t *attr;
-     void *(*start_routine) (void *);
-     void *arg;
-{
-  /* The ATTR attribute is not really of type `pthread_attr_t *'.  It has
-     the old size and access to the new members might crash the program.
-     We convert the struct now.  */
-  struct pthread_attr new_attr;
-
-  if (attr != NULL)
-    {
-      struct pthread_attr *iattr = (struct pthread_attr *) attr;
-      size_t ps = __getpagesize ();
-
-      /* Copy values from the user-provided attributes.  */
-      new_attr.schedparam = iattr->schedparam;
-      new_attr.schedpolicy = iattr->schedpolicy;
-      new_attr.flags = iattr->flags;
-
-      /* Fill in default values for the fields not present in the old
-	 implementation.  */
-      new_attr.guardsize = ps;
-      new_attr.stackaddr = NULL;
-      new_attr.stacksize = 0;
-      new_attr.cpuset = NULL;
-
-      /* We will pass this value on to the real implementation.  */
-      attr = (pthread_attr_t *) &new_attr;
-    }
-
-  return __pthread_create_2_1 (newthread, attr, start_routine, arg);
-}
-compat_symbol (libpthread, __pthread_create_2_0, pthread_create,
-	       GLIBC_2_0);
-#endif
-
-/* Information for libthread_db.  */
-
-#include "../nptl_db/db_info.c"
-
-/* If pthread_create is present, libgcc_eh.a and libsupc++.a expects some other POSIX thread
-   functions to be present as well.  */
-PTHREAD_STATIC_FN_REQUIRE (pthread_mutex_lock)
-PTHREAD_STATIC_FN_REQUIRE (pthread_mutex_unlock)
-
-PTHREAD_STATIC_FN_REQUIRE (pthread_once)
-PTHREAD_STATIC_FN_REQUIRE (pthread_cancel)
-
-PTHREAD_STATIC_FN_REQUIRE (pthread_key_create)
-PTHREAD_STATIC_FN_REQUIRE (pthread_setspecific)
-PTHREAD_STATIC_FN_REQUIRE (pthread_getspecific)
diff -x CVS -rupN libc/nptl/sem_open.c libc/nptl/sem_open.c
--- libc/nptl/sem_open.c	2003-05-31 21:57:50.000000000 +0200
+++ libc/nptl/sem_open.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,404 +0,0 @@
-/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <errno.h>
-#include <fcntl.h>
-#include <mntent.h>
-#include <paths.h>
-#include <pthread.h>
-#include <search.h>
-#include <semaphore.h>
-#include <stdarg.h>
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-#include <unistd.h>
-#include <sys/mman.h>
-#include <sys/stat.h>
-#include <sys/statfs.h>
-#include <linux_fsinfo.h>
-#include "semaphoreP.h"
-
-
-
-/* Information about the mount point.  */
-struct mountpoint_info mountpoint attribute_hidden;
-
-/* This is the default mount point.  */
-static const char defaultmount[] = "/dev/shm";
-/* This is the default directory.  */
-static const char defaultdir[] = "/dev/shm/sem.";
-
-/* Protect the `mountpoint' variable above.  */
-pthread_once_t __namedsem_once attribute_hidden = PTHREAD_ONCE_INIT;
-
-
-/* Determine where the shmfs is mounted (if at all).  */
-void
-attribute_hidden
-__where_is_shmfs (void)
-{
-  char buf[512];
-  struct statfs f;
-  struct mntent resmem;
-  struct mntent *mp;
-  FILE *fp;
-
-  /* The canonical place is /dev/shm.  This is at least what the
-     documentation tells everybody to do.  */
-  if (__statfs (defaultmount, &f) == 0 && f.f_type == SHMFS_SUPER_MAGIC)
-    {
-      /* It is in the normal place.  */
-      mountpoint.dir = (char *) defaultdir;
-      mountpoint.dirlen = sizeof (defaultdir) - 1;
-
-      return;
-    }
-
-  /* OK, do it the hard way.  Look through the /proc/mounts file and if
-     this does not exist through /etc/fstab to find the mount point.  */
-  fp = __setmntent ("/proc/mounts", "r");
-  if (__builtin_expect (fp == NULL, 0))
-    {
-      fp = __setmntent (_PATH_MNTTAB, "r");
-      if (__builtin_expect (fp == NULL, 0))
-	/* There is nothing we can do.  Blind guesses are not helpful.  */
-	return;
-    }
-
-  /* Now read the entries.  */
-  while ((mp = __getmntent_r (fp, &resmem, buf, sizeof buf)) != NULL)
-    /* The original name is "shm" but this got changed in early Linux
-       2.4.x to "tmpfs".  */
-    if (strcmp (mp->mnt_type, "tmpfs") == 0
-	|| strcmp (mp->mnt_type, "shm") == 0)
-      {
-	/* Found it.  There might be more than one place where the
-           filesystem is mounted but one is enough for us.  */
-	size_t namelen;
-
-	/* First make sure this really is the correct entry.  At least
-	   some versions of the kernel give wrong information because
-	   of the implicit mount of the shmfs for SysV IPC.  */
-	if (__statfs (mp->mnt_dir, &f) != 0 || f.f_type != SHMFS_SUPER_MAGIC)
-	  continue;
-
-	namelen = strlen (mp->mnt_dir);
-
-	if (namelen == 0)
-	  /* Hum, maybe some crippled entry.  Keep on searching.  */
-	  continue;
-
-	mountpoint.dir = (char *) malloc (namelen + 4 + 2);
-	if (mountpoint.dir != NULL)
-	  {
-	    char *cp = __mempcpy (mountpoint.dir, mp->mnt_dir, namelen);
-	    if (cp[-1] != '/')
-	      *cp++ = '/';
-	    cp = stpcpy (cp, "sem.");
-	    mountpoint.dirlen = cp - mountpoint.dir;
-	  }
-
-	break;
-      }
-
-  /* Close the stream.  */
-  __endmntent (fp);
-}
-
-
-/* Comparison function for search of existing mapping.  */
-int
-attribute_hidden
-__sem_search (const void *a, const void *b)
-{
-  const struct inuse_sem *as = (const struct inuse_sem *) a;
-  const struct inuse_sem *bs = (const struct inuse_sem *) b;
-
-  if (as->ino != bs->ino)
-    /* Cannot return the difference the type is larger than int.  */
-    return as->ino < bs->ino ? -1 : (as->ino == bs->ino ? 0 : 1);
-
-  if (as->dev != bs->dev)
-    /* Cannot return the difference the type is larger than int.  */
-    return as->dev < bs->dev ? -1 : (as->dev == bs->dev ? 0 : 1);
-
-  return strcmp (as->name, bs->name);
-}
-
-
-/* The search tree for existing mappings.  */
-void *__sem_mappings attribute_hidden;
-
-/* Lock to protect the search tree.  */
-lll_lock_t __sem_mappings_lock = LLL_LOCK_INITIALIZER;
-
-
-/* Search for existing mapping and if possible add the one provided.  */
-static sem_t *
-check_add_mapping (const char *name, size_t namelen, int fd, sem_t *existing)
-{
-  sem_t *result = SEM_FAILED;
-
-  /* Get the information about the file.  */
-  struct stat64 st;
-  if (__fxstat64 (_STAT_VER, fd, &st) == 0)
-    {
-      /* Get the lock.  */
-      lll_lock (__sem_mappings_lock);
-
-      /* Search for an existing mapping given the information we have.  */
-      struct inuse_sem *fake;
-      fake = (struct inuse_sem *) alloca (sizeof (*fake) + namelen);
-      memcpy (fake->name, name, namelen);
-      fake->dev = st.st_dev;
-      fake->ino = st.st_ino;
-
-      struct inuse_sem **foundp = tfind (fake, &__sem_mappings, __sem_search);
-      if (foundp != NULL)
-	{
-	  /* There is already a mapping.  Use it.  */
-	  result = (*foundp)->sem;
-	  ++(*foundp)->refcnt;
-	}
-      else
-	{
-	  /* We haven't found a mapping.  Install ione.  */
-	  struct inuse_sem *newp;
-
-	  newp = (struct inuse_sem *) malloc (sizeof (*newp) + namelen);
-	  if (newp != NULL)
-	    {
-	      /* If the caller hasn't provided any map it now.  */
-	      if (existing == SEM_FAILED)
-		existing = (sem_t *) mmap (NULL, sizeof (sem_t),
-					   PROT_READ | PROT_WRITE, MAP_SHARED,
-					   fd, 0);
-
-	      newp->dev = st.st_dev;
-	      newp->ino = st.st_ino;
-	      newp->refcnt = 1;
-	      newp->sem = existing;
-	      memcpy (newp->name, name, namelen);
-
-	      /* Insert the new value.  */
-	      if (existing != MAP_FAILED
-		  && tsearch (newp, &__sem_mappings, __sem_search) != NULL)
-		/* Successful.  */
-		result = existing;
-	      else
-		/* Something went wrong while inserting the new
-		   value.  We fail completely.  */
-		free (newp);
-	    }
-	}
-
-      /* Release the lock.  */
-      lll_unlock (__sem_mappings_lock);
-    }
-
-  if (result != existing && existing != SEM_FAILED && existing != MAP_FAILED)
-    {
-      /* Do not disturb errno.  */
-      INTERNAL_SYSCALL_DECL (err);
-      INTERNAL_SYSCALL (munmap, err, 2, existing, sizeof (sem_t));
-    }
-
-  return result;
-}
-
-
-sem_t *
-sem_open (const char *name, int oflag, ...)
-{
-  char *finalname;
-  sem_t *result = SEM_FAILED;
-  int fd;
-
-  /* Determine where the shmfs is mounted.  */
-  INTUSE(__pthread_once) (&__namedsem_once, __where_is_shmfs);
-
-  /* If we don't know the mount points there is nothing we can do.  Ever.  */
-  if (mountpoint.dir == NULL)
-    {
-      __set_errno (ENOSYS);
-      return SEM_FAILED;
-    }
-
-  /* Construct the filename.  */
-  while (name[0] == '/')
-    ++name;
-
-  if (name[0] == '\0')
-    {
-      /* The name "/" is not supported.  */
-      __set_errno (EINVAL);
-      return SEM_FAILED;
-    }
-  size_t namelen = strlen (name) + 1;
-
-  /* Create the name of the final file.  */
-  finalname = (char *) alloca (mountpoint.dirlen + namelen);
-  __mempcpy (__mempcpy (finalname, mountpoint.dir, mountpoint.dirlen),
-	     name, namelen);
-
-  /* If the semaphore object has to exist simply open it.  */
-  if ((oflag & O_CREAT) == 0 || (oflag & O_EXCL) == 0)
-    {
-    try_again:
-      fd = __libc_open (finalname,
-			(oflag & ~(O_CREAT|O_ACCMODE)) | O_NOFOLLOW | O_RDWR);
-
-      if (fd == -1)
-	{
-	  /* If we are supposed to create the file try this next.  */
-	  if ((oflag & O_CREAT) != 0 && errno == ENOENT)
-	    goto try_create;
-
-	  /* Return.  errno is already set.  */
-	}
-      else
-	/* Check whether we already have this semaphore mapped and
-	   create one if necessary.  */
-	result = check_add_mapping (name, namelen, fd, SEM_FAILED);
-    }
-  else
-    {
-      /* We have to open a temporary file first since it must have the
-	 correct form before we can start using it.  */
-      char *tmpfname;
-      mode_t mode;
-      unsigned int value;
-      va_list ap;
-
-    try_create:
-      va_start (ap, oflag);
-
-      mode = va_arg (ap, mode_t);
-      value = va_arg (ap, unsigned int);
-
-      va_end (ap);
-
-      if (value > SEM_VALUE_MAX)
-	{
-	  __set_errno (EINVAL);
-	  return SEM_FAILED;
-	}
-
-      /* Create the initial file content.  */
-      sem_t initsem;
-
-      struct sem *iinitsem = (struct sem *) &initsem;
-      iinitsem->count = value;
-
-      /* Initialize the remaining bytes as well.  */
-      memset ((char *) &initsem + sizeof (struct sem), '\0',
-	      sizeof (sem_t) - sizeof (struct sem));
-
-      tmpfname = (char *) alloca (mountpoint.dirlen + 6 + 1);
-      char *xxxxxx = __mempcpy (tmpfname, mountpoint.dir, mountpoint.dirlen);
-
-      int retries = 0;
-#define NRETRIES 50
-      while (1)
-	{
-	  /* Add the suffix for mktemp.  */
-	  strcpy (xxxxxx, "XXXXXX");
-
-	  /* We really want to use mktemp here.  We cannot use mkstemp
-	     since the file must be opened with a specific mode.  The
-	     mode cannot later be set since then we cannot apply the
-	     file create mask.  */
-	  if (mktemp (tmpfname) == NULL)
-	    return SEM_FAILED;
-
-	  /* Open the file.  Make sure we do not overwrite anything.  */
-	  fd = __libc_open (tmpfname, O_RDWR | O_CREAT | O_EXCL, mode);
-	  if (fd == -1)
-	    {
-	      if (errno == EEXIST)
-		{
-		  if (++retries < NRETRIES)
-		    continue;
-
-		  __set_errno (EAGAIN);
-		}
-
-	      return SEM_FAILED;
-	    }
-
-	  /* We got a file.  */
-	  break;
-	}
-
-      if (TEMP_FAILURE_RETRY (__libc_write (fd, &initsem, sizeof (sem_t)))
-	  == sizeof (sem_t)
-	  /* Map the sem_t structure from the file.  */
-	  && (result = (sem_t *) mmap (NULL, sizeof (sem_t),
-				       PROT_READ | PROT_WRITE, MAP_SHARED,
-				       fd, 0)) != MAP_FAILED)
-	{
-	  /* Create the file.  Don't overwrite an existing file.  */
-	  if (link (tmpfname, finalname) != 0)
-	    {
-	      /* Undo the mapping.  */
-	      (void) munmap (result, sizeof (sem_t));
-
-	      /* Reinitialize 'result'.  */
-	      result = SEM_FAILED;
-
-	      /* This failed.  If O_EXCL is not set and the problem was
-		 that the file exists, try again.  */
-	      if ((oflag & O_EXCL) == 0 && errno == EEXIST)
-		{
-		  /* Remove the file.  */
-		  (void) unlink (tmpfname);
-
-		  /* Close the file.  */
-		  (void) __libc_close (fd);
-
-		  goto try_again;
-		}
-	    }
-	  else
-	    /* Insert the mapping into the search tree.  This also
-	       determines whether another thread sneaked by and already
-	       added such a mapping despite the fact that we created it.  */
-	    result = check_add_mapping (name, namelen, fd, result);
-	}
-
-      /* Now remove the temporary name.  This should never fail.  If
-	 it fails we leak a file name.  Better fix the kernel.  */
-      (void) unlink (tmpfname);
-    }
-
-  /* Map the mmap error to the error we need.  */
-  if (MAP_FAILED != (void *) SEM_FAILED && result == MAP_FAILED)
-    result = SEM_FAILED;
-
-  /* We don't need the file descriptor anymore.  */
-  if (fd != -1)
-    {
-      /* Do not disturb errno.  */
-      INTERNAL_SYSCALL_DECL (err);
-      INTERNAL_SYSCALL (close, err, 1, fd);
-    }
-
-  return result;
-}
diff -x CVS -rupN libc/nptl/sysdeps/i386/pthreaddef.h libc/nptl/sysdeps/i386/pthreaddef.h
--- libc/nptl/sysdeps/i386/pthreaddef.h	2003-03-25 20:19:43.000000000 +0100
+++ libc/nptl/sysdeps/i386/pthreaddef.h	2005-01-23 19:35:29.000000000 +0100
@@ -35,6 +35,8 @@
 #define CURRENT_STACK_FRAME	__builtin_frame_address (0)
 
 
+#if 0
+
 /* XXX Until we have a better place keep the definitions here.  */
 
 /* While there is no such syscall.  */
@@ -46,3 +48,5 @@
       asm volatile ("movl %1, %%ebx; int $0x80"				      \
 		    :: "a" (__NR_exit), "r" (val));			      \
   }
+
+#endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/allocatestack.c libc/nptl/sysdeps/l4/hurd/allocatestack.c
--- libc/nptl/sysdeps/l4/hurd/allocatestack.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/allocatestack.c	2005-01-23 20:02:13.000000000 +0100
@@ -0,0 +1,949 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <assert.h>
+#include <errno.h>
+#include <signal.h>
+#include <stdint.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/param.h>
+#include <dl-sysdep.h>
+#include <tls.h>
+#include <lowlevellock.h>
+
+
+#ifndef NEED_SEPARATE_REGISTER_STACK
+
+/* Most architectures have exactly one stack pointer.  Some have more.  */
+# define STACK_VARIABLES void *stackaddr
+
+/* How to pass the values to the 'create_thread' function.  */
+# define STACK_VARIABLES_ARGS stackaddr
+
+/* How to declare function which gets there parameters.  */
+# define STACK_VARIABLES_PARMS void *stackaddr
+
+/* How to declare allocate_stack.  */
+# define ALLOCATE_STACK_PARMS void **stack
+
+/* This is how the function is called.  We do it this way to allow
+   other variants of the function to have more parameters.  */
+# define ALLOCATE_STACK(attr, pd) allocate_stack (attr, pd, &stackaddr)
+
+#else
+
+/* We need two stacks.  The kernel will place them but we have to tell
+   the kernel about the size of the reserved address space.  */
+# define STACK_VARIABLES void *stackaddr; size_t stacksize
+
+/* How to pass the values to the 'create_thread' function.  */
+# define STACK_VARIABLES_ARGS stackaddr, stacksize
+
+/* How to declare function which gets there parameters.  */
+# define STACK_VARIABLES_PARMS void *stackaddr, size_t stacksize
+
+/* How to declare allocate_stack.  */
+# define ALLOCATE_STACK_PARMS void **stack, size_t *stacksize
+
+/* This is how the function is called.  We do it this way to allow
+   other variants of the function to have more parameters.  */
+# define ALLOCATE_STACK(attr, pd) \
+  allocate_stack (attr, pd, &stackaddr, &stacksize)
+
+#endif
+
+
+/* Default alignment of stack.  */
+#ifndef STACK_ALIGN
+# define STACK_ALIGN __alignof__ (long double)
+#endif
+
+/* Default value for minimal stack size after allocating thread
+   descriptor and guard.  */
+#ifndef MINIMAL_REST_STACK
+# define MINIMAL_REST_STACK	4096
+#endif
+
+
+/* Let the architecture add some flags to the mmap() call used to
+   allocate stacks.  */
+#ifndef ARCH_MAP_FLAGS
+# define ARCH_MAP_FLAGS 0
+#endif
+
+/* This yields the pointer that TLS support code calls the thread pointer.  */
+#if TLS_TCB_AT_TP
+# define TLS_TPADJ(pd) (pd)
+#elif TLS_DTV_AT_TP
+# define TLS_TPADJ(pd) ((struct pthread *)((char *) (pd) + TLS_PRE_TCB_SIZE))
+#endif
+
+/* Cache handling for not-yet free stacks.  */
+
+/* Maximum size in kB of cache.  */
+static size_t stack_cache_maxsize = 40 * 1024 * 1024; /* 40MiBi by default.  */
+static size_t stack_cache_actsize;
+
+/* Mutex protecting this variable.  */
+static lll_lock_t stack_cache_lock = LLL_LOCK_INITIALIZER;
+
+/* List of queued stack frames.  */
+static LIST_HEAD (stack_cache);
+
+/* List of the stacks in use.  */
+static LIST_HEAD (stack_used);
+
+/* List of the threads with user provided stacks in use.  No need to
+   initialize this, since it's done in __pthread_initialize_minimal.  */
+list_t __stack_user __attribute__ ((nocommon));
+hidden_data_def (__stack_user)
+
+#if COLORING_INCREMENT != 0
+/* Number of threads created.  */
+static unsigned int nptl_ncreated;
+#endif
+
+
+/* Check whether the stack is still used or not.  */
+#define FREE_P(descr) ((descr)->tid <= 0)
+
+
+/* We create a double linked list of all cache entries.  Double linked
+   because this allows removing entries from the end.  */
+
+
+/* Get a stack frame from the cache.  We have to match by size since
+   some blocks might be too small or far too large.  */
+static struct pthread *
+get_cached_stack (size_t *sizep, void **memp)
+{
+  size_t size = *sizep;
+  struct pthread *result = NULL;
+  list_t *entry;
+
+  lll_lock (stack_cache_lock);
+
+  /* Search the cache for a matching entry.  We search for the
+     smallest stack which has at least the required size.  Note that
+     in normal situations the size of all allocated stacks is the
+     same.  As the very least there are only a few different sizes.
+     Therefore this loop will exit early most of the time with an
+     exact match.  */
+  list_for_each (entry, &stack_cache)
+    {
+      struct pthread *curr;
+
+      curr = list_entry (entry, struct pthread, list);
+      if (FREE_P (curr) && curr->stackblock_size >= size)
+	{
+	  if (curr->stackblock_size == size)
+	    {
+	      result = curr;
+	      break;
+	    }
+
+	  if (result == NULL
+	      || result->stackblock_size > curr->stackblock_size)
+	    result = curr;
+	}
+    }
+
+  if (__builtin_expect (result == NULL, 0)
+      /* Make sure the size difference is not too excessive.  In that
+	 case we do not use the block.  */
+      || __builtin_expect (result->stackblock_size > 4 * size, 0))
+    {
+      /* Release the lock.  */
+      lll_unlock (stack_cache_lock);
+
+      return NULL;
+    }
+
+  /* Dequeue the entry.  */
+  list_del (&result->list);
+
+  /* And add to the list of stacks in use.  */
+  list_add (&result->list, &stack_used);
+
+  /* And decrease the cache size.  */
+  stack_cache_actsize -= result->stackblock_size;
+
+  /* Release the lock early.  */
+  lll_unlock (stack_cache_lock);
+
+  /* Report size and location of the stack to the caller.  */
+  *sizep = result->stackblock_size;
+  *memp = result->stackblock;
+
+  /* Cancellation handling is back to the default.  */
+  result->cancelhandling = 0;
+  result->cleanup = NULL;
+
+  /* No pending event.  */
+  result->nextevent = NULL;
+
+  /* Clear the DTV.  */
+  dtv_t *dtv = GET_DTV (TLS_TPADJ (result));
+  memset (dtv, '\0', (dtv[-1].counter + 1) * sizeof (dtv_t));
+
+  /* Re-initialize the TLS.  */
+  _dl_allocate_tls_init (TLS_TPADJ (result));
+
+  return result;
+}
+
+
+/* Add a stack frame which is not used anymore to the stack.  Must be
+   called with the cache lock held.  */
+static inline void
+__attribute ((always_inline))
+queue_stack (struct pthread *stack)
+{
+  /* We unconditionally add the stack to the list.  The memory may
+     still be in use but it will not be reused until the kernel marks
+     the stack as not used anymore.  */
+  list_add (&stack->list, &stack_cache);
+
+  stack_cache_actsize += stack->stackblock_size;
+  if (__builtin_expect (stack_cache_actsize > stack_cache_maxsize, 0))
+    {
+      /* We reduce the size of the cache.  Remove the last entries
+	 until the size is below the limit.  */
+      list_t *entry;
+      list_t *prev;
+
+      /* Search from the end of the list.  */
+      list_for_each_prev_safe (entry, prev, &stack_cache)
+	{
+	  struct pthread *curr;
+
+	  curr = list_entry (entry, struct pthread, list);
+	  if (FREE_P (curr))
+	    {
+	      /* Unlink the block.  */
+	      list_del (entry);
+
+	      /* Account for the freed memory.  */
+	      stack_cache_actsize -= curr->stackblock_size;
+
+	      /* Free the memory associated with the ELF TLS.  */
+	      _dl_deallocate_tls (TLS_TPADJ (curr), false);
+
+	      /* Remove this block.  This should never fail.  If it
+		 does something is really wrong.  */
+	      if (munmap (curr->stackblock, curr->stackblock_size) != 0)
+		abort ();
+
+	      /* Maybe we have freed enough.  */
+	      if (stack_cache_actsize <= stack_cache_maxsize)
+		break;
+	    }
+	}
+    }
+}
+
+
+static int
+internal_function
+change_stack_perm (struct pthread *pd
+#ifdef NEED_SEPARATE_REGISTER_STACK
+		   , size_t pagemask
+#endif
+		   )
+{
+#ifdef NEED_SEPARATE_REGISTER_STACK
+  void *stack = (pd->stackblock
+		 + (((((pd->stackblock_size - pd->guardsize) / 2)
+		      & pagemask) + pd->guardsize) & pagemask));
+  size_t len = pd->stackblock + pd->stackblock_size - stack;
+#else
+  void *stack = pd->stackblock + pd->guardsize;
+  size_t len = pd->stackblock_size - pd->guardsize;
+#endif
+  if (mprotect (stack, len, PROT_READ | PROT_WRITE | PROT_EXEC) != 0)
+    return errno;
+
+  return 0;
+}
+
+
+static int
+allocate_stack (const struct pthread_attr *attr, struct pthread **pdp,
+		ALLOCATE_STACK_PARMS)
+{
+  struct pthread *pd;
+  size_t size;
+  size_t pagesize_m1 = __getpagesize () - 1;
+  void *stacktop;
+
+  assert (attr != NULL);
+  assert (powerof2 (pagesize_m1 + 1));
+  assert (TCB_ALIGNMENT >= STACK_ALIGN);
+
+  /* Get the stack size from the attribute if it is set.  Otherwise we
+     use the default we determined at start time.  */
+  size = attr->stacksize ?: __default_stacksize;
+
+  /* Get memory for the stack.  */
+  if (__builtin_expect (attr->flags & ATTR_FLAG_STACKADDR, 0))
+    {
+      uintptr_t adj;
+
+      /* If the user also specified the size of the stack make sure it
+	 is large enough.  */
+      if (attr->stacksize != 0
+	  && attr->stacksize < (__static_tls_size + MINIMAL_REST_STACK))
+	return EINVAL;
+
+      /* Adjust stack size for alignment of the TLS block.  */
+#if TLS_TCB_AT_TP
+      adj = ((uintptr_t) attr->stackaddr - TLS_TCB_SIZE)
+	    & __static_tls_align_m1;
+      assert (size > adj + TLS_TCB_SIZE);
+#elif TLS_DTV_AT_TP
+      adj = ((uintptr_t) attr->stackaddr - __static_tls_size)
+	    & __static_tls_align_m1;
+      assert (size > adj);
+#endif
+
+      /* The user provided some memory.  Let's hope it matches the
+	 size...  We do not allocate guard pages if the user provided
+	 the stack.  It is the user's responsibility to do this if it
+	 is wanted.  */
+#if TLS_TCB_AT_TP
+      pd = (struct pthread *) ((uintptr_t) attr->stackaddr
+			       - TLS_TCB_SIZE - adj);
+#elif TLS_DTV_AT_TP
+      pd = (struct pthread *) (((uintptr_t) attr->stackaddr
+			        - __static_tls_size - adj)
+			       - TLS_PRE_TCB_SIZE);
+#endif
+
+      /* The user provided stack memory needs to be cleared.  */
+      memset (pd, '\0', sizeof (struct pthread));
+
+      /* The first TSD block is included in the TCB.  */
+      pd->specific[0] = pd->specific_1stblock;
+
+      /* Remember the stack-related values.  */
+      pd->stackblock = (char *) attr->stackaddr - size;
+      pd->stackblock_size = size;
+
+      /* This is a user-provided stack.  It will not be queued in the
+	 stack cache nor will the memory (except the TLS memory) be freed.  */
+      pd->user_stack = true;
+
+      /* This is at least the second thread.  */
+      pd->header.multiple_threads = 1;
+#ifndef TLS_MULTIPLE_THREADS_IN_TCB
+      __pthread_multiple_threads = *__libc_multiple_threads_ptr = 1;
+#endif
+
+#ifdef NEED_DL_SYSINFO
+      /* Copy the sysinfo value from the parent.  */
+      THREAD_SYSINFO(pd) = THREAD_SELF_SYSINFO;
+#endif
+
+      /* The process ID is also the same as that of the caller.  */
+      pd->pid = THREAD_GETMEM (THREAD_SELF, pid);
+
+      /* Allocate the DTV for this thread.  */
+      if (_dl_allocate_tls (TLS_TPADJ (pd)) == NULL)
+	{
+	  /* Something went wrong.  */
+	  assert (errno == ENOMEM);
+	  return EAGAIN;
+	}
+
+
+      /* Prepare to modify global data.  */
+      lll_lock (stack_cache_lock);
+
+      /* And add to the list of stacks in use.  */
+      list_add (&pd->list, &__stack_user);
+
+      lll_unlock (stack_cache_lock);
+    }
+  else
+    {
+      /* Allocate some anonymous memory.  If possible use the cache.  */
+      size_t guardsize;
+      size_t reqsize;
+      void *mem;
+      const int prot = (PROT_READ | PROT_WRITE
+			| ((GL(dl_stack_flags) & PF_X) ? PROT_EXEC : 0));
+
+#if COLORING_INCREMENT != 0
+      /* Add one more page for stack coloring.  Don't do it for stacks
+	 with 16 times pagesize or larger.  This might just cause
+	 unnecessary misalignment.  */
+      if (size <= 16 * pagesize_m1)
+	size += pagesize_m1 + 1;
+#endif
+
+      /* Adjust the stack size for alignment.  */
+      size &= ~__static_tls_align_m1;
+      assert (size != 0);
+
+      /* Make sure the size of the stack is enough for the guard and
+	 eventually the thread descriptor.  */
+      guardsize = (attr->guardsize + pagesize_m1) & ~pagesize_m1;
+      if (__builtin_expect (size < (guardsize + __static_tls_size
+				    + MINIMAL_REST_STACK + pagesize_m1 + 1),
+			    0))
+	/* The stack is too small (or the guard too large).  */
+	return EINVAL;
+
+      /* Try to get a stack from the cache.  */
+      reqsize = size;
+      pd = get_cached_stack (&size, &mem);
+      if (pd == NULL)
+	{
+	  /* To avoid aliasing effects on a larger scale than pages we
+	     adjust the allocated stack size if necessary.  This way
+	     allocations directly following each other will not have
+	     aliasing problems.  */
+#if MULTI_PAGE_ALIASING != 0
+	  if ((size % MULTI_PAGE_ALIASING) == 0)
+	    size += pagesize_m1 + 1;
+#endif
+
+	  mem = mmap (NULL, size, prot,
+		      MAP_PRIVATE | MAP_ANONYMOUS | ARCH_MAP_FLAGS, -1, 0);
+
+	  if (__builtin_expect (mem == MAP_FAILED, 0))
+	    {
+#ifdef ARCH_RETRY_MMAP
+	      mem = ARCH_RETRY_MMAP (size);
+	      if (__builtin_expect (mem == MAP_FAILED, 0))
+#endif
+		return errno;
+	    }
+
+	  /* SIZE is guaranteed to be greater than zero.
+	     So we can never get a null pointer back from mmap.  */
+	  assert (mem != NULL);
+
+#if COLORING_INCREMENT != 0
+	  /* Atomically increment NCREATED.  */
+	  unsigned int ncreated = atomic_increment_val (&nptl_ncreated);
+
+	  /* We chose the offset for coloring by incrementing it for
+	     every new thread by a fixed amount.  The offset used
+	     module the page size.  Even if coloring would be better
+	     relative to higher alignment values it makes no sense to
+	     do it since the mmap() interface does not allow us to
+	     specify any alignment for the returned memory block.  */
+	  size_t coloring = (ncreated * COLORING_INCREMENT) & pagesize_m1;
+
+	  /* Make sure the coloring offsets does not disturb the alignment
+	     of the TCB and static TLS block.  */
+	  if (__builtin_expect ((coloring & __static_tls_align_m1) != 0, 0))
+	    coloring = (((coloring + __static_tls_align_m1)
+			 & ~(__static_tls_align_m1))
+			& ~pagesize_m1);
+#else
+	  /* Unless specified we do not make any adjustments.  */
+# define coloring 0
+#endif
+
+	  /* Place the thread descriptor at the end of the stack.  */
+#if TLS_TCB_AT_TP
+	  pd = (struct pthread *) ((char *) mem + size - coloring) - 1;
+#elif TLS_DTV_AT_TP
+	  pd = (struct pthread *) ((((uintptr_t) mem + size - coloring
+				    - __static_tls_size)
+				    & ~__static_tls_align_m1)
+				   - TLS_PRE_TCB_SIZE);
+#endif
+
+	  /* Remember the stack-related values.  */
+	  pd->stackblock = mem;
+	  pd->stackblock_size = size;
+
+	  /* We allocated the first block thread-specific data array.
+	     This address will not change for the lifetime of this
+	     descriptor.  */
+	  pd->specific[0] = pd->specific_1stblock;
+
+	  /* This is at least the second thread.  */
+	  pd->header.multiple_threads = 1;
+#ifndef TLS_MULTIPLE_THREADS_IN_TCB
+	  __pthread_multiple_threads = *__libc_multiple_threads_ptr = 1;
+#endif
+
+#ifdef NEED_DL_SYSINFO
+	  /* Copy the sysinfo value from the parent.  */
+	  THREAD_SYSINFO(pd) = THREAD_SELF_SYSINFO;
+#endif
+
+	  /* The process ID is also the same as that of the caller.  */
+	  pd->pid = THREAD_GETMEM (THREAD_SELF, pid);
+
+	  /* Allocate the DTV for this thread.  */
+	  if (_dl_allocate_tls (TLS_TPADJ (pd)) == NULL)
+	    {
+	      /* Something went wrong.  */
+	      assert (errno == ENOMEM);
+
+	      /* Free the stack memory we just allocated.  */
+	      (void) munmap (mem, size);
+
+	      return EAGAIN;
+	    }
+
+
+	  /* Prepare to modify global data.  */
+	  lll_lock (stack_cache_lock);
+
+	  /* And add to the list of stacks in use.  */
+	  list_add (&pd->list, &stack_used);
+
+	  lll_unlock (stack_cache_lock);
+
+
+	  /* There might have been a race.  Another thread might have
+	     caused the stacks to get exec permission while this new
+	     stack was prepared.  Detect if this was possible and
+	     change the permission if necessary.  */
+	  if (__builtin_expect ((GL(dl_stack_flags) & PF_X) != 0
+				&& (prot & PROT_EXEC) == 0, 0))
+	    {
+	      int err = change_stack_perm (pd
+#ifdef NEED_SEPARATE_REGISTER_STACK
+					   , ~pagesize_m1
+#endif
+					   );
+	      if (err != 0)
+		{
+		  /* Free the stack memory we just allocated.  */
+		  (void) munmap (mem, size);
+
+		  return err;
+		}
+	    }
+
+
+	  /* Note that all of the stack and the thread descriptor is
+	     zeroed.  This means we do not have to initialize fields
+	     with initial value zero.  This is specifically true for
+	     the 'tid' field which is always set back to zero once the
+	     stack is not used anymore and for the 'guardsize' field
+	     which will be read next.  */
+	}
+
+      /* Create or resize the guard area if necessary.  */
+      if (__builtin_expect (guardsize > pd->guardsize, 0))
+	{
+#ifdef NEED_SEPARATE_REGISTER_STACK
+	  char *guard = mem + (((size - guardsize) / 2) & ~pagesize_m1);
+#else
+	  char *guard = mem;
+#endif
+	  if (mprotect (guard, guardsize, PROT_NONE) != 0)
+	    {
+	      int err;
+	    mprot_error:
+	      err = errno;
+
+	      lll_lock (stack_cache_lock);
+
+	      /* Remove the thread from the list.  */
+	      list_del (&pd->list);
+
+	      lll_unlock (stack_cache_lock);
+
+	      /* Get rid of the TLS block we allocated.  */
+	      _dl_deallocate_tls (TLS_TPADJ (pd), false);
+
+	      /* Free the stack memory regardless of whether the size
+		 of the cache is over the limit or not.  If this piece
+		 of memory caused problems we better do not use it
+		 anymore.  Uh, and we ignore possible errors.  There
+		 is nothing we could do.  */
+	      (void) munmap (mem, size);
+
+	      return err;
+	    }
+
+	  pd->guardsize = guardsize;
+	}
+      else if (__builtin_expect (pd->guardsize - guardsize > size - reqsize,
+				 0))
+	{
+	  /* The old guard area is too large.  */
+
+#ifdef NEED_SEPARATE_REGISTER_STACK
+	  char *guard = mem + (((size - guardsize) / 2) & ~pagesize_m1);
+	  char *oldguard = mem + (((size - pd->guardsize) / 2) & ~pagesize_m1);
+
+	  if (oldguard < guard
+	      && mprotect (oldguard, guard - oldguard, prot) != 0)
+	    goto mprot_error;
+
+	  if (mprotect (guard + guardsize,
+			oldguard + pd->guardsize - guard - guardsize,
+			prot) != 0)
+	    goto mprot_error;
+#else
+	  if (mprotect ((char *) mem + guardsize, pd->guardsize - guardsize,
+			prot) != 0)
+	    goto mprot_error;
+#endif
+
+	  pd->guardsize = guardsize;
+	}
+      /* The pthread_getattr_np() calls need to get passed the size
+	 requested in the attribute, regardless of how large the
+	 actually used guardsize is.  */
+      pd->reported_guardsize = guardsize;
+    }
+
+  /* Initialize the lock.  We have to do this unconditionally since the
+     stillborn thread could be canceled while the lock is taken.  */
+  pd->lock = LLL_LOCK_INITIALIZER;
+
+  /* We place the thread descriptor at the end of the stack.  */
+  *pdp = pd;
+
+#if TLS_TCB_AT_TP
+  /* The stack begins before the TCB and the static TLS block.  */
+  stacktop = ((char *) (pd + 1) - __static_tls_size);
+#elif TLS_DTV_AT_TP
+  stacktop = (char *) (pd - 1);
+#endif
+
+#ifdef NEED_SEPARATE_REGISTER_STACK
+  *stack = pd->stackblock;
+  *stacksize = stacktop - *stack;
+#else
+  *stack = stacktop;
+#endif
+
+  return 0;
+}
+
+
+void
+internal_function
+__deallocate_stack (struct pthread *pd)
+{
+  lll_lock (stack_cache_lock);
+
+  /* Remove the thread from the list of threads with user defined
+     stacks.  */
+  list_del (&pd->list);
+
+  /* Not much to do.  Just free the mmap()ed memory.  Note that we do
+     not reset the 'used' flag in the 'tid' field.  This is done by
+     the kernel.  If no thread has been created yet this field is
+     still zero.  */
+  if (__builtin_expect (! pd->user_stack, 1))
+    (void) queue_stack (pd);
+  else
+    /* Free the memory associated with the ELF TLS.  */
+    _dl_deallocate_tls (TLS_TPADJ (pd), false);
+
+  lll_unlock (stack_cache_lock);
+}
+
+
+int
+internal_function
+__make_stacks_executable (void **stack_endp)
+{
+  /* First the main thread's stack.  */
+  int err = _dl_make_stack_executable (stack_endp);
+  if (err != 0)
+    return err;
+
+#ifdef NEED_SEPARATE_REGISTER_STACK
+  const size_t pagemask = ~(__getpagesize () - 1);
+#endif
+
+  lll_lock (stack_cache_lock);
+
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    {
+      err = change_stack_perm (list_entry (runp, struct pthread, list)
+#ifdef NEED_SEPARATE_REGISTER_STACK
+			       , pagemask
+#endif
+			       );
+      if (err != 0)
+	break;
+    }
+
+  /* Also change the permission for the currently unused stacks.  This
+     might be wasted time but better spend it here than adding a check
+     in the fast path.  */
+  if (err == 0)
+    list_for_each (runp, &stack_cache)
+      {
+	err = change_stack_perm (list_entry (runp, struct pthread, list)
+#ifdef NEED_SEPARATE_REGISTER_STACK
+				 , pagemask
+#endif
+				 );
+	if (err != 0)
+	  break;
+      }
+
+  lll_unlock (stack_cache_lock);
+
+  return err;
+}
+
+
+/* In case of a fork() call the memory allocation in the child will be
+   the same but only one thread is running.  All stacks except that of
+   the one running thread are not used anymore.  We have to recycle
+   them.  */
+void
+__reclaim_stacks (void)
+{
+  struct pthread *self = (struct pthread *) THREAD_SELF;
+
+  /* No locking necessary.  The caller is the only stack in use.  */
+
+  /* Mark all stacks except the still running one as free.  */
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    {
+      struct pthread *curp;
+
+      curp = list_entry (runp, struct pthread, list);
+      if (curp != self)
+	{
+	  /* This marks the stack as free.  */
+	  curp->tid = 0;
+
+	  /* The PID field must be initialized for the new process.  */
+	  curp->pid = self->pid;
+
+	  /* Account for the size of the stack.  */
+	  stack_cache_actsize += curp->stackblock_size;
+	}
+    }
+
+  /* Add the stack of all running threads to the cache.  */
+  list_splice (&stack_used, &stack_cache);
+
+  /* Remove the entry for the current thread to from the cache list
+     and add it to the list of running threads.  Which of the two
+     lists is decided by the user_stack flag.  */
+  list_del (&self->list);
+
+  /* Re-initialize the lists for all the threads.  */
+  INIT_LIST_HEAD (&stack_used);
+  INIT_LIST_HEAD (&__stack_user);
+
+  if (__builtin_expect (THREAD_GETMEM (self, user_stack), 0))
+    list_add (&self->list, &__stack_user);
+  else
+    list_add (&self->list, &stack_used);
+
+  /* There is one thread running.  */
+  __nptl_nthreads = 1;
+
+  /* Initialize the lock.  */
+  stack_cache_lock = LLL_LOCK_INITIALIZER;
+}
+
+
+#if HP_TIMING_AVAIL
+# undef __find_thread_by_id
+/* Find a thread given the thread ID.  */
+attribute_hidden
+struct pthread *
+__find_thread_by_id (pid_t tid)
+{
+  struct pthread *result = NULL;
+
+  lll_lock (stack_cache_lock);
+
+  /* Iterate over the list with system-allocated threads first.  */
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    {
+      struct pthread *curp;
+
+      curp = list_entry (runp, struct pthread, list);
+
+      if (curp->tid == tid)
+	{
+	  result = curp;
+	  goto out;
+	}
+    }
+
+  /* Now the list with threads using user-allocated stacks.  */
+  list_for_each (runp, &__stack_user)
+    {
+      struct pthread *curp;
+
+      curp = list_entry (runp, struct pthread, list);
+
+      if (curp->tid == tid)
+	{
+	  result = curp;
+	  goto out;
+	}
+    }
+
+ out:
+  lll_unlock (stack_cache_lock);
+
+  return result;
+}
+#endif
+
+int
+attribute_hidden
+__nptl_setxid (struct xid_command *cmdp)
+{
+#if 0
+  int result;
+  lll_lock (stack_cache_lock);
+
+  __xidcmd = cmdp;
+  cmdp->cntr = 0;
+
+  INTERNAL_SYSCALL_DECL (err);
+
+  struct pthread *self = THREAD_SELF;
+
+  /* Iterate over the list with system-allocated threads first.  */
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    {
+      struct pthread *t = list_entry (runp, struct pthread, list);
+      if (t != self)
+	{
+	  int val;
+#if __ASSUME_TGKILL
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid),
+				  t->tid, SIGSETXID);
+#else
+# ifdef __NR_tgkill
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid),
+				  t->tid, SIGSETXID);
+	  if (INTERNAL_SYSCALL_ERROR_P (val, err)
+	      && INTERNAL_SYSCALL_ERRNO (val, err) == ENOSYS)
+# endif
+	    val = INTERNAL_SYSCALL (tkill, err, 2, t->tid, SIGSETXID);
+#endif
+
+	  if (!INTERNAL_SYSCALL_ERROR_P (val, err))
+	    atomic_increment (&cmdp->cntr);
+	}
+    }
+
+  /* Now the list with threads using user-allocated stacks.  */
+  list_for_each (runp, &__stack_user)
+    {
+      struct pthread *t = list_entry (runp, struct pthread, list);
+      if (t != self)
+	{
+	  int val;
+#if __ASSUME_TGKILL
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid),
+				  t->tid, SIGSETXID);
+#else
+# ifdef __NR_tgkill
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid),
+				  t->tid, SIGSETXID);
+	  if (INTERNAL_SYSCALL_ERROR_P (val, err)
+	      && INTERNAL_SYSCALL_ERRNO (val, err) == ENOSYS)
+# endif
+	    val = INTERNAL_SYSCALL (tkill, err, 2, t->tid, SIGSETXID);
+#endif
+
+	  if (!INTERNAL_SYSCALL_ERROR_P (val, err))
+	    atomic_increment (&cmdp->cntr);
+	}
+    }
+
+  int cur = cmdp->cntr;
+  while (cur != 0)
+    {
+      lll_futex_wait (&cmdp->cntr, cur);
+      cur = cmdp->cntr;
+    }
+
+  /* This must be last, otherwise the current thread might not have
+     permissions to send SIGSETXID syscall to the other threads.  */
+  result = INTERNAL_SYSCALL_NCS (cmdp->syscall_no, err, 3,
+				 cmdp->id[0], cmdp->id[1], cmdp->id[2]);
+  if (INTERNAL_SYSCALL_ERROR_P (result, err))
+    {
+      __set_errno (INTERNAL_SYSCALL_ERRNO (result, err));
+      result = -1;
+    }
+
+  lll_unlock (stack_cache_lock);
+  return result;
+#endif
+}
+
+static inline void __attribute__((always_inline))
+init_one_static_tls (struct pthread *curp, struct link_map *map)
+{
+  dtv_t *dtv = GET_DTV (TLS_TPADJ (curp));
+# if TLS_TCB_AT_TP
+  void *dest = (char *) curp - map->l_tls_offset;
+# elif TLS_DTV_AT_TP
+  void *dest = (char *) curp + map->l_tls_offset + TLS_PRE_TCB_SIZE;
+# else
+#  error "Either TLS_TCB_AT_TP or TLS_DTV_AT_TP must be defined"
+# endif
+
+  /* Fill in the DTV slot so that a later LD/GD access will find it.  */
+  dtv[map->l_tls_modid].pointer.val = dest;
+  dtv[map->l_tls_modid].pointer.is_static = true;
+
+  /* Initialize the memory.  */
+  memset (__mempcpy (dest, map->l_tls_initimage, map->l_tls_initimage_size),
+	  '\0', map->l_tls_blocksize - map->l_tls_initimage_size);
+}
+
+void
+attribute_hidden
+__pthread_init_static_tls (struct link_map *map)
+{
+  lll_lock (stack_cache_lock);
+
+  /* Iterate over the list with system-allocated threads first.  */
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    init_one_static_tls (list_entry (runp, struct pthread, list), map);
+
+  /* Now the list with threads using user-allocated stacks.  */
+  list_for_each (runp, &__stack_user)
+    init_one_static_tls (list_entry (runp, struct pthread, list), map);
+
+  lll_unlock (stack_cache_lock);
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/bits/local_lim.h libc/nptl/sysdeps/l4/hurd/bits/local_lim.h
--- libc/nptl/sysdeps/l4/hurd/bits/local_lim.h	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/bits/local_lim.h	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,71 @@
+/* Minimum guaranteed maximum values for system limits.  Hurd version.
+   Copyright (C) 1993,94,96,98,2002 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+/* GNU has no arbitrary fixed limits on most of these things, so we
+   don't define the macros.  Some things are unlimited.  Some are in
+   fact limited but the limit is run-time dependent and fetched with
+   `sysconf' or `pathconf'.  */
+
+/* This one value is actually constrained by the `struct dirent'
+   layout, in which the `d_namlen' member is only 8 bits wide.  */
+
+#define NAME_MAX	255
+
+/* POSIX.1 requires that we define NGROUPS_MAX (though none of the others
+   is required).  GNU allows any number of supplementary groups,
+   dynamically allocated.  So we pick a number which seems vaguely
+   suitable, and `sysconf' will return a number at least as large.  */
+
+#define NGROUPS_MAX	256
+
+/* The maximum number of symbolic links that are allowed in a single file
+   name resolution.  When a further link is encountered, the call returns
+   ELOOP.  This name is a GNU extension; POSIX.1 has no such limit, and BSD
+   calls it MAXSYMLINKS in <sys/param.h>.  (We define the name under
+   _BSD_SOURCE even without _GNU_SOURCE because our <sys/param.h> uses it
+   to define MAXSYMLINKS.)  */
+
+#if defined __USE_GNU || defined __USE_BSD /* 1003.1a defines this */
+#define	SYMLOOP_MAX	8
+#endif
+
+/* The number of data keys per process.  */
+#define _POSIX_THREAD_KEYS_MAX	128
+/* This is the value this implementation supports.  */
+#define PTHREAD_KEYS_MAX	1024
+
+/* Controlling the iterations of destructors for thread-specific data.  */
+#define _POSIX_THREAD_DESTRUCTOR_ITERATIONS	4
+/* Number of iterations this implementation does.  */
+#define PTHREAD_DESTRUCTOR_ITERATIONS	_POSIX_THREAD_DESTRUCTOR_ITERATIONS
+
+/* The number of threads per process.  */
+#define _POSIX_THREAD_THREADS_MAX	64
+/* We have no predefined limit on the number of threads.  */
+#undef PTHREAD_THREADS_MAX
+
+/* Maximum amount by which a process can descrease its asynchronous I/O
+   priority level.  */
+#define AIO_PRIO_DELTA_MAX	20
+
+/* Minimum size for a thread.  We are free to choose a reasonable value.  */
+#define PTHREAD_STACK_MIN	16384
+
+/* Maximum number of timer expiration overruns.  */
+#define DELAYTIMER_MAX	2147483647
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/createthread.c libc/nptl/sysdeps/l4/hurd/createthread.c
--- libc/nptl/sysdeps/l4/hurd/createthread.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/createthread.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,266 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <sched.h>
+#include <setjmp.h>
+#include <signal.h>
+#include <stdlib.h>
+#include <atomic.h>
+#include <ldsodefs.h>
+#include <tls.h>
+
+#if 0
+#include "kernel-features.h"
+#endif
+
+#define CLONE_SIGNAL    	(CLONE_SIGHAND | CLONE_THREAD)
+
+/* Unless otherwise specified, the thread "register" is going to be
+   initialized with a pointer to the TCB.  */
+#ifndef TLS_VALUE
+# define TLS_VALUE pd
+#endif
+
+#ifndef ARCH_CLONE
+# define ARCH_CLONE __clone
+#endif
+
+
+#ifndef TLS_MULTIPLE_THREADS_IN_TCB
+/* Variable set to a nonzero value if more than one thread runs or ran.  */
+int __pthread_multiple_threads attribute_hidden;
+/* Pointer to the corresponding variable in libc.  */
+int *__libc_multiple_threads_ptr attribute_hidden;
+#endif
+
+
+static int
+do_clone (struct pthread *pd, const struct pthread_attr *attr,
+	  int clone_flags, int (*fct) (void *), STACK_VARIABLES_PARMS,
+	  int stopped)
+{
+#ifdef PREPARE_CREATE
+  PREPARE_CREATE;
+#endif
+
+  if (stopped)
+    /* We Make sure the thread does not run far by forcing it to get a
+       lock.  We lock it here too so that the new thread cannot continue
+       until we tell it to.  */
+    lll_lock (pd->lock);
+
+  /* One more thread.  We cannot have the thread do this itself, since it
+     might exist but not have been scheduled yet by the time we've returned
+     and need to check the value to behave correctly.  We must do it before
+     creating the thread, in case it does get scheduled first and then
+     might mistakenly think it was the only thread.  In the failure case,
+     we momentarily store a false value; this doesn't matter because there
+     is no kosher thing a signal handler interrupting us right here can do
+     that cares whether the thread count is correct.  */
+  atomic_increment (&__nptl_nthreads);
+
+  if (ARCH_CLONE (fct, STACK_VARIABLES_ARGS, clone_flags,
+		  pd, &pd->tid, TLS_VALUE, &pd->tid) == -1)
+    {
+      atomic_decrement (&__nptl_nthreads); /* Oops, we lied for a second.  */
+
+      /* Failed.  If the thread is detached, remove the TCB here since
+	 the caller cannot do this.  The caller remembered the thread
+	 as detached and cannot reverify that it is not since it must
+	 not access the thread descriptor again.  */
+      if (IS_DETACHED (pd))
+	__deallocate_stack (pd);
+
+      return errno;
+    }
+
+#if 0
+  /* Now we have the possibility to set scheduling parameters etc.  */
+  if (__builtin_expect (stopped != 0, 0))
+    {
+      INTERNAL_SYSCALL_DECL (err);
+      int res = 0;
+
+      /* Set the affinity mask if necessary.  */
+      if (attr->cpuset != NULL)
+	{
+	  res = INTERNAL_SYSCALL (sched_setaffinity, err, 3, pd->tid,
+				  sizeof (cpu_set_t), attr->cpuset);
+
+	  if (__builtin_expect (INTERNAL_SYSCALL_ERROR_P (res, err), 0))
+	    {
+	      /* The operation failed.  We have to kill the thread.  First
+		 send it the cancellation signal.  */
+	      INTERNAL_SYSCALL_DECL (err2);
+	    err_out:
+#if __ASSUME_TGKILL
+	      (void) INTERNAL_SYSCALL (tgkill, err2, 3,
+				       THREAD_GETMEM (THREAD_SELF, pid),
+				       pd->tid, SIGCANCEL);
+#else
+	      (void) INTERNAL_SYSCALL (tkill, err2, 2, pd->tid, SIGCANCEL);
+#endif
+
+	      return (INTERNAL_SYSCALL_ERROR_P (res, err)
+		      ? INTERNAL_SYSCALL_ERRNO (res, err)
+		      : 0);
+	    }
+	}
+
+      /* Set the scheduling parameters.  */
+      if ((attr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0)
+	{
+	  res = INTERNAL_SYSCALL (sched_setscheduler, err, 3, pd->tid,
+				  pd->schedpolicy, &pd->schedparam);
+
+	  if (__builtin_expect (INTERNAL_SYSCALL_ERROR_P (res, err), 0))
+	    goto err_out;
+	}
+    }
+#endif
+
+  /* We now have for sure more than one thread.  The main thread might
+     not yet have the flag set.  No need to set the global variable
+     again if this is what we use.  */
+  THREAD_SETMEM (THREAD_SELF, header.multiple_threads, 1);
+
+  return 0;
+}
+
+
+static int
+create_thread (struct pthread *pd, const struct pthread_attr *attr,
+	       STACK_VARIABLES_PARMS)
+{
+#ifdef TLS_TCB_AT_TP
+  assert (pd->header.tcb != NULL);
+#endif
+
+  /* We rely heavily on various flags the CLONE function understands:
+
+     CLONE_VM, CLONE_FS, CLONE_FILES
+	These flags select semantics with shared address space and
+	file descriptors according to what POSIX requires.
+
+     CLONE_SIGNAL
+	This flag selects the POSIX signal semantics.
+
+     CLONE_SETTLS
+	The sixth parameter to CLONE determines the TLS area for the
+	new thread.
+
+     CLONE_PARENT_SETTID
+	The kernels writes the thread ID of the newly created thread
+	into the location pointed to by the fifth parameters to CLONE.
+
+	Note that it would be semantically equivalent to use
+	CLONE_CHILD_SETTID but it is be more expensive in the kernel.
+
+     CLONE_CHILD_CLEARTID
+	The kernels clears the thread ID of a thread that has called
+	sys_exit() in the location pointed to by the seventh parameter
+	to CLONE.
+
+     CLONE_DETACHED
+	No signal is generated if the thread exists and it is
+	automatically reaped.
+
+     The termination signal is chosen to be zero which means no signal
+     is sent.  */
+#if 0
+  int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGNAL
+		     | CLONE_SETTLS | CLONE_PARENT_SETTID
+		     | CLONE_CHILD_CLEARTID | CLONE_SYSVSEM
+#if __ASSUME_NO_CLONE_DETACHED == 0
+		     | CLONE_DETACHED
+#endif
+		     | 0);
+#endif
+
+  if (__builtin_expect (THREAD_GETMEM (THREAD_SELF, report_events), 0))
+    {
+      /* The parent thread is supposed to report events.  Check whether
+	 the TD_CREATE event is needed, too.  */
+      const int _idx = __td_eventword (TD_CREATE);
+      const uint32_t _mask = __td_eventmask (TD_CREATE);
+
+      if ((_mask & (__nptl_threads_events.event_bits[_idx]
+		    | pd->eventbuf.eventmask.event_bits[_idx])) != 0)
+	{
+	  /* We always must have the thread start stopped.  */
+	  pd->stopped_start = true;
+
+#if 0
+	  /* Create the thread.  We always create the thread stopped
+	     so that it does not get far before we tell the debugger.  */
+	  int res = do_clone (pd, attr, clone_flags, start_thread,
+			      STACK_VARIABLES_ARGS, 1);
+	  if (res == 0)
+	    {
+	      /* Now fill in the information about the new thread in
+		 the newly created thread's data structure.  We cannot let
+		 the new thread do this since we don't know whether it was
+		 already scheduled when we send the event.  */
+	      pd->eventbuf.eventnum = TD_CREATE;
+	      pd->eventbuf.eventdata = pd;
+
+	      /* Enqueue the descriptor.  */
+	      do
+		pd->nextevent = __nptl_last_event;
+	      while (atomic_compare_and_exchange_bool_acq (&__nptl_last_event,
+							   pd, pd->nextevent)
+		     != 0);
+
+	      /* Now call the function which signals the event.  */
+	      __nptl_create_event ();
+
+	      /* And finally restart the new thread.  */
+	      lll_unlock (pd->lock);
+	    }
+
+	  return res;
+#endif
+	}
+    }
+
+#ifdef NEED_DL_SYSINFO
+  assert (THREAD_SELF_SYSINFO == THREAD_SYSINFO (pd));
+#endif
+
+  /* Determine whether the newly created threads has to be started
+     stopped since we have to set the scheduling parameters or set the
+     affinity.  */
+  bool stopped = false;
+  if (attr != NULL && (attr->cpuset != NULL
+		       || (attr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0))
+    stopped = true;
+  pd->stopped_start = stopped;
+
+#if 0
+  /* Actually create the thread.  */
+  int res = do_clone (pd, attr, clone_flags, start_thread,
+		      STACK_VARIABLES_ARGS, stopped);
+
+  if (res == 0 && stopped)
+    /* And finally restart the new thread.  */
+    lll_unlock (pd->lock);
+
+  return res;
+#endif
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/fork.c libc/nptl/sysdeps/l4/hurd/fork.c
--- libc/nptl/sysdeps/l4/hurd/fork.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/fork.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,214 @@
+/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <assert.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <sys/types.h>
+#include <sysdep.h>
+#include <libio/libioP.h>
+#include <tls.h>
+#include "fork.h"
+#include <hp-timing.h>
+#include <ldsodefs.h>
+#include <bits/stdio-lock.h>
+#include <atomic.h>
+
+
+unsigned long int *__fork_generation_pointer;
+
+
+
+/* The single linked list of all currently registered for handlers.  */
+struct fork_handler *__fork_handlers;
+
+
+static void
+fresetlockfiles (void)
+{
+  _IO_ITER i;
+
+  for (i = _IO_iter_begin(); i != _IO_iter_end(); i = _IO_iter_next(i))
+    _IO_lock_init (*((_IO_lock_t *) _IO_iter_file(i)->_lock));
+}
+
+
+pid_t
+__libc_fork (void)
+{
+  pid_t pid;
+  struct used_handler
+  {
+    struct fork_handler *handler;
+    struct used_handler *next;
+  } *allp = NULL;
+
+  /* Run all the registered preparation handlers.  In reverse order.
+     While doing this we build up a list of all the entries.  */
+  struct fork_handler *runp;
+  while ((runp = __fork_handlers) != NULL)
+    {
+      unsigned int oldval = runp->refcntr;
+
+      if (oldval == 0)
+	/* This means some other thread removed the list just after
+	   the pointer has been loaded.  Try again.  Either the list
+	   is empty or we can retry it.  */
+	continue;
+
+      /* Bump the reference counter.  */
+      if (atomic_compare_and_exchange_bool_acq (&__fork_handlers->refcntr,
+						oldval + 1, oldval))
+	/* The value changed, try again.  */
+	continue;
+
+      /* We bumped the reference counter for the first entry in the
+	 list.  That means that none of the following entries will
+	 just go away.  The unloading code works in the order of the
+	 list.
+
+         While executing the registered handlers we are building a
+         list of all the entries so that we can go backward later on.  */
+      while (1)
+	{
+	  /* Execute the handler if there is one.  */
+	  if (runp->prepare_handler != NULL)
+	    runp->prepare_handler ();
+
+	  /* Create a new element for the list.  */
+	  struct used_handler *newp
+	    = (struct used_handler *) alloca (sizeof (*newp));
+	  newp->handler = runp;
+	  newp->next = allp;
+	  allp = newp;
+
+	  /* Advance to the next handler.  */
+	  runp = runp->next;
+	  if (runp == NULL)
+	    break;
+
+	  /* Bump the reference counter for the next entry.  */
+	  atomic_increment (&runp->refcntr);
+	}
+
+      /* We are done.  */
+      break;
+    }
+
+  _IO_list_lock ();
+
+#ifndef NDEBUG
+  pid_t ppid = THREAD_GETMEM (THREAD_SELF, tid);
+#endif
+
+  /* We need to prevent the getpid() code to update the PID field so
+     that, if a signal arrives in the child very early and the signal
+     handler uses getpid(), the value returned is correct.  */
+  pid_t parentpid = THREAD_GETMEM (THREAD_SELF, pid);
+  THREAD_SETMEM (THREAD_SELF, pid, -parentpid);
+
+#if 0
+#ifdef ARCH_FORK
+  pid = ARCH_FORK ();
+#else
+# error "ARCH_FORK must be defined so that the CLONE_SETTID flag is used"
+  pid = INLINE_SYSCALL (fork, 0);
+#endif
+#endif
+
+  if (pid == 0)
+    {
+      struct pthread *self = THREAD_SELF;
+
+      assert (THREAD_GETMEM (self, tid) != ppid);
+
+      if (__fork_generation_pointer != NULL)
+	*__fork_generation_pointer += 4;
+
+      /* Adjust the PID field for the new process.  */
+      THREAD_SETMEM (self, pid, THREAD_GETMEM (self, tid));
+
+#if HP_TIMING_AVAIL
+      /* The CPU clock of the thread and process have to be set to zero.  */
+      hp_timing_t now;
+      HP_TIMING_NOW (now);
+      THREAD_SETMEM (self, cpuclock_offset, now);
+      GL(dl_cpuclock_offset) = now;
+#endif
+
+      /* Reset the file list.  These are recursive mutexes.  */
+      fresetlockfiles ();
+
+      /* Reset locks in the I/O code.  */
+      _IO_list_resetlock ();
+
+      /* Reset the lock the dynamic loader uses to protect its data.  */
+      __rtld_lock_initialize (GL(dl_load_lock));
+
+      /* Run the handlers registered for the child.  */
+      while (allp != NULL)
+	{
+	  if (allp->handler->child_handler != NULL)
+	    allp->handler->child_handler ();
+
+	  /* Note that we do not have to wake any possible waiter.
+	     This is the only thread in the new process.  */
+	  --allp->handler->refcntr;
+
+	  /* XXX We could at this point look through the object pool
+	     and mark all objects not on the __fork_handlers list as
+	     unused.  This is necessary in case the fork() happened
+	     while another thread called dlclose() and that call had
+	     to create a new list.  */
+
+	  allp = allp->next;
+	}
+
+      /* Initialize the fork lock.  */
+      __fork_lock = (lll_lock_t) LLL_LOCK_INITIALIZER;
+    }
+  else
+    {
+      assert (THREAD_GETMEM (THREAD_SELF, tid) == ppid);
+
+      /* Restore the PID value.  */
+      THREAD_SETMEM (THREAD_SELF, pid, parentpid);
+
+      /* We execute this even if the 'fork' call failed.  */
+      _IO_list_unlock ();
+
+      /* Run the handlers registered for the parent.  */
+      while (allp != NULL)
+	{
+	  if (allp->handler->parent_handler != NULL)
+	    allp->handler->parent_handler ();
+
+	  if (atomic_decrement_and_test (&allp->handler->refcntr)
+	      && allp->handler->need_signal)
+	    lll_futex_wake (allp->handler->refcntr, 1);
+
+	  allp = allp->next;
+	}
+    }
+
+  return pid;
+}
+weak_alias (__libc_fork, __fork)
+libc_hidden_def (__fork)
+weak_alias (__libc_fork, fork)
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/fork.h libc/nptl/sysdeps/l4/hurd/fork.h
--- libc/nptl/sysdeps/l4/hurd/fork.h	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/fork.h	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,57 @@
+/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <lowlevellock.h>
+
+/* The fork generation counter, defined in libpthread.  */
+extern unsigned long int __fork_generation attribute_hidden;
+
+/* Pointer to the fork generation counter in the thread library.  */
+extern unsigned long int *__fork_generation_pointer attribute_hidden;
+
+/* Lock to protect allocation and deallocation of fork handlers.  */
+extern lll_lock_t __fork_lock attribute_hidden;
+
+/* Elements of the fork handler lists.  */
+struct fork_handler
+{
+  struct fork_handler *next;
+  void (*prepare_handler) (void);
+  void (*parent_handler) (void);
+  void (*child_handler) (void);
+  void *dso_handle;
+  unsigned int refcntr;
+  int need_signal;
+};
+
+/* The single linked list of all currently registered for handlers.  */
+extern struct fork_handler *__fork_handlers;
+
+
+/* Function to call to unregister fork handlers.  */
+extern void __unregister_atfork (void *dso_handle) attribute_hidden;
+#define UNREGISTER_ATFORK(dso_handle) __unregister_atfork (dso_handle)
+
+
+/* C library side function to register new fork handlers.  */
+extern int __register_atfork (void (*__prepare) (void),
+			      void (*__parent) (void),
+			      void (*__child) (void),
+			      void *dso_handle);
+libc_hidden_proto (__register_atfork)
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/i386/bits/pthreadtypes.h libc/nptl/sysdeps/l4/hurd/i386/bits/pthreadtypes.h
--- libc/nptl/sysdeps/l4/hurd/i386/bits/pthreadtypes.h	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/i386/bits/pthreadtypes.h	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,164 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#ifndef _BITS_PTHREADTYPES_H
+#define _BITS_PTHREADTYPES_H	1
+
+/* FIXME: Marcus */
+#define __SIZEOF_PTHREAD_ATTR_T 36
+#define __SIZEOF_PTHREAD_MUTEX_T 24
+#define __SIZEOF_PTHREAD_MUTEXATTR_T 4
+#define __SIZEOF_PTHREAD_COND_T 48
+#define __SIZEOF_PTHREAD_COND_COMPAT_T 12
+#define __SIZEOF_PTHREAD_CONDATTR_T 4
+#define __SIZEOF_PTHREAD_RWLOCK_T 32
+#define __SIZEOF_PTHREAD_RWLOCKATTR_T 8
+#define __SIZEOF_PTHREAD_BARRIER_T 20
+#define __SIZEOF_PTHREAD_BARRIERATTR_T 4
+
+
+/* Thread identifiers.  The structure of the attribute type is not
+   exposed on purpose.  */
+typedef unsigned long int pthread_t;
+
+
+typedef union
+{
+  char __size[__SIZEOF_PTHREAD_ATTR_T];
+  long int __align;
+} pthread_attr_t;
+
+
+/* Data structures for mutex handling.  The structure of the attribute
+   type is not exposed on purpose.  */
+typedef union
+{
+  struct
+  {
+    /* FIXME: Marcus */
+    int __lock;
+    unsigned int __count;
+    int __owner;
+    /* KIND must stay at this position in the structure to maintain
+       binary compatibility.  */
+    int __kind;
+    unsigned int __nusers;
+    int __spins;
+  } __data;
+  char __size[__SIZEOF_PTHREAD_MUTEX_T];
+  long int __align;
+} pthread_mutex_t;
+
+typedef union
+{
+  char __size[__SIZEOF_PTHREAD_MUTEXATTR_T];
+  long int __align;
+} pthread_mutexattr_t;
+
+
+/* Data structure for conditional variable handling.  The structure of
+   the attribute type is not exposed on purpose.  */
+typedef union
+{
+  struct
+  {
+    /* FIXME: Marcus */
+    int __lock;
+    unsigned int __futex;
+    __extension__ unsigned long long int __total_seq;
+    __extension__ unsigned long long int __wakeup_seq;
+    __extension__ unsigned long long int __woken_seq;
+    void *__mutex;
+    unsigned int __nwaiters;
+    unsigned int __broadcast_seq;
+  } __data;
+  char __size[__SIZEOF_PTHREAD_COND_T];
+  __extension__ long long int __align;
+} pthread_cond_t;
+
+typedef union
+{
+  char __size[__SIZEOF_PTHREAD_CONDATTR_T];
+  long int __align;
+} pthread_condattr_t;
+
+
+/* Keys for thread-specific data */
+typedef unsigned int pthread_key_t;
+
+
+/* Once-only execution */
+typedef int pthread_once_t;
+
+
+#if defined __USE_UNIX98 || defined __USE_XOPEN2K
+/* Data structure for read-write lock variable handling.  The
+   structure of the attribute type is not exposed on purpose.  */
+typedef union
+{
+  struct
+  {
+    /* FIXME: Marcus */
+    int __lock;
+    unsigned int __nr_readers;
+    unsigned int __readers_wakeup;
+    unsigned int __writer_wakeup;
+    unsigned int __nr_readers_queued;
+    unsigned int __nr_writers_queued;
+    /* FLAGS must stay at this position in the structure to maintain
+       binary compatibility.  */
+    unsigned int __flags;
+    int __writer;
+  } __data;
+  char __size[__SIZEOF_PTHREAD_RWLOCK_T];
+  long int __align;
+} pthread_rwlock_t;
+
+typedef union
+{
+  char __size[__SIZEOF_PTHREAD_RWLOCKATTR_T];
+  long int __align;
+} pthread_rwlockattr_t;
+#endif
+
+
+#ifdef __USE_XOPEN2K
+/* POSIX spinlock data type.  */
+typedef volatile int pthread_spinlock_t;
+
+
+/* POSIX barriers data type.  The structure of the type is
+   deliberately not exposed.  */
+typedef union
+{
+  char __size[__SIZEOF_PTHREAD_BARRIER_T];
+  long int __align;
+} pthread_barrier_t;
+
+typedef union
+{
+  char __size[__SIZEOF_PTHREAD_BARRIERATTR_T];
+  int __align;
+} pthread_barrierattr_t;
+#endif
+
+
+/* Extra attributes for the cleanup functions.  */
+#define __cleanup_fct_attribute __attribute ((regparm (1)))
+
+#endif	/* bits/pthreadtypes.h */
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/i386/bits/semaphore.h libc/nptl/sysdeps/l4/hurd/i386/bits/semaphore.h
--- libc/nptl/sysdeps/l4/hurd/i386/bits/semaphore.h	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/i386/bits/semaphore.h	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,39 @@
+/* Copyright (C) 2002 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#ifndef _SEMAPHORE_H
+# error "Never use <bits/semaphore.h> directly; include <semaphore.h> instead."
+#endif
+
+
+#define __SIZEOF_SEM_T	16
+
+
+/* Value returned if `sem_open' failed.  */
+#define SEM_FAILED      ((sem_t *) 0)
+
+/* Maximum value the semaphore can have.  */
+#define SEM_VALUE_MAX   (2147483647)
+
+
+typedef union
+{
+  char __size[__SIZEOF_SEM_T];
+  long int __align;
+} sem_t;
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/i386/pthread_once.S libc/nptl/sysdeps/l4/hurd/i386/pthread_once.S
--- libc/nptl/sysdeps/l4/hurd/i386/pthread_once.S	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/i386/pthread_once.S	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,183 @@
+/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <sysdep.h>
+
+#ifndef UP
+# define LOCK lock
+#else
+# define LOCK
+#endif
+
+#define SYS_futex	240
+#define FUTEX_WAKE	1
+
+	.comm	__fork_generation, 4, 4
+
+	.text
+
+
+	.globl	__pthread_once
+	.type	__pthread_once,@function
+	.align	16
+	cfi_startproc
+__pthread_once:
+#if 0
+	movl	4(%esp), %ecx
+	testl	$2, (%ecx)
+	jz	1f
+	xorl	%eax, %eax
+	ret
+
+1:	pushl	%ebx
+	cfi_adjust_cfa_offset (4)
+	cfi_rel_offset (3, 0)
+	pushl	%esi
+	cfi_adjust_cfa_offset (4)
+	cfi_rel_offset (6, 0)
+	movl	%ecx, %ebx
+	xorl	%esi, %esi
+
+	/* Not yet initialized or initialization in progress.
+	   Get the fork generation counter now.  */
+6:	movl	(%ebx), %eax
+#ifdef PIC
+	call	__i686.get_pc_thunk.cx
+	addl	$_GLOBAL_OFFSET_TABLE_, %ecx
+#endif
+
+5:	movl	%eax, %edx
+
+	testl	$2, %eax
+	jnz	4f
+
+	andl	$3, %edx
+#ifdef PIC
+	orl	__fork_generation@GOTOFF(%ecx), %edx
+#else
+	orl	__fork_generation, %edx
+#endif
+	orl	$1, %edx
+
+	LOCK
+	cmpxchgl %edx, (%ebx)
+	jnz	5b
+
+	/* Check whether another thread already runs the initializer.  */
+	testl	$1, %eax
+	jz	3f	/* No -> do it.  */
+
+	/* Check whether the initializer execution was interrupted
+	   by a fork.  */
+	xorl	%edx, %eax
+	testl	$0xfffffffc, %eax
+	jnz	3f	/* Different for generation -> run initializer.  */
+
+	/* Somebody else got here first.  Wait.  */
+	movl	%esi, %ecx		/* movl $FUTEX_WAIT, %ecx */
+	movl	$SYS_futex, %eax
+	ENTER_KERNEL
+	jmp	6b
+
+3:	/* Call the initializer function after setting up the
+	   cancellation handler.  Note that it is not possible here
+	   to use the unwind-based cleanup handling.  This would require
+	   that the user-provided function and all the code it calls
+	   is compiled with exceptions.  Unfortunately this cannot be
+	   guaranteed.  */
+	subl	$UNWINDBUFSIZE+8, %esp
+	cfi_adjust_cfa_offset (UNWINDBUFSIZE+8)
+	movl	%ecx, %ebx		/* PIC register value.  */
+
+	leal	8+UWJMPBUF(%esp), %eax
+	movl	$0, 4(%esp)
+	movl	%eax, (%esp)
+	call	__sigsetjmp@PLT
+	testl	%eax, %eax
+	jne	7f
+
+	leal	8(%esp), %eax
+	call	HIDDEN_JUMPTARGET(__pthread_register_cancel)
+
+	/* Call the user-provided initialization function.  */
+	call	*24+UNWINDBUFSIZE(%esp)
+
+	/* Pop the cleanup handler.  */
+	leal	8(%esp), %eax
+	call	HIDDEN_JUMPTARGET(__pthread_unregister_cancel)
+	addl	$UNWINDBUFSIZE+8, %esp
+	cfi_adjust_cfa_offset (-UNWINDBUFSIZE-8)
+
+	/* Sucessful run of the initializer.  Signal that we are done.  */
+	movl	12(%esp), %ebx
+	LOCK
+	addl	$1, (%ebx)
+
+	/* Wake up all other threads.  */
+	movl	$0x7fffffff, %edx
+	movl	$FUTEX_WAKE, %ecx
+	movl	$SYS_futex, %eax
+	ENTER_KERNEL
+
+4:	popl	%esi
+	cfi_adjust_cfa_offset (-4)
+	cfi_restore (6)
+	popl	%ebx
+	cfi_adjust_cfa_offset (-4)
+	cfi_restore (3)
+	xorl	%eax, %eax
+	ret
+
+7:	/* __sigsetjmp returned for the second time.  */
+	movl	20+UNWINDBUFSIZE(%esp), %ebx
+	cfi_adjust_cfa_offset (UNWINDBUFSIZE+16)
+	cfi_offset (3, -8)
+	cfi_offset (6, -12)
+	movl	$0, (%ebx)
+
+	movl	$0x7fffffff, %edx
+	movl	$FUTEX_WAKE, %ecx
+	movl	$SYS_futex, %eax
+	ENTER_KERNEL
+
+	leal	8(%esp), %eax
+	call	HIDDEN_JUMPTARGET (__pthread_unwind_next)
+#endif
+	/* NOTREACHED */
+	hlt
+	cfi_endproc
+	.size	__pthread_once,.-__pthread_once
+
+	.globl	__pthread_once_internal
+__pthread_once_internal = __pthread_once
+
+	.globl	pthread_once
+pthread_once = __pthread_once
+
+
+#ifdef PIC
+	.section .gnu.linkonce.t.__i686.get_pc_thunk.cx,"ax",@progbits
+	.globl	__i686.get_pc_thunk.cx
+	.hidden	__i686.get_pc_thunk.cx
+	.type	__i686.get_pc_thunk.cx,@function
+__i686.get_pc_thunk.cx:
+	movl (%esp), %ecx;
+	ret
+	.size	__i686.get_pc_thunk.cx,.-__i686.get_pc_thunk.cx
+#endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/Implies libc/nptl/sysdeps/l4/hurd/Implies
--- libc/nptl/sysdeps/l4/hurd/Implies	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/Implies	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1 @@
+pthread
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/init.c libc/nptl/sysdeps/l4/hurd/init.c
--- libc/nptl/sysdeps/l4/hurd/init.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/init.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,365 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <assert.h>
+#include <limits.h>
+#include <signal.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <sys/param.h>
+#include <sys/resource.h>
+#include <pthreadP.h>
+#include <atomic.h>
+#include <ldsodefs.h>
+#include <tls.h>
+#include <fork.h>
+#include <version.h>
+#include <shlib-compat.h>
+#include <smp.h>
+#include <lowlevellock.h>
+
+
+#ifndef __NR_set_tid_address
+/* XXX For the time being...  Once we can rely on the kernel headers
+   having the definition remove these lines.  */
+#if defined __s390__
+# define __NR_set_tid_address	252
+#elif defined __ia64__
+# define __NR_set_tid_address	1233
+#elif defined __i386__
+# define __NR_set_tid_address	258
+#elif defined __x86_64__
+# define __NR_set_tid_address	218
+#elif defined __powerpc__
+# define __NR_set_tid_address	232
+#elif defined __sparc__
+# define __NR_set_tid_address	166
+#else
+# error "define __NR_set_tid_address"
+#endif
+#endif
+
+
+/* Default stack size.  */
+size_t __default_stacksize attribute_hidden;
+
+/* Size and alignment of static TLS block.  */
+size_t __static_tls_size;
+size_t __static_tls_align_m1;
+
+/* Flag whether the machine is SMP or not.  */
+int __is_smp attribute_hidden;
+
+/* Version of the library, used in libthread_db to detect mismatches.  */
+static const char nptl_version[] __attribute_used__ = VERSION;
+
+
+#if defined USE_TLS && !defined SHARED
+extern void __libc_setup_tls (size_t tcbsize, size_t tcbalign);
+#endif
+
+
+#ifdef SHARED
+static const struct pthread_functions pthread_functions =
+  {
+    .ptr_pthread_attr_destroy = __pthread_attr_destroy,
+# if SHLIB_COMPAT(libpthread, GLIBC_2_0, GLIBC_2_1)
+    .ptr___pthread_attr_init_2_0 = __pthread_attr_init_2_0,
+# endif
+    .ptr___pthread_attr_init_2_1 = __pthread_attr_init_2_1,
+    .ptr_pthread_attr_getdetachstate = __pthread_attr_getdetachstate,
+    .ptr_pthread_attr_setdetachstate = __pthread_attr_setdetachstate,
+    .ptr_pthread_attr_getinheritsched = __pthread_attr_getinheritsched,
+    .ptr_pthread_attr_setinheritsched = __pthread_attr_setinheritsched,
+    .ptr_pthread_attr_getschedparam = __pthread_attr_getschedparam,
+    .ptr_pthread_attr_setschedparam = __pthread_attr_setschedparam,
+    .ptr_pthread_attr_getschedpolicy = __pthread_attr_getschedpolicy,
+    .ptr_pthread_attr_setschedpolicy = __pthread_attr_setschedpolicy,
+    .ptr_pthread_attr_getscope = __pthread_attr_getscope,
+    .ptr_pthread_attr_setscope = __pthread_attr_setscope,
+    .ptr_pthread_condattr_destroy = __pthread_condattr_destroy,
+    .ptr_pthread_condattr_init = __pthread_condattr_init,
+    .ptr___pthread_cond_broadcast = __pthread_cond_broadcast,
+    .ptr___pthread_cond_destroy = __pthread_cond_destroy,
+    .ptr___pthread_cond_init = __pthread_cond_init,
+    .ptr___pthread_cond_signal = __pthread_cond_signal,
+    .ptr___pthread_cond_wait = __pthread_cond_wait,
+    .ptr___pthread_cond_timedwait = __pthread_cond_timedwait,
+# if SHLIB_COMPAT(libpthread, GLIBC_2_0, GLIBC_2_3_2)
+    .ptr___pthread_cond_broadcast_2_0 = __pthread_cond_broadcast_2_0,
+    .ptr___pthread_cond_destroy_2_0 = __pthread_cond_destroy_2_0,
+    .ptr___pthread_cond_init_2_0 = __pthread_cond_init_2_0,
+    .ptr___pthread_cond_signal_2_0 = __pthread_cond_signal_2_0,
+    .ptr___pthread_cond_wait_2_0 = __pthread_cond_wait_2_0,
+    .ptr___pthread_cond_timedwait_2_0 = __pthread_cond_timedwait_2_0,
+# endif
+    .ptr_pthread_equal = __pthread_equal,
+    .ptr___pthread_exit = __pthread_exit,
+    .ptr_pthread_getschedparam = __pthread_getschedparam,
+    .ptr_pthread_setschedparam = __pthread_setschedparam,
+    .ptr_pthread_mutex_destroy = INTUSE(__pthread_mutex_destroy),
+    .ptr_pthread_mutex_init = INTUSE(__pthread_mutex_init),
+    .ptr_pthread_mutex_lock = INTUSE(__pthread_mutex_lock),
+    .ptr_pthread_mutex_unlock = INTUSE(__pthread_mutex_unlock),
+    .ptr_pthread_self = __pthread_self,
+    .ptr_pthread_setcancelstate = __pthread_setcancelstate,
+    .ptr_pthread_setcanceltype = __pthread_setcanceltype,
+    .ptr___pthread_cleanup_upto = __pthread_cleanup_upto,
+    .ptr___pthread_once = __pthread_once_internal,
+    .ptr___pthread_rwlock_rdlock = __pthread_rwlock_rdlock_internal,
+    .ptr___pthread_rwlock_wrlock = __pthread_rwlock_wrlock_internal,
+    .ptr___pthread_rwlock_unlock = __pthread_rwlock_unlock_internal,
+    .ptr___pthread_key_create = __pthread_key_create_internal,
+    .ptr___pthread_getspecific = __pthread_getspecific_internal,
+    .ptr___pthread_setspecific = __pthread_setspecific_internal,
+    .ptr__pthread_cleanup_push_defer = __pthread_cleanup_push_defer,
+    .ptr__pthread_cleanup_pop_restore = __pthread_cleanup_pop_restore,
+    .ptr_nthreads = &__nptl_nthreads,
+    .ptr___pthread_unwind = &__pthread_unwind,
+    .ptr__nptl_deallocate_tsd = __nptl_deallocate_tsd,
+    .ptr__nptl_setxid = __nptl_setxid
+  };
+# define ptr_pthread_functions &pthread_functions
+#else
+# define ptr_pthread_functions NULL
+#endif
+
+
+/* For asynchronous cancellation we use a signal.  This is the handler.  */
+static void
+sigcancel_handler (int sig, siginfo_t *si, void *ctx)
+{
+#if 0
+  /* FIXME: This needs to be implemented differently.  */
+
+  /* Safety check.  It would be possible to call this function for
+     other signals and send a signal from another process.  This is not
+     correct and might even be a security problem.  Try to catch as
+     many incorrect invocations as possible.  */
+  if (sig != SIGCANCEL
+#ifdef __ASSUME_CORRECT_SI_PID
+      /* Kernels before 2.5.75 stored the thread ID and not the process
+	 ID in si_pid so we skip this test.  */
+      || si->si_pid != THREAD_GETMEM (THREAD_SELF, pid)
+#endif
+      || si->si_code != SI_TKILL)
+    return;
+
+  struct pthread *self = THREAD_SELF;
+
+  int oldval = THREAD_GETMEM (self, cancelhandling);
+  while (1)
+    {
+      /* We are canceled now.  When canceled by another thread this flag
+	 is already set but if the signal is directly send (internally or
+	 from another process) is has to be done here.  */
+      int newval = oldval | CANCELING_BITMASK | CANCELED_BITMASK;
+
+      if (oldval == newval || (oldval & EXITING_BITMASK) != 0)
+	/* Already canceled or exiting.  */
+	break;
+
+      int curval = THREAD_ATOMIC_CMPXCHG_VAL (self, cancelhandling, newval,
+					      oldval);
+      if (curval == oldval)
+	{
+	  /* Set the return value.  */
+	  THREAD_SETMEM (self, result, PTHREAD_CANCELED);
+
+	  /* Make sure asynchronous cancellation is still enabled.  */
+	  if ((newval & CANCELTYPE_BITMASK) != 0)
+	    /* Run the registered destructors and terminate the thread.  */
+	    __do_cancel ();
+
+	  break;
+	}
+
+      oldval = curval;
+    }
+#endif
+}
+
+
+struct xid_command *__xidcmd attribute_hidden;
+
+/* For asynchronous cancellation we use a signal.  This is the handler.  */
+static void
+sighandler_setxid (int sig, siginfo_t *si, void *ctx)
+{
+#if 0
+  /* FIXME: This needs to be implemented differently.  */
+
+  /* Safety check.  It would be possible to call this function for
+     other signals and send a signal from another process.  This is not
+     correct and might even be a security problem.  Try to catch as
+     many incorrect invocations as possible.  */
+  if (sig != SIGSETXID
+#ifdef __ASSUME_CORRECT_SI_PID
+      /* Kernels before 2.5.75 stored the thread ID and not the process
+	 ID in si_pid so we skip this test.  */
+      || si->si_pid != THREAD_GETMEM (THREAD_SELF, pid)
+#endif
+      || si->si_code != SI_TKILL)
+    return;
+
+  INTERNAL_SYSCALL_DECL (err);
+  INTERNAL_SYSCALL_NCS (__xidcmd->syscall_no, err, 3, __xidcmd->id[0],
+			__xidcmd->id[1], __xidcmd->id[2]);
+
+  if (atomic_decrement_val (&__xidcmd->cntr) == 0)
+    lll_futex_wake (&__xidcmd->cntr, 1);
+
+#endif
+}
+
+
+/* When using __thread for this, we do it in libc so as not
+   to give libpthread its own TLS segment just for this.  */
+extern void **__libc_dl_error_tsd (void) __attribute__ ((const));
+
+
+void
+__pthread_initialize_minimal_internal (void)
+{
+#ifndef SHARED
+  /* Unlike in the dynamically linked case the dynamic linker has not
+     taken care of initializing the TLS data structures.  */
+  __libc_setup_tls (TLS_TCB_SIZE, TLS_TCB_ALIGN);
+
+  /* We must prevent gcc from being clever and move any of the
+     following code ahead of the __libc_setup_tls call.  This function
+     will initialize the thread register which is subsequently
+     used.  */
+  __asm __volatile ("");
+#endif
+
+  /* Minimal initialization of the thread descriptor.  */
+  struct pthread *pd = THREAD_SELF;
+#if 0
+  /* FIXME: Implement this.  */
+  INTERNAL_SYSCALL_DECL (err);
+  pd->pid = pd->tid = INTERNAL_SYSCALL (set_tid_address, err, 1, &pd->tid);
+#endif
+
+  THREAD_SETMEM (pd, specific[0], &pd->specific_1stblock[0]);
+  THREAD_SETMEM (pd, user_stack, true);
+  if (LLL_LOCK_INITIALIZER != 0)
+    THREAD_SETMEM (pd, lock, LLL_LOCK_INITIALIZER);
+#if HP_TIMING_AVAIL
+  THREAD_SETMEM (pd, cpuclock_offset, GL(dl_cpuclock_offset));
+#endif
+
+  /* Set initial thread's stack block from 0 up to __libc_stack_end.
+     It will be bigger than it actually is, but for unwind.c/pt-longjmp.c
+     purposes this is good enough.  */
+  THREAD_SETMEM (pd, stackblock_size, (size_t) __libc_stack_end);
+
+  /* Initialize the list of all running threads with the main thread.  */
+  INIT_LIST_HEAD (&__stack_user);
+  list_add (&pd->list, &__stack_user);
+
+
+#if 0
+  /* FIXME: This needs to be implemented somehow.  */
+  /* Install the cancellation signal handler.  If for some reason we
+     cannot install the handler we do not abort.  Maybe we should, but
+     it is only asynchronous cancellation which is affected.  */
+  struct sigaction sa;
+  sa.sa_sigaction = sigcancel_handler;
+  sa.sa_flags = SA_SIGINFO;
+  sigemptyset (&sa.sa_mask);
+
+  (void) __libc_sigaction (SIGCANCEL, &sa, NULL);
+
+  /* Install the handle to change the threads' uid/gid.  */
+  sa.sa_sigaction = sighandler_setxid;
+  sa.sa_flags = SA_SIGINFO | SA_RESTART;
+
+  (void) __libc_sigaction (SIGSETXID, &sa, NULL);
+
+  /* The parent process might have left the signal blocked.  Just in
+     case, unblock it.  We reuse the signal mask in the sigaction
+     structure.  It is already cleared.  */
+  __sigaddset (&sa.sa_mask, SIGCANCEL);
+  (void) INTERNAL_SYSCALL (rt_sigprocmask, err, 4, SIG_UNBLOCK, &sa.sa_mask,
+			   NULL, _NSIG / 8);
+#endif
+
+  /* Determine the default allowed stack size.  This is the size used
+     in case the user does not specify one.  */
+  struct rlimit limit;
+  if (getrlimit (RLIMIT_STACK, &limit) != 0
+      || limit.rlim_cur == RLIM_INFINITY)
+    /* The system limit is not usable.  Use an architecture-specific
+       default.  */
+    __default_stacksize = ARCH_STACK_DEFAULT_SIZE;
+  else if (limit.rlim_cur < PTHREAD_STACK_MIN)
+    /* The system limit is unusably small.
+       Use the minimal size acceptable.  */
+    __default_stacksize = PTHREAD_STACK_MIN;
+  else
+    {
+      /* Round the resource limit up to page size.  */
+      const uintptr_t pagesz = __sysconf (_SC_PAGESIZE);
+      __default_stacksize = (limit.rlim_cur + pagesz - 1) & -pagesz;
+    }
+
+  /* Get the size of the static and alignment requirements for the TLS
+     block.  */
+  size_t static_tls_align;
+  _dl_get_tls_static_info (&__static_tls_size, &static_tls_align);
+
+  /* Make sure the size takes all the alignments into account.  */
+  if (STACK_ALIGN > static_tls_align)
+    static_tls_align = STACK_ALIGN;
+  __static_tls_align_m1 = static_tls_align - 1;
+
+  __static_tls_size = roundup (__static_tls_size, static_tls_align);
+
+#ifdef SHARED
+  /* Transfer the old value from the dynamic linker's internal location.  */
+  *__libc_dl_error_tsd () = *(*GL(dl_error_catch_tsd)) ();
+  GL(dl_error_catch_tsd) = &__libc_dl_error_tsd;
+
+  /* Make __rtld_lock_{,un}lock_recursive use pthread_mutex_{,un}lock,
+     keep the lock count from the ld.so implementation.  */
+  GL(dl_rtld_lock_recursive) = (void *) INTUSE (__pthread_mutex_lock);
+  GL(dl_rtld_unlock_recursive) = (void *) INTUSE (__pthread_mutex_unlock);
+  unsigned int rtld_lock_count = GL(dl_load_lock).mutex.__data.__count;
+  GL(dl_load_lock).mutex.__data.__count = 0;
+  while (rtld_lock_count-- > 0)
+    INTUSE (__pthread_mutex_lock) (&GL(dl_load_lock).mutex);
+
+  GL(dl_make_stack_executable_hook) = &__make_stacks_executable;
+#endif
+
+  GL(dl_init_static_tls) = &__pthread_init_static_tls;
+
+  /* Register the fork generation counter with the libc.  */
+#ifndef TLS_MULTIPLE_THREADS_IN_TCB
+  __libc_multiple_threads_ptr =
+#endif
+    __libc_pthread_init (&__fork_generation, __reclaim_stacks,
+			 ptr_pthread_functions);
+
+  /* Determine whether the machine is SMP or not.  */
+  __is_smp = is_smp_system ();
+}
+strong_alias (__pthread_initialize_minimal_internal,
+	      __pthread_initialize_minimal)
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/internaltypes.h libc/nptl/sysdeps/l4/hurd/internaltypes.h
--- libc/nptl/sysdeps/l4/hurd/internaltypes.h	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/internaltypes.h	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,152 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#ifndef _INTERNALTYPES_H
+#define _INTERNALTYPES_H	1
+
+#include <stdint.h>
+
+
+struct pthread_attr
+{
+  /* Scheduler parameters and priority.  */
+  struct sched_param schedparam;
+  int schedpolicy;
+  /* Various flags like detachstate, scope, etc.  */
+  int flags;
+  /* Size of guard area.  */
+  size_t guardsize;
+  /* Stack handling.  */
+  void *stackaddr;
+  size_t stacksize;
+  /* Affinity map.  */
+  cpu_set_t *cpuset;
+  size_t cpusetsize;
+};
+
+#define ATTR_FLAG_DETACHSTATE		0x0001
+#define ATTR_FLAG_NOTINHERITSCHED	0x0002
+#define ATTR_FLAG_SCOPEPROCESS		0x0004
+#define ATTR_FLAG_STACKADDR		0x0008
+#define ATTR_FLAG_OLDATTR		0x0010
+#define ATTR_FLAG_SCHED_SET		0x0020
+#define ATTR_FLAG_POLICY_SET		0x0040
+
+
+/* Mutex attribute data structure.  */
+struct pthread_mutexattr
+{
+  /* Identifier for the kind of mutex.
+
+     Bit 31 is set if the mutex is to be shared between processes.
+
+     Bit 0 to 30 contain one of the PTHREAD_MUTEX_ values to identify
+     the type of the mutex.  */
+  int mutexkind;
+};
+
+
+/* Conditional variable attribute data structure.  */
+struct pthread_condattr
+{
+  /* Combination of values:
+
+     Bit 0  : flag whether coditional variable will be shareable between
+	      processes.
+
+     Bit 1-7: clock ID.  */
+  int value;
+};
+
+
+/* The __NWAITERS field is used as a counter and to house the number
+   of bits which represent the clock.  COND_CLOCK_BITS is the number
+   of bits reserved for the clock.  */
+#define COND_CLOCK_BITS	1
+
+
+/* Read-write lock variable attribute data structure.  */
+struct pthread_rwlockattr
+{
+  int lockkind;
+  int pshared;
+};
+
+
+/* Barrier data structure.  */
+struct pthread_barrier
+{
+  unsigned int curr_event;
+  int lock;
+  unsigned int left;
+  unsigned int init_count;
+};
+
+
+/* Barrier variable attribute data structure.  */
+struct pthread_barrierattr
+{
+  int pshared;
+};
+
+
+/* Thread-local data handling.  */
+struct pthread_key_struct
+{
+  /* Sequence numbers.  Even numbers indicated vacant entries.  Note
+     that zero is even.  We use uintptr_t to not require padding on
+     32- and 64-bit machines.  On 64-bit machines it helps to avoid
+     wrapping, too.  */
+  uintptr_t seq;
+
+  /* Destructor for the data.  */
+  void (*destr) (void *);
+};
+
+/* Check whether an entry is unused.  */
+#define KEY_UNUSED(p) (((p) & 1) == 0)
+/* Check whether a key is usable.  We cannot reuse an allocated key if
+   the sequence counter would overflow after the next destroy call.
+   This would mean that we potentially free memory for a key with the
+   same sequence.  This is *very* unlikely to happen, A program would
+   have to create and destroy a key 2^31 times (on 32-bit platforms,
+   on 64-bit platforms that would be 2^63).  If it should happen we
+   simply don't use this specific key anymore.  */
+#define KEY_USABLE(p) (((uintptr_t) (p)) < ((uintptr_t) ((p) + 2)))
+
+
+/* Handling of read-write lock data.  */
+// XXX For now there is only one flag.  Maybe more in future.
+#define RWLOCK_RECURSIVE(rwlock) ((rwlock)->__data.__flags != 0)
+
+
+/* Semaphore variable structure.  */
+struct sem
+{
+  unsigned int count;
+};
+
+
+/* Compatibility type for old conditional variable interfaces.  */
+typedef struct
+{
+  pthread_cond_t *cond;
+} pthread_cond_2_0_t;
+
+#endif	/* internaltypes.h */
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/libc-lowlevellock.c libc/nptl/sysdeps/l4/hurd/libc-lowlevellock.c
--- libc/nptl/sysdeps/l4/hurd/libc-lowlevellock.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/libc-lowlevellock.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,21 @@
+/* Copyright (C) 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Paul Mackerras <paulus@au.ibm.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	 See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+/* No difference to lowlevellock.c, except we lose a couple of functions.  */
+#include "lowlevellock.c"
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/lowlevellock.c libc/nptl/sysdeps/l4/hurd/lowlevellock.c
--- libc/nptl/sysdeps/l4/hurd/lowlevellock.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/lowlevellock.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,25 @@
+/* low level locking for pthread library.  Generic futex-using version.
+   Copyright (C) 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Paul Mackerras <paulus@au.ibm.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	 See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <sysdep.h>
+#include <lowlevellock.h>
+#include <sys/time.h>
+
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/lowlevellock.h libc/nptl/sysdeps/l4/hurd/lowlevellock.h
--- libc/nptl/sysdeps/l4/hurd/lowlevellock.h	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/lowlevellock.h	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,374 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#ifndef _LOWLEVELLOCK_H
+#define _LOWLEVELLOCK_H	1
+
+#include <time.h>
+#include <sys/param.h>
+#include <bits/pthreadtypes.h>
+
+/* Type for lock object.  */
+typedef int lll_lock_t;
+
+/* Initializers for lock.  */
+#define LLL_LOCK_INITIALIZER		(0)
+#define LLL_LOCK_INITIALIZER_LOCKED	(1)
+
+# define lll_trylock(futex) (0)
+# define lll_lock(futex) (0)
+# define lll_unlock(futex) (0)
+
+extern int lll_unlock_wake_cb (lll_lock_t *__lock) attribute_hidden;
+
+#define lll_mutex_lock(futex) (0) 
+#define lll_mutex_trylock(futex) (0) 
+#define lll_mutex_islocked(futex) (futex != 0)
+#define lll_futex_wake(futex, nr) (0)
+#define lll_futex_wait(futex, val) (0)
+#define lll_mutex_unlock(futex) (0) 
+#define lll_futex_timed_wait(ftx, val, timespec) (0)
+
+#define lll_mutex_cond_trylock(futex) (0)
+#define lll_mutex_cond_lock(futex) (0)
+
+
+#if 0
+
+/* Initializer for compatibility lock.  */
+#define LLL_MUTEX_LOCK_INITIALIZER		(0)
+#define LLL_MUTEX_LOCK_INITIALIZER_LOCKED	(1)
+#define LLL_MUTEX_LOCK_INITIALIZER_WAITERS	(2)
+
+
+#ifdef PIC
+# define LLL_EBX_LOAD	"xchgl %2, %%ebx\n"
+# define LLL_EBX_REG	"D"
+#else
+# define LLL_EBX_LOAD
+# define LLL_EBX_REG	"b"
+#endif
+
+
+/* Delay in spinlock loop.  */
+#define BUSY_WAIT_NOP          asm ("rep; nop")
+
+
+#define lll_futex_wait(futex, val) \
+  do {									      \
+    int __ignore;							      \
+    register __typeof (val) _val asm ("edx") = (val);			      \
+    __asm __volatile (LLL_EBX_LOAD					      \
+		      LLL_ENTER_KERNEL					      \
+		      LLL_EBX_LOAD					      \
+		      : "=a" (__ignore)					      \
+		      : "0" (SYS_futex), LLL_EBX_REG (futex), "S" (0),	      \
+			"c" (FUTEX_WAIT), "d" (_val),			      \
+			"i" (offsetof (tcbhead_t, sysinfo)));		      \
+  } while (0)
+
+
+#define lll_futex_wake(futex, nr) \
+  do {									      \
+    int __ignore;							      \
+    register __typeof (nr) _nr asm ("edx") = (nr);			      \
+    __asm __volatile (LLL_EBX_LOAD					      \
+		      LLL_ENTER_KERNEL					      \
+		      LLL_EBX_LOAD					      \
+		      : "=a" (__ignore)					      \
+		      : "0" (SYS_futex), LLL_EBX_REG (futex),		      \
+			"c" (FUTEX_WAKE), "d" (_nr),			      \
+			"i" (0) /* phony, to align next arg's number */,      \
+			"i" (offsetof (tcbhead_t, sysinfo)));		      \
+  } while (0)
+
+
+/* Does not preserve %eax and %ecx.  */
+extern int __lll_mutex_lock_wait (int val, int *__futex)
+     __attribute ((regparm (2))) attribute_hidden;
+/* Does not preserve %eax, %ecx, and %edx.  */
+extern int __lll_mutex_timedlock_wait (int val, int *__futex,
+				       const struct timespec *abstime)
+     __attribute ((regparm (3))) attribute_hidden;
+/* Preserves all registers but %eax.  */
+extern int __lll_mutex_unlock_wake (int *__futex)
+     __attribute ((regparm (1))) attribute_hidden;
+
+
+/* NB: in the lll_mutex_trylock macro we simply return the value in %eax
+   after the cmpxchg instruction.  In case the operation succeded this
+   value is zero.  In case the operation failed, the cmpxchg instruction
+   has loaded the current value of the memory work which is guaranteed
+   to be nonzero.  */
+#define lll_mutex_trylock(futex) \
+  ({ int ret;								      \
+     __asm __volatile (LOCK_INSTR "cmpxchgl %2, %1"			      \
+		       : "=a" (ret), "=m" (futex)			      \
+		       : "r" (LLL_MUTEX_LOCK_INITIALIZER_LOCKED), "m" (futex),\
+			 "0" (LLL_MUTEX_LOCK_INITIALIZER)		      \
+		       : "memory");					      \
+     ret; })
+
+
+#define lll_mutex_cond_trylock(futex) \
+  ({ int ret;								      \
+     __asm __volatile (LOCK_INSTR "cmpxchgl %2, %1"			      \
+		       : "=a" (ret), "=m" (futex)			      \
+		       : "r" (LLL_MUTEX_LOCK_INITIALIZER_WAITERS),	      \
+			  "m" (futex), "0" (LLL_MUTEX_LOCK_INITIALIZER)	      \
+		       : "memory");					      \
+     ret; })
+
+
+#define lll_mutex_lock(futex) \
+  (void) ({ int ignore1, ignore2;					      \
+	    __asm __volatile (LOCK_INSTR "cmpxchgl %1, %2\n\t"		      \
+			      "jnz _L_mutex_lock_%=\n\t"		      \
+			      ".subsection 1\n\t"			      \
+			      ".type _L_mutex_lock_%=,@function\n"	      \
+			      "_L_mutex_lock_%=:\n\t"			      \
+			      "leal %2, %%ecx\n\t"			      \
+			      "call __lll_mutex_lock_wait\n\t"		      \
+			      "jmp 1f\n\t"				      \
+			      ".size _L_mutex_lock_%=,.-_L_mutex_lock_%=\n"   \
+			      ".previous\n"				      \
+			      "1:"					      \
+			      : "=a" (ignore1), "=c" (ignore2), "=m" (futex)  \
+			      : "0" (0), "1" (1), "m" (futex)		      \
+			      : "memory"); })
+
+
+/* Special version of lll_mutex_lock which causes the unlock function to
+   always wakeup waiters.  */
+#define lll_mutex_cond_lock(futex) \
+  (void) ({ int ignore1, ignore2;					      \
+	    __asm __volatile (LOCK_INSTR "cmpxchgl %1, %2\n\t"		      \
+			      "jnz _L_mutex_cond_lock_%=\n\t"		      \
+			      ".subsection 1\n\t"			      \
+			      ".type _L_mutex_cond_lock_%=,@function\n"	      \
+			      "_L_mutex_cond_lock_%=:\n\t"		      \
+			      "leal %2, %%ecx\n\t"			      \
+			      "call __lll_mutex_lock_wait\n\t"		      \
+			      "jmp 1f\n\t"				      \
+			      ".size _L_mutex_cond_lock_%=,.-_L_mutex_cond_lock_%=\n"   \
+			      ".previous\n"				      \
+			      "1:"					      \
+			      : "=a" (ignore1), "=c" (ignore2), "=m" (futex)  \
+			      : "0" (0), "1" (2), "m" (futex)		      \
+			      : "memory"); })
+
+
+#define lll_mutex_timedlock(futex, timeout) \
+  ({ int result, ignore1, ignore2;					      \
+     __asm __volatile (LOCK_INSTR "cmpxchgl %1, %3\n\t"			      \
+		       "jnz _L_mutex_timedlock_%=\n\t"			      \
+		       ".subsection 1\n\t"				      \
+		       ".type _L_mutex_timedlock_%=,@function\n"	      \
+		       "_L_mutex_timedlock_%=:\n\t"			      \
+		       "leal %3, %%ecx\n\t"				      \
+		       "movl %7, %%edx\n\t"				      \
+		       "call __lll_mutex_timedlock_wait\n\t"		      \
+		       "jmp 1f\n\t"					      \
+		       ".size _L_mutex_timedlock_%=,.-_L_mutex_timedlock_%=\n"\
+		       ".previous\n"					      \
+		       "1:"						      \
+		       : "=a" (result), "=c" (ignore1), "=&d" (ignore2),      \
+			 "=m" (futex)					      \
+		       : "0" (0), "1" (1), "m" (futex), "m" (timeout)	      \
+		       : "memory");					      \
+     result; })
+
+
+#define lll_mutex_unlock(futex) \
+  (void) ({ int ignore;							      \
+            __asm __volatile (LOCK_INSTR "subl $1,%0\n\t"		      \
+			      "jne _L_mutex_unlock_%=\n\t"		      \
+			      ".subsection 1\n\t"			      \
+			      ".type _L_mutex_unlock_%=,@function\n"	      \
+			      "_L_mutex_unlock_%=:\n\t"			      \
+			      "leal %0, %%eax\n\t"			      \
+			      "call __lll_mutex_unlock_wake\n\t"	      \
+			      "jmp 1f\n\t"				      \
+			      ".size _L_mutex_unlock_%=,.-_L_mutex_unlock_%=\n" \
+			      ".previous\n"				      \
+			      "1:"					      \
+			      : "=m" (futex), "=&a" (ignore)		      \
+			      : "m" (futex)				      \
+			      : "memory"); })
+
+
+#define lll_mutex_islocked(futex) \
+  (futex != 0)
+
+
+/* We have a separate internal lock implementation which is not tied
+   to binary compatibility.  */
+
+
+
+extern int __lll_lock_wait (int val, int *__futex)
+     __attribute ((regparm (2))) attribute_hidden;
+extern int __lll_unlock_wake (int *__futex)
+     __attribute ((regparm (1))) attribute_hidden;
+extern int lll_unlock_wake_cb (int *__futex) attribute_hidden;
+
+
+/* The states of a lock are:
+    0  -  untaken
+    1  -  taken by one user
+    2  -  taken by more users */
+
+
+#if defined NOT_IN_libc || defined UP
+# define lll_trylock(futex) lll_mutex_trylock (futex)
+# define lll_lock(futex) lll_mutex_lock (futex)
+# define lll_unlock(futex) lll_mutex_unlock (futex)
+#else
+/* Special versions of the macros for use in libc itself.  They avoid
+   the lock prefix when the thread library is not used.
+
+   XXX In future we might even want to avoid it on UP machines.  */
+# include <tls.h>
+
+# define lll_trylock(futex) \
+  ({ unsigned char ret;							      \
+     __asm __volatile ("cmpl $0, %%gs:%P5\n\t"				      \
+		       "je,pt 0f\n\t"					      \
+		       "lock\n"						      \
+		       "0:\tcmpxchgl %2, %1; setne %0"			      \
+		       : "=a" (ret), "=m" (futex)			      \
+		       : "r" (LLL_MUTEX_LOCK_INITIALIZER_LOCKED), "m" (futex),\
+			 "0" (LLL_MUTEX_LOCK_INITIALIZER),		      \
+		         "i" (offsetof (tcbhead_t, multiple_threads))	      \
+		       : "memory");					      \
+     ret; })
+
+
+# define lll_lock(futex) \
+  (void) ({ int ignore1, ignore2;					      \
+	    __asm __volatile ("cmpl $0, %%gs:%P6\n\t"			      \
+			      "je,pt 0f\n\t"				      \
+			      "lock\n"					      \
+			      "0:\tcmpxchgl %1, %2\n\t"			      \
+			      "jnz _L_mutex_lock_%=\n\t"		      \
+			      ".subsection 1\n\t"			      \
+			      ".type _L_mutex_lock_%=,@function\n"	      \
+			      "_L_mutex_lock_%=:\n\t"			      \
+			      "leal %2, %%ecx\n\t"			      \
+			      "call __lll_mutex_lock_wait\n\t"		      \
+			      "jmp 1f\n\t"				      \
+			      ".size _L_mutex_lock_%=,.-_L_mutex_lock_%=\n"   \
+			      ".previous\n"				      \
+			      "1:"					      \
+			      : "=a" (ignore1), "=c" (ignore2), "=m" (futex)  \
+			      : "0" (0), "1" (1), "m" (futex),		      \
+		                "i" (offsetof (tcbhead_t, multiple_threads))  \
+			      : "memory"); })
+
+
+# define lll_unlock(futex) \
+  (void) ({ int ignore;							      \
+            __asm __volatile ("cmpl $0, %%gs:%P3\n\t"			      \
+			      "je,pt 0f\n\t"				      \
+			      "lock\n"					      \
+			      "0:\tsubl $1,%0\n\t"		      \
+			      "jne _L_mutex_unlock_%=\n\t"		      \
+			      ".subsection 1\n\t"			      \
+			      ".type _L_mutex_unlock_%=,@function\n"	      \
+			      "_L_mutex_unlock_%=:\n\t"			      \
+			      "leal %0, %%eax\n\t"			      \
+			      "call __lll_mutex_unlock_wake\n\t"	      \
+			      "jmp 1f\n\t"				      \
+			      ".size _L_mutex_unlock_%=,.-_L_mutex_unlock_%=\n" \
+			      ".previous\n"				      \
+			      "1:"					      \
+			      : "=m" (futex), "=&a" (ignore)		      \
+			      : "m" (futex),				      \
+				"i" (offsetof (tcbhead_t, multiple_threads))  \
+			      : "memory"); })
+#endif
+
+
+#define lll_islocked(futex) \
+  (futex != LLL_LOCK_INITIALIZER)
+
+
+/* The kernel notifies a process with uses CLONE_CLEARTID via futex
+   wakeup when the clone terminates.  The memory location contains the
+   thread ID while the clone is running and is reset to zero
+   afterwards.
+
+   The macro parameter must not have any side effect.  */
+#define lll_wait_tid(tid) \
+  do {									      \
+    int __ignore;							      \
+    register __typeof (tid) _tid asm ("edx") = (tid);			      \
+    if (_tid != 0)							      \
+      __asm __volatile (LLL_EBX_LOAD					      \
+			"1:\tmovl %1, %%eax\n\t"			      \
+			LLL_ENTER_KERNEL				      \
+			"cmpl $0, (%%ebx)\n\t"				      \
+			"jne,pn 1b\n\t"					      \
+			LLL_EBX_LOAD					      \
+			: "=&a" (__ignore)				      \
+			: "i" (SYS_futex), LLL_EBX_REG (&tid), "S" (0),	      \
+			  "c" (FUTEX_WAIT), "d" (_tid),			      \
+			  "i" (offsetof (tcbhead_t, sysinfo)));		      \
+  } while (0)
+
+extern int __lll_timedwait_tid (int *tid, const struct timespec *abstime)
+     __attribute__ ((regparm (2))) attribute_hidden;
+#define lll_timedwait_tid(tid, abstime) \
+  ({									      \
+    int __result = 0;							      \
+    if (tid != 0)							      \
+      {									      \
+	if (abstime->tv_nsec < 0 || abstime->tv_nsec >= 1000000000)	      \
+	  __result = EINVAL;						      \
+	else								      \
+	  __result = __lll_timedwait_tid (&tid, abstime);		      \
+      }									      \
+    __result; })
+
+
+/* Conditional variable handling.  */
+
+extern void __lll_cond_wait (pthread_cond_t *cond)
+     __attribute ((regparm (1))) attribute_hidden;
+extern int __lll_cond_timedwait (pthread_cond_t *cond,
+				 const struct timespec *abstime)
+     __attribute ((regparm (2))) attribute_hidden;
+extern void __lll_cond_wake (pthread_cond_t *cond)
+     __attribute ((regparm (1))) attribute_hidden;
+extern void __lll_cond_broadcast (pthread_cond_t *cond)
+     __attribute ((regparm (1))) attribute_hidden;
+
+
+#define lll_cond_wait(cond) \
+  __lll_cond_wait (cond)
+#define lll_cond_timedwait(cond, abstime) \
+  __lll_cond_timedwait (cond, abstime)
+#define lll_cond_wake(cond) \
+  __lll_cond_wake (cond)
+#define lll_cond_broadcast(cond) \
+  __lll_cond_broadcast (cond)
+
+#endif
+
+#endif	/* lowlevellock.h */
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/Makefile libc/nptl/sysdeps/l4/hurd/Makefile
--- libc/nptl/sysdeps/l4/hurd/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/Makefile	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,30 @@
+# Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+# This file is part of the GNU C Library.
+# Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+# The GNU C Library is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 2.1 of the License, or (at your option) any later version.
+
+# The GNU C Library is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+# Lesser General Public License for more details.
+
+# You should have received a copy of the GNU Lesser General Public
+# License along with the GNU C Library; if not, write to the Free
+# Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+# 02111-1307 USA.  */
+
+ifeq ($(subdir),nptl)
+sysdep_routines += register-atfork unregister-atfork
+
+libpthread-sysdep_routines += pt-fork pthread_mutex_cond_lock
+endif
+
+ifeq ($(subdir),posix)
+CFLAGS-fork.c = -D_IO_MTSAFE_IO
+CFLAGS-getpid.o = -fomit-frame-pointer
+CFLAGS-getpid.os = -fomit-frame-pointer
+endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pt-fork.c libc/nptl/sysdeps/l4/hurd/pt-fork.c
--- libc/nptl/sysdeps/l4/hurd/pt-fork.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pt-fork.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,28 @@
+/* Copyright (C) 2002 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <unistd.h>
+
+
+pid_t
+__fork (void)
+{
+  return __libc_fork ();
+}
+strong_alias (__fork, fork)
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_attr_getaffinity.c libc/nptl/sysdeps/l4/hurd/pthread_attr_getaffinity.c
--- libc/nptl/sysdeps/l4/hurd/pthread_attr_getaffinity.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_attr_getaffinity.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,71 @@
+/* Copyright (C) 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <assert.h>
+#include <errno.h>
+#include <pthreadP.h>
+#include <string.h>
+#include <sysdep.h>
+#include <sys/types.h>
+#include <shlib-compat.h>
+
+
+int
+__pthread_attr_getaffinity_new (const pthread_attr_t *attr, size_t cpusetsize,
+				cpu_set_t *cpuset)
+{
+  const struct pthread_attr *iattr;
+
+#if 0
+  assert (sizeof (*attr) >= sizeof (struct pthread_attr));
+  iattr = (const struct pthread_attr *) attr;
+
+  if (iattr->cpuset != NULL)
+    {
+      /* Check whether there are any bits set beyond the limits
+	 the user requested.  */
+      for (size_t cnt = cpusetsize; cnt < iattr->cpusetsize; ++cnt)
+	if (((char *) iattr->cpuset)[cnt] != 0)
+	  return EINVAL;
+
+      void *p = mempcpy (cpuset, iattr->cpuset, iattr->cpusetsize);
+      if (cpusetsize > iattr->cpusetsize)
+	memset (p, '\0', cpusetsize - iattr->cpusetsize);
+    }
+  else
+    /* We have no information.  */
+    memset (cpuset, -1, cpusetsize);
+#endif
+
+  return 0;
+}
+versioned_symbol (libpthread, __pthread_attr_getaffinity_new,
+		  pthread_attr_getaffinity_np, GLIBC_2_3_4);
+
+
+#if SHLIB_COMPAT (libpthread, GLIBC_2_3_3, GLIBC_2_3_4)
+int
+__pthread_attr_getaffinity_old (const pthread_attr_t *attr, cpu_set_t *cpuset)
+{
+  /* The old interface by default assumed a 1024 processor bitmap.  */
+  return __pthread_attr_getaffinity_new (attr, 128, cpuset);
+}
+compat_symbol (libpthread, __pthread_attr_getaffinity_old,
+	       pthread_attr_getaffinity_np, GLIBC_2_3_3);
+#endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_attr_setaffinity.c libc/nptl/sysdeps/l4/hurd/pthread_attr_setaffinity.c
--- libc/nptl/sysdeps/l4/hurd/pthread_attr_setaffinity.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_attr_setaffinity.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,97 @@
+/* Copyright (C) 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <assert.h>
+#include <errno.h>
+#include <limits.h>
+#include <stdlib.h>
+#include <string.h>
+#include <pthreadP.h>
+#include <shlib-compat.h>
+
+
+/* Defined in pthread_setaffinity.c.  */
+extern size_t __kernel_cpumask_size;
+extern int __determine_cpumask_size (pid_t tid);
+
+
+int
+__pthread_attr_setaffinity_new (pthread_attr_t *attr, size_t cpusetsize,
+				const cpu_set_t *cpuset)
+{
+  struct pthread_attr *iattr;
+
+#if 0
+  assert (sizeof (*attr) >= sizeof (struct pthread_attr));
+  iattr = (struct pthread_attr *) attr;
+
+  if (cpuset == NULL || cpusetsize == 0)
+    {
+      free (iattr->cpuset);
+      iattr->cpuset = NULL;
+      iattr->cpusetsize = 0;
+    }
+  else
+    {
+      if (__kernel_cpumask_size == 0)
+	{
+	  int res = __determine_cpumask_size (THREAD_SELF->tid);
+	  if (res != 0)
+	    /* Some serious problem.  */
+	    return res;
+	}
+
+      /* Check whether the new bitmask has any bit set beyond the
+	 last one the kernel accepts.  */
+      for (size_t cnt = __kernel_cpumask_size; cnt < cpusetsize; ++cnt)
+	if (((char *) cpuset)[cnt] != '\0')
+	  /* Found a nonzero byte.  This means the user request cannot be
+	     fulfilled.  */
+	  return EINVAL;
+
+      if (iattr->cpusetsize != cpusetsize)
+	{
+	  void *newp = (cpu_set_t *) realloc (iattr->cpuset, cpusetsize);
+	  if (newp == NULL)
+	    return ENOMEM;
+
+	  iattr->cpuset = newp;
+	  iattr->cpusetsize = cpusetsize;
+	}
+
+      memcpy (iattr->cpuset, cpuset, cpusetsize);
+    }
+
+#endif
+  return 0;
+}
+versioned_symbol (libpthread, __pthread_attr_setaffinity_new,
+		  pthread_attr_setaffinity_np, GLIBC_2_3_4);
+
+
+#if SHLIB_COMPAT (libpthread, GLIBC_2_3_3, GLIBC_2_3_4)
+int
+__pthread_attr_setaffinity_old (pthread_attr_t *attr, cpu_set_t *cpuset)
+{
+  /* The old interface by default assumed a 1024 processor bitmap.  */
+  return __pthread_attr_setaffinity_new (attr, 128, cpuset);
+}
+compat_symbol (libpthread, __pthread_attr_setaffinity_old,
+	       pthread_attr_setaffinity_np, GLIBC_2_3_3);
+#endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_cancel.c libc/nptl/sysdeps/l4/hurd/pthread_cancel.c
--- libc/nptl/sysdeps/l4/hurd/pthread_cancel.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_cancel.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,105 @@
+/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <signal.h>
+#include "pthreadP.h"
+#include "atomic.h"
+#include <sysdep.h>
+
+
+int
+pthread_cancel (th)
+     pthread_t th;
+{
+  volatile struct pthread *pd = (volatile struct pthread *) th;
+
+  /* Make sure the descriptor is valid.  */
+  if (INVALID_TD_P (pd))
+    /* Not a valid thread handle.  */
+    return ESRCH;
+
+#ifdef SHARED
+  pthread_cancel_init ();
+#endif
+  int result = 0;
+  int oldval;
+  int newval;
+  do
+    {
+      oldval = pd->cancelhandling;
+      newval = oldval | CANCELING_BITMASK | CANCELED_BITMASK;
+
+      /* Avoid doing unnecessary work.  The atomic operation can
+	 potentially be expensive if the bug has to be locked and
+	 remote cache lines have to be invalidated.  */
+      if (oldval == newval)
+	break;
+
+      /* If the cancellation is handled asynchronously just send a
+	 signal.  We avoid this if possible since it's more
+	 expensive.  */
+      if (CANCEL_ENABLED_AND_CANCELED_AND_ASYNCHRONOUS (newval))
+	{
+	  /* Mark the cancellation as "in progress".  */
+	  atomic_bit_set (&pd->cancelhandling, CANCELING_BIT);
+
+#if 0
+	  /* FIXME */
+
+	  /* The cancellation handler will take care of marking the
+	     thread as canceled.  */
+	  INTERNAL_SYSCALL_DECL (err);
+
+	  /* One comment: The PID field in the TCB can temporarily be
+	     changed (in fork).  But this must not affect this code
+	     here.  Since this function would have to be called while
+	     the thread is executing fork, it would have to happen in
+	     a signal handler.  But this is no allowed, pthread_cancel
+	     is not guaranteed to be async-safe.  */
+	  int val;
+#if __ASSUME_TGKILL
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid), pd->tid,
+				  SIGCANCEL);
+#else
+# ifdef __NR_tgkill
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid), pd->tid,
+				  SIGCANCEL);
+	  if (INTERNAL_SYSCALL_ERROR_P (val, err)
+	      && INTERNAL_SYSCALL_ERRNO (val, err) == ENOSYS)
+# endif
+	    val = INTERNAL_SYSCALL (tkill, err, 2, pd->tid, SIGCANCEL);
+#endif
+
+	  if (INTERNAL_SYSCALL_ERROR_P (val, err))
+	    result = INTERNAL_SYSCALL_ERRNO (val, err);
+
+#endif
+	  break;
+	}
+    }
+  /* Mark the thread as canceled.  This has to be done
+     atomically since other bits could be modified as well.  */
+  while (atomic_compare_and_exchange_bool_acq (&pd->cancelhandling, newval,
+					       oldval));
+
+  return result;
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_condattr_setclock.c libc/nptl/sysdeps/l4/hurd/pthread_condattr_setclock.c
--- libc/nptl/sysdeps/l4/hurd/pthread_condattr_setclock.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_condattr_setclock.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,73 @@
+/* Copyright (C) 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <assert.h>
+#include <errno.h>
+#include <stdbool.h>
+#include <time.h>
+#include <sysdep.h>
+#include "pthreadP.h"
+
+
+int
+pthread_condattr_setclock (attr, clock_id)
+     pthread_condattr_t *attr;
+     clockid_t clock_id;
+{
+#if 0
+  /* Only a few clocks are allowed.  CLOCK_REALTIME is always allowed.
+     CLOCK_MONOTONIC only if the kernel has the necessary support.  */
+  if (clock_id == CLOCK_MONOTONIC)
+    {
+#ifndef __ASSUME_POSIX_TIMERS
+# ifdef __NR_clock_getres
+      /* Check whether the clock is available.  */
+      static int avail;
+
+      if (avail == 0)
+	{
+	  struct timespec ts;
+
+	  INTERNAL_SYSCALL_DECL (err);
+	  int val;
+	  val = INTERNAL_SYSCALL (clock_getres, err, 2, CLOCK_MONOTONIC, &ts);
+	  avail = INTERNAL_SYSCALL_ERROR_P (val, err) ? -1 : 1;
+	}
+
+      if (avail < 0)
+# endif
+	/* Not available.  */
+	return EINVAL;
+#endif
+    }
+  else if (clock_id != CLOCK_REALTIME)
+    /* If more clocks are allowed some day the storing of the clock ID
+       in the pthread_cond_t structure needs to be adjusted.  */
+#endif
+    return EINVAL;
+#if 0
+  /* Make sure the value fits in the bits we reserved.  */
+  assert (clock_id < (1 << COND_CLOCK_BITS));
+
+  int *valuep = &((struct pthread_condattr *) attr)->value;
+
+  *valuep = (*valuep & ~(1 << (COND_CLOCK_BITS + 1)) & ~1) | (clock_id << 1);
+  return 0;
+#endif
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_create.c libc/nptl/sysdeps/l4/hurd/pthread_create.c
--- libc/nptl/sysdeps/l4/hurd/pthread_create.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_create.c	2005-01-23 20:07:51.000000000 +0100
@@ -0,0 +1,528 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <stdbool.h>
+#include <stdlib.h>
+#include <string.h>
+#include "pthreadP.h"
+#include <hp-timing.h>
+#include <ldsodefs.h>
+#include <atomic.h>
+#include <libc-internal.h>
+#include <resolv.h>
+
+#include <shlib-compat.h>
+
+
+/* Local function to start thread and handle cleanup.  */
+static int start_thread (void *arg);
+
+
+/* Nozero if debugging mode is enabled.  */
+int __pthread_debug;
+
+/* Globally enabled events.  */
+static td_thr_events_t __nptl_threads_events;
+
+/* Pointer to descriptor with the last event.  */
+static struct pthread *__nptl_last_event;
+
+/* Number of threads running.  */
+unsigned int __nptl_nthreads = 1;
+
+
+/* Code to allocate and deallocate a stack.  */
+#include "allocatestack.c"
+
+/* Code to create the thread.  */
+#include "createthread.c"
+
+
+struct pthread *
+internal_function
+__find_in_stack_list (pd)
+     struct pthread *pd;
+{
+  list_t *entry;
+  struct pthread *result = NULL;
+
+  lll_lock (stack_cache_lock);
+
+  list_for_each (entry, &stack_used)
+    {
+      struct pthread *curp;
+
+      curp = list_entry (entry, struct pthread, list);
+      if (curp == pd)
+	{
+	  result = curp;
+	  break;
+	}
+    }
+
+  if (result == NULL)
+    list_for_each (entry, &__stack_user)
+      {
+	struct pthread *curp;
+
+	curp = list_entry (entry, struct pthread, list);
+	if (curp == pd)
+	  {
+	    result = curp;
+	    break;
+	  }
+      }
+
+  lll_unlock (stack_cache_lock);
+
+  return result;
+}
+
+
+/* Deallocate POSIX thread-local-storage.  */
+void
+attribute_hidden
+__nptl_deallocate_tsd (void)
+{
+  struct pthread *self = THREAD_SELF;
+
+  /* Maybe no data was ever allocated.  This happens often so we have
+     a flag for this.  */
+  if (THREAD_GETMEM (self, specific_used))
+    {
+      size_t round;
+      size_t cnt;
+
+      round = 0;
+      do
+	{
+	  size_t idx;
+
+	  /* So far no new nonzero data entry.  */
+	  THREAD_SETMEM (self, specific_used, false);
+
+	  for (cnt = idx = 0; cnt < PTHREAD_KEY_1STLEVEL_SIZE; ++cnt)
+	    {
+	      struct pthread_key_data *level2;
+
+	      level2 = THREAD_GETMEM_NC (self, specific, cnt);
+
+	      if (level2 != NULL)
+		{
+		  size_t inner;
+
+		  for (inner = 0; inner < PTHREAD_KEY_2NDLEVEL_SIZE;
+		       ++inner, ++idx)
+		    {
+		      void *data = level2[inner].data;
+
+		      if (data != NULL)
+			{
+			  /* Always clear the data.  */
+			  level2[inner].data = NULL;
+
+			  /* Make sure the data corresponds to a valid
+			     key.  This test fails if the key was
+			     deallocated and also if it was
+			     re-allocated.  It is the user's
+			     responsibility to free the memory in this
+			     case.  */
+			  if (level2[inner].seq
+			      == __pthread_keys[idx].seq
+			      /* It is not necessary to register a destructor
+				 function.  */
+			      && __pthread_keys[idx].destr != NULL)
+			    /* Call the user-provided destructor.  */
+			    __pthread_keys[idx].destr (data);
+			}
+		    }
+		}
+	      else
+		idx += PTHREAD_KEY_1STLEVEL_SIZE;
+	    }
+
+	  if (THREAD_GETMEM (self, specific_used) == 0)
+	    /* No data has been modified.  */
+	    goto just_free;
+	}
+      /* We only repeat the process a fixed number of times.  */
+      while (__builtin_expect (++round < PTHREAD_DESTRUCTOR_ITERATIONS, 0));
+
+      /* Just clear the memory of the first block for reuse.  */
+      memset (&THREAD_SELF->specific_1stblock, '\0',
+	      sizeof (self->specific_1stblock));
+
+    just_free:
+      /* Free the memory for the other blocks.  */
+      for (cnt = 1; cnt < PTHREAD_KEY_1STLEVEL_SIZE; ++cnt)
+	{
+	  struct pthread_key_data *level2;
+
+	  level2 = THREAD_GETMEM_NC (self, specific, cnt);
+	  if (level2 != NULL)
+	    {
+	      /* The first block is allocated as part of the thread
+		 descriptor.  */
+	      free (level2);
+	      THREAD_SETMEM_NC (self, specific, cnt, NULL);
+	    }
+	}
+
+      THREAD_SETMEM (self, specific_used, false);
+    }
+}
+
+
+/* Deallocate a thread's stack after optionally making sure the thread
+   descriptor is still valid.  */
+void
+internal_function
+__free_tcb (struct pthread *pd)
+{
+  /* The thread is exiting now.  */
+  if (__builtin_expect (atomic_bit_test_set (&pd->cancelhandling,
+					     TERMINATED_BIT) == 0, 1))
+    {
+      /* Remove the descriptor from the list.  */
+      if (DEBUGGING_P && __find_in_stack_list (pd) == NULL)
+	/* Something is really wrong.  The descriptor for a still
+	   running thread is gone.  */
+	abort ();
+
+      /* Queue the stack memory block for reuse and exit the process.  The
+	 kernel will signal via writing to the address returned by
+	 QUEUE-STACK when the stack is available.  */
+      __deallocate_stack (pd);
+    }
+}
+
+
+static int
+start_thread (void *arg)
+{
+  struct pthread *pd = (struct pthread *) arg;
+
+#if HP_TIMING_AVAIL
+  /* Remember the time when the thread was started.  */
+  hp_timing_t now;
+  HP_TIMING_NOW (now);
+  THREAD_SETMEM (pd, cpuclock_offset, now);
+#endif
+
+  /* Initialize resolver state pointer.  */
+  __resp = &pd->res;
+
+  /* This is where the try/finally block should be created.  For
+     compilers without that support we do use setjmp.  */
+  struct pthread_unwind_buf unwind_buf;
+
+  /* No previous handlers.  */
+  unwind_buf.priv.data.prev = NULL;
+  unwind_buf.priv.data.cleanup = NULL;
+
+  int not_first_call;
+  not_first_call = setjmp ((struct __jmp_buf_tag *) unwind_buf.cancel_jmp_buf);
+  if (__builtin_expect (! not_first_call, 1))
+    {
+      /* Store the new cleanup handler info.  */
+      THREAD_SETMEM (pd, cleanup_jmp_buf, &unwind_buf);
+
+      if (__builtin_expect (pd->stopped_start, 0))
+	{
+	  int oldtype = CANCEL_ASYNC ();
+
+	  /* Get the lock the parent locked to force synchronization.  */
+	  lll_lock (pd->lock);
+	  /* And give it up right away.  */
+	  lll_unlock (pd->lock);
+
+	  CANCEL_RESET (oldtype);
+	}
+
+      /* Run the code the user provided.  */
+#ifdef CALL_THREAD_FCT
+      THREAD_SETMEM (pd, result, CALL_THREAD_FCT (pd));
+#else
+      THREAD_SETMEM (pd, result, pd->start_routine (pd->arg));
+#endif
+    }
+
+  /* Run the destructor for the thread-local data.  */
+  __nptl_deallocate_tsd ();
+
+  /* Clean up any state libc stored in thread-local variables.  */
+  __libc_thread_freeres ();
+
+  /* If this is the last thread we terminate the process now.  We
+     do not notify the debugger, it might just irritate it if there
+     is no thread left.  */
+  if (__builtin_expect (atomic_decrement_and_test (&__nptl_nthreads), 0))
+    /* This was the last thread.  */
+    exit (0);
+
+  /* Report the death of the thread if this is wanted.  */
+  if (__builtin_expect (pd->report_events, 0))
+    {
+      /* See whether TD_DEATH is in any of the mask.  */
+      const int idx = __td_eventword (TD_DEATH);
+      const uint32_t mask = __td_eventmask (TD_DEATH);
+
+      if ((mask & (__nptl_threads_events.event_bits[idx]
+		   | pd->eventbuf.eventmask.event_bits[idx])) != 0)
+	{
+	  /* Yep, we have to signal the death.  Add the descriptor to
+	     the list but only if it is not already on it.  */
+	  if (pd->nextevent == NULL)
+	    {
+	      pd->eventbuf.eventnum = TD_DEATH;
+	      pd->eventbuf.eventdata = pd;
+
+	      do
+		pd->nextevent = __nptl_last_event;
+	      while (atomic_compare_and_exchange_bool_acq (&__nptl_last_event,
+							   pd, pd->nextevent));
+	    }
+
+	  /* Now call the function to signal the event.  */
+	  __nptl_death_event ();
+	}
+    }
+
+  /* The thread is exiting now.  Don't set this bit until after we've hit
+     the event-reporting breakpoint, so that td_thr_get_info on us while at
+     the breakpoint reports TD_THR_RUN state rather than TD_THR_ZOMBIE.  */
+  atomic_bit_set (&pd->cancelhandling, EXITING_BIT);
+
+  /* If the thread is detached free the TCB.  */
+  if (IS_DETACHED (pd))
+    /* Free the TCB.  */
+    __free_tcb (pd);
+
+  /* We cannot call '_exit' here.  '_exit' will terminate the process.
+
+     The 'exit' implementation in the kernel will signal when the
+     process is really dead since 'clone' got passed the CLONE_CLEARTID
+     flag.  The 'tid' field in the TCB will be set to zero.
+
+     The exit code is zero since in case all threads exit by calling
+     'pthread_exit' the exit status must be 0 (zero).  */
+#if 0
+  __exit_thread_inline (0);
+#else
+  __exit_thread (0);
+#endif
+
+  /* NOTREACHED */
+  return 0;
+}
+
+
+/* Default thread attributes for the case when the user does not
+   provide any.  */
+static const struct pthread_attr default_attr =
+  {
+    /* Just some value > 0 which gets rounded to the nearest page size.  */
+    .guardsize = 1,
+  };
+
+
+int
+__pthread_create_2_1 (newthread, attr, start_routine, arg)
+     pthread_t *newthread;
+     const pthread_attr_t *attr;
+     void *(*start_routine) (void *);
+     void *arg;
+{
+  STACK_VARIABLES;
+  const struct pthread_attr *iattr;
+  struct pthread *pd;
+  int err;
+
+  iattr = (struct pthread_attr *) attr;
+  if (iattr == NULL)
+    /* Is this the best idea?  On NUMA machines this could mean
+       accessing far-away memory.  */
+    iattr = &default_attr;
+
+  err = ALLOCATE_STACK (iattr, &pd);
+  if (__builtin_expect (err != 0, 0))
+    /* Something went wrong.  Maybe a parameter of the attributes is
+       invalid or we could not allocate memory.  */
+    return err;
+
+
+  /* Initialize the TCB.  All initializations with zero should be
+     performed in 'get_cached_stack'.  This way we avoid doing this if
+     the stack freshly allocated with 'mmap'.  */
+
+#ifdef TLS_TCB_AT_TP
+  /* Reference to the TCB itself.  */
+  pd->header.self = pd;
+
+  /* Self-reference for TLS.  */
+  pd->header.tcb = pd;
+#endif
+
+  /* Store the address of the start routine and the parameter.  Since
+     we do not start the function directly the stillborn thread will
+     get the information from its thread descriptor.  */
+  pd->start_routine = start_routine;
+  pd->arg = arg;
+
+  /* Copy the thread attribute flags.  */
+  struct pthread *self = THREAD_SELF;
+  pd->flags = ((iattr->flags & ~(ATTR_FLAG_SCHED_SET | ATTR_FLAG_POLICY_SET))
+	       | (self->flags & (ATTR_FLAG_SCHED_SET | ATTR_FLAG_POLICY_SET)));
+
+  /* Initialize the field for the ID of the thread which is waiting
+     for us.  This is a self-reference in case the thread is created
+     detached.  */
+  pd->joinid = iattr->flags & ATTR_FLAG_DETACHSTATE ? pd : NULL;
+
+  /* The debug events are inherited from the parent.  */
+  pd->eventbuf = self->eventbuf;
+
+
+  /* Copy the parent's scheduling parameters.  The flags will say what
+     is valid and what is not.  */
+  pd->schedpolicy = self->schedpolicy;
+  pd->schedparam = self->schedparam;
+
+  /* Determine scheduling parameters for the thread.  */
+  if (attr != NULL
+      && __builtin_expect ((iattr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0, 0)
+      && (iattr->flags & (ATTR_FLAG_SCHED_SET | ATTR_FLAG_POLICY_SET)) != 0)
+    {
+#if 0
+      INTERNAL_SYSCALL_DECL (err);
+
+      /* Use the scheduling parameters the user provided.  */
+      if (iattr->flags & ATTR_FLAG_POLICY_SET)
+	pd->schedpolicy = iattr->schedpolicy;
+      else if ((pd->flags & ATTR_FLAG_POLICY_SET) == 0)
+	{
+	  pd->schedpolicy = INTERNAL_SYSCALL (sched_getscheduler, err, 1, 0);
+	  pd->flags |= ATTR_FLAG_POLICY_SET;
+	}
+
+      if (iattr->flags & ATTR_FLAG_SCHED_SET)
+	memcpy (&pd->schedparam, &iattr->schedparam,
+		sizeof (struct sched_param));
+      else if ((pd->flags & ATTR_FLAG_SCHED_SET) == 0)
+	{
+	  INTERNAL_SYSCALL (sched_getparam, err, 2, 0, &pd->schedparam);
+	  pd->flags |= ATTR_FLAG_SCHED_SET;
+	}
+
+      /* Check for valid priorities.  */
+      int minprio = INTERNAL_SYSCALL (sched_get_priority_min, err, 1,
+				      iattr->schedpolicy);
+      int maxprio = INTERNAL_SYSCALL (sched_get_priority_max, err, 1,
+				      iattr->schedpolicy);
+      if (pd->schedparam.sched_priority < minprio
+	  || pd->schedparam.sched_priority > maxprio)
+	{
+	  err = EINVAL;
+	  goto errout;
+	}
+#endif
+    }
+
+  /* Pass the descriptor to the caller.  */
+  *newthread = (pthread_t) pd;
+
+  /* Remember whether the thread is detached or not.  In case of an
+     error we have to free the stacks of non-detached stillborn
+     threads.  */
+  bool is_detached = IS_DETACHED (pd);
+
+  /* Start the thread.  */
+  err = create_thread (pd, iattr, STACK_VARIABLES_ARGS);
+  if (err != 0)
+    {
+      /* Something went wrong.  Free the resources.  */
+      if (!is_detached)
+	{
+	errout:
+	  __deallocate_stack (pd);
+	}
+      return err;
+    }
+
+  return 0;
+}
+versioned_symbol (libpthread, __pthread_create_2_1, pthread_create, GLIBC_2_1);
+
+
+#if SHLIB_COMPAT(libpthread, GLIBC_2_0, GLIBC_2_1)
+int
+__pthread_create_2_0 (newthread, attr, start_routine, arg)
+     pthread_t *newthread;
+     const pthread_attr_t *attr;
+     void *(*start_routine) (void *);
+     void *arg;
+{
+  /* The ATTR attribute is not really of type `pthread_attr_t *'.  It has
+     the old size and access to the new members might crash the program.
+     We convert the struct now.  */
+  struct pthread_attr new_attr;
+
+  if (attr != NULL)
+    {
+      struct pthread_attr *iattr = (struct pthread_attr *) attr;
+      size_t ps = __getpagesize ();
+
+      /* Copy values from the user-provided attributes.  */
+      new_attr.schedparam = iattr->schedparam;
+      new_attr.schedpolicy = iattr->schedpolicy;
+      new_attr.flags = iattr->flags;
+
+      /* Fill in default values for the fields not present in the old
+	 implementation.  */
+      new_attr.guardsize = ps;
+      new_attr.stackaddr = NULL;
+      new_attr.stacksize = 0;
+      new_attr.cpuset = NULL;
+
+      /* We will pass this value on to the real implementation.  */
+      attr = (pthread_attr_t *) &new_attr;
+    }
+
+  return __pthread_create_2_1 (newthread, attr, start_routine, arg);
+}
+compat_symbol (libpthread, __pthread_create_2_0, pthread_create,
+	       GLIBC_2_0);
+#endif
+
+/* Information for libthread_db.  */
+
+#include "../nptl_db/db_info.c"
+
+/* If pthread_create is present, libgcc_eh.a and libsupc++.a expects some other POSIX thread
+   functions to be present as well.  */
+PTHREAD_STATIC_FN_REQUIRE (pthread_mutex_lock)
+PTHREAD_STATIC_FN_REQUIRE (pthread_mutex_unlock)
+
+PTHREAD_STATIC_FN_REQUIRE (pthread_once)
+PTHREAD_STATIC_FN_REQUIRE (pthread_cancel)
+
+PTHREAD_STATIC_FN_REQUIRE (pthread_key_create)
+PTHREAD_STATIC_FN_REQUIRE (pthread_setspecific)
+PTHREAD_STATIC_FN_REQUIRE (pthread_getspecific)
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_getaffinity.c libc/nptl/sysdeps/l4/hurd/pthread_getaffinity.c
--- libc/nptl/sysdeps/l4/hurd/pthread_getaffinity.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_getaffinity.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,62 @@
+/* Copyright (C) 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <limits.h>
+#include <pthreadP.h>
+#include <string.h>
+#include <sysdep.h>
+#include <sys/param.h>
+#include <sys/types.h>
+#include <shlib-compat.h>
+
+
+int
+__pthread_getaffinity_new (pthread_t th, size_t cpusetsize, cpu_set_t *cpuset)
+{
+  const struct pthread *pd = (const struct pthread *) th;
+
+#if 0
+  INTERNAL_SYSCALL_DECL (err);
+  int res = INTERNAL_SYSCALL (sched_getaffinity, err, 3, pd->tid,
+			      MIN (INT_MAX, cpusetsize), cpuset);
+  if (INTERNAL_SYSCALL_ERROR_P (res, err))
+    return INTERNAL_SYSCALL_ERRNO (res, err);
+
+  /* Clean the rest of the memory the kernel didn't do.  */
+  memset ((char *) cpuset + res, '\0', cpusetsize - res);
+
+#endif
+  return 0;
+}
+strong_alias (__pthread_getaffinity_new, __pthread_getaffinity_np)
+versioned_symbol (libpthread, __pthread_getaffinity_new,
+		  pthread_getaffinity_np, GLIBC_2_3_4);
+
+
+#if SHLIB_COMPAT (libpthread, GLIBC_2_3_3, GLIBC_2_3_4)
+int
+__pthread_getaffinity_old (pthread_t th, cpu_set_t *cpuset)
+{
+  /* The old interface by default assumed a 1024 processor bitmap.  */
+  return __pthread_getaffinity_new (th, 128, cpuset);
+}
+compat_symbol (libpthread, __pthread_getaffinity_old, pthread_getaffinity_np,
+	       GLIBC_2_3_3);
+#endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_kill.c libc/nptl/sysdeps/l4/hurd/pthread_kill.c
--- libc/nptl/sysdeps/l4/hurd/pthread_kill.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_kill.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,42 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <signal.h>
+#include <pthreadP.h>
+#include <tls.h>
+#include <sysdep.h>
+
+
+int
+__pthread_kill (threadid, signo)
+     pthread_t threadid;
+     int signo;
+{
+  struct pthread *pd = (struct pthread *) threadid;
+
+  /* Make sure the descriptor is valid.  */
+  if (INVALID_TD_P (pd))
+    /* Not a valid thread handle.  */
+    return ESRCH;
+
+  /*FIXME*/
+  return ENOSYS;
+}
+strong_alias (__pthread_kill, pthread_kill)
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_mutex_cond_lock.c libc/nptl/sysdeps/l4/hurd/pthread_mutex_cond_lock.c
--- libc/nptl/sysdeps/l4/hurd/pthread_mutex_cond_lock.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_mutex_cond_lock.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,8 @@
+#include <pthreadP.h>
+
+#define LLL_MUTEX_LOCK(mutex) lll_mutex_cond_lock(mutex)
+#define LLL_MUTEX_TRYLOCK(mutex) lll_mutex_cond_trylock(mutex)
+#define __pthread_mutex_lock __pthread_mutex_cond_lock
+#define NO_INCR
+
+#include <nptl/pthread_mutex_lock.c>
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_setaffinity.c libc/nptl/sysdeps/l4/hurd/pthread_setaffinity.c
--- libc/nptl/sysdeps/l4/hurd/pthread_setaffinity.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_setaffinity.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,102 @@
+/* Copyright (C) 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <alloca.h>
+#include <errno.h>
+#include <pthreadP.h>
+#include <sysdep.h>
+#include <sys/types.h>
+#include <shlib-compat.h>
+
+
+/* Determine the current affinity.  As a side affect we learn
+   about the size of the cpumask_t in the kernel.  */
+int
+__determine_cpumask_size (pid_t tid)
+{
+#if 0
+  INTERNAL_SYSCALL_DECL (err);
+#endif
+  int res;
+
+#if 0
+  size_t psize = 128;
+  void *p = alloca (psize);
+
+  while (res = INTERNAL_SYSCALL (sched_getaffinity, err, 3, tid, psize, p),
+	 INTERNAL_SYSCALL_ERROR_P (res, err)
+	 && INTERNAL_SYSCALL_ERRNO (res, err) == EINVAL)
+    p = extend_alloca (p, psize, 2 * psize);
+
+  if (res == 0 || INTERNAL_SYSCALL_ERROR_P (res, err))
+    return INTERNAL_SYSCALL_ERRNO (res, err);
+
+  __kernel_cpumask_size = res;
+#endif
+
+  return 0;
+}
+
+
+int
+__pthread_setaffinity_new (pthread_t th, size_t cpusetsize,
+			   const cpu_set_t *cpuset)
+{
+  const struct pthread *pd = (const struct pthread *) th;
+
+#if 0
+  INTERNAL_SYSCALL_DECL (err);
+  int res;
+
+  if (__builtin_expect (__kernel_cpumask_size == 0, 0))
+    {
+      res = __determine_cpumask_size (pd->tid);
+      if (res != 0)
+	return res;
+    }
+
+  /* We now know the size of the kernel cpumask_t.  Make sure the user
+     does not request to set a bit beyond that.  */
+  for (size_t cnt = __kernel_cpumask_size; cnt < cpusetsize; ++cnt)
+    if (((char *) cpuset)[cnt] != '\0')
+      /* Found a nonzero byte.  This means the user request cannot be
+	 fulfilled.  */
+      return EINVAL;
+
+  res = INTERNAL_SYSCALL (sched_setaffinity, err, 3, pd->tid, cpusetsize,
+			  cpuset);
+  return (INTERNAL_SYSCALL_ERROR_P (res, err)
+	  ? INTERNAL_SYSCALL_ERRNO (res, err)
+	  : 0);
+#endif
+}
+versioned_symbol (libpthread, __pthread_setaffinity_new,
+		  pthread_setaffinity_np, GLIBC_2_3_4);
+
+
+#if SHLIB_COMPAT (libpthread, GLIBC_2_3_3, GLIBC_2_3_4)
+int
+__pthread_setaffinity_old (pthread_t th, cpu_set_t *cpuset)
+{
+  /* The old interface by default assumed a 1024 processor bitmap.  */
+  return __pthread_setaffinity_new (th, 128, cpuset);
+}
+compat_symbol (libpthread, __pthread_setaffinity_old, pthread_setaffinity_np,
+	       GLIBC_2_3_3);
+#endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_sigmask.c libc/nptl/sysdeps/l4/hurd/pthread_sigmask.c
--- libc/nptl/sysdeps/l4/hurd/pthread_sigmask.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_sigmask.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,33 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <signal.h>
+#include <pthreadP.h>
+#include <sysdep.h>
+
+
+int
+pthread_sigmask (how, newmask, oldmask)
+     int how;
+     const sigset_t *newmask;
+     sigset_t *oldmask;
+{
+  return sigprocmask (how, newmask, oldmask) == -1 ? errno : 0;
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/pthread_yield.c libc/nptl/sysdeps/l4/hurd/pthread_yield.c
--- libc/nptl/sysdeps/l4/hurd/pthread_yield.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/pthread_yield.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,30 @@
+/* Copyright (C) 2002 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <pthread.h>
+#include <sched.h>
+
+
+/* With the 1-on-1 model we implement this function is equivalent to
+   the 'sched_yield' function.  */
+int
+pthread_yield (void)
+{
+  return sched_yield ();
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/register-atfork.c libc/nptl/sysdeps/l4/hurd/register-atfork.c
--- libc/nptl/sysdeps/l4/hurd/register-atfork.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/register-atfork.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,135 @@
+/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <stdlib.h>
+#include <string.h>
+#include "fork.h"
+
+
+/* Lock to protect allocation and deallocation of fork handlers.  */
+lll_lock_t __fork_lock = LLL_LOCK_INITIALIZER;
+
+
+/* Number of pre-allocated handler entries.  */
+#define NHANDLER 48
+
+/* Memory pool for fork handler structures.  */
+static struct fork_handler_pool
+{
+  struct fork_handler_pool *next;
+  struct fork_handler mem[NHANDLER];
+} fork_handler_pool;
+
+
+static struct fork_handler *
+fork_handler_alloc (void)
+{
+  struct fork_handler_pool *runp = &fork_handler_pool;
+  struct fork_handler *result = NULL;
+  unsigned int i;
+
+  do
+    {
+      /* Search for an empty entry.  */
+      for (i = 0; i < NHANDLER; ++i)
+	if (runp->mem[i].refcntr == 0)
+	  goto found;
+    }
+  while ((runp = runp->next) != NULL);
+
+  /* We have to allocate a new entry.  */
+  runp = (struct fork_handler_pool *) calloc (1, sizeof (*runp));
+  if (runp != NULL)
+    {
+      /* Enqueue the new memory pool into the list.  */
+      runp->next = fork_handler_pool.next;
+      fork_handler_pool.next = runp;
+
+      /* We use the last entry on the page.  This means when we start
+	 searching from the front the next time we will find the first
+	 entry unused.  */
+      i = NHANDLER - 1;
+
+    found:
+      result = &runp->mem[i];
+      result->refcntr = 1;
+      result->need_signal = 0;
+    }
+
+  return result;
+}
+
+
+int
+__register_atfork (prepare, parent, child, dso_handle)
+     void (*prepare) (void);
+     void (*parent) (void);
+     void (*child) (void);
+     void *dso_handle;
+{
+  /* Get the lock to not conflict with other allocations.  */
+  lll_lock (__fork_lock);
+
+  struct fork_handler *newp = fork_handler_alloc ();
+
+  if (newp != NULL)
+    {
+      /* Initialize the new record.  */
+      newp->prepare_handler = prepare;
+      newp->parent_handler = parent;
+      newp->child_handler = child;
+      newp->dso_handle = dso_handle;
+
+      newp->next = __fork_handlers;
+      __fork_handlers = newp;
+    }
+
+  /* Release the lock.  */
+  lll_unlock (__fork_lock);
+
+  return newp == NULL ? ENOMEM : 0;
+}
+libc_hidden_def (__register_atfork)
+
+
+libc_freeres_fn (free_mem)
+{
+  /* Get the lock to not conflict with running forks.  */
+  lll_lock (__fork_lock);
+
+  /* No more fork handlers.  */
+  __fork_handlers = NULL;
+
+  /* Free eventually alloated memory blocks for the object pool.  */
+  struct fork_handler_pool *runp = fork_handler_pool.next;
+
+  memset (&fork_handler_pool, '\0', sizeof (fork_handler_pool));
+
+  /* Release the lock.  */
+  lll_unlock (__fork_lock);
+
+  /* We can free the memory after releasing the lock.  */
+  while (runp != NULL)
+    {
+      struct fork_handler_pool *oldp = runp;
+      runp = runp->next;
+      free (oldp);
+    }
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/sem_open.c libc/nptl/sysdeps/l4/hurd/sem_open.c
--- libc/nptl/sysdeps/l4/hurd/sem_open.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/sem_open.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,414 @@
+/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <fcntl.h>
+#include <mntent.h>
+#include <paths.h>
+#include <pthread.h>
+#include <search.h>
+#include <semaphore.h>
+#include <stdarg.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <sys/statfs.h>
+#include "semaphoreP.h"
+
+
+
+/* Information about the mount point.  */
+struct mountpoint_info mountpoint attribute_hidden;
+
+/* This is the default mount point.  */
+static const char defaultmount[] = "/dev/shm";
+/* This is the default directory.  */
+static const char defaultdir[] = "/dev/shm/sem.";
+
+/* Protect the `mountpoint' variable above.  */
+pthread_once_t __namedsem_once attribute_hidden = PTHREAD_ONCE_INIT;
+
+
+/* Determine where the shmfs is mounted (if at all).  */
+void
+attribute_hidden
+__where_is_shmfs (void)
+{
+  char buf[512];
+  struct statfs f;
+  struct mntent resmem;
+  struct mntent *mp;
+  FILE *fp;
+
+#if 0
+  /* The canonical place is /dev/shm.  This is at least what the
+     documentation tells everybody to do.  */
+  if (__statfs (defaultmount, &f) == 0 && f.f_type == SHMFS_SUPER_MAGIC)
+    {
+      /* It is in the normal place.  */
+      mountpoint.dir = (char *) defaultdir;
+      mountpoint.dirlen = sizeof (defaultdir) - 1;
+
+      return;
+    }
+#endif
+
+  /* OK, do it the hard way.  Look through the /proc/mounts file and if
+     this does not exist through /etc/fstab to find the mount point.  */
+  fp = __setmntent ("/proc/mounts", "r");
+  if (__builtin_expect (fp == NULL, 0))
+    {
+      fp = __setmntent (_PATH_MNTTAB, "r");
+      if (__builtin_expect (fp == NULL, 0))
+	/* There is nothing we can do.  Blind guesses are not helpful.  */
+	return;
+    }
+
+  /* Now read the entries.  */
+  while ((mp = __getmntent_r (fp, &resmem, buf, sizeof buf)) != NULL)
+    /* The original name is "shm" but this got changed in early Linux
+       2.4.x to "tmpfs".  */
+    if (strcmp (mp->mnt_type, "tmpfs") == 0
+	|| strcmp (mp->mnt_type, "shm") == 0)
+      {
+	/* Found it.  There might be more than one place where the
+           filesystem is mounted but one is enough for us.  */
+	size_t namelen;
+
+	/* First make sure this really is the correct entry.  At least
+	   some versions of the kernel give wrong information because
+	   of the implicit mount of the shmfs for SysV IPC.  */
+#if 0
+	if (__statfs (mp->mnt_dir, &f) != 0 || f.f_type != SHMFS_SUPER_MAGIC)
+	  continue;
+#endif
+
+	namelen = strlen (mp->mnt_dir);
+
+	if (namelen == 0)
+	  /* Hum, maybe some crippled entry.  Keep on searching.  */
+	  continue;
+
+	mountpoint.dir = (char *) malloc (namelen + 4 + 2);
+	if (mountpoint.dir != NULL)
+	  {
+	    char *cp = __mempcpy (mountpoint.dir, mp->mnt_dir, namelen);
+	    if (cp[-1] != '/')
+	      *cp++ = '/';
+	    cp = stpcpy (cp, "sem.");
+	    mountpoint.dirlen = cp - mountpoint.dir;
+	  }
+
+	break;
+      }
+
+  /* Close the stream.  */
+  __endmntent (fp);
+}
+
+
+/* Comparison function for search of existing mapping.  */
+int
+attribute_hidden
+__sem_search (const void *a, const void *b)
+{
+  const struct inuse_sem *as = (const struct inuse_sem *) a;
+  const struct inuse_sem *bs = (const struct inuse_sem *) b;
+
+  if (as->ino != bs->ino)
+    /* Cannot return the difference the type is larger than int.  */
+    return as->ino < bs->ino ? -1 : (as->ino == bs->ino ? 0 : 1);
+
+  if (as->dev != bs->dev)
+    /* Cannot return the difference the type is larger than int.  */
+    return as->dev < bs->dev ? -1 : (as->dev == bs->dev ? 0 : 1);
+
+  return strcmp (as->name, bs->name);
+}
+
+
+/* The search tree for existing mappings.  */
+void *__sem_mappings attribute_hidden;
+
+/* Lock to protect the search tree.  */
+lll_lock_t __sem_mappings_lock = LLL_LOCK_INITIALIZER;
+
+
+/* Search for existing mapping and if possible add the one provided.  */
+static sem_t *
+check_add_mapping (const char *name, size_t namelen, int fd, sem_t *existing)
+{
+  sem_t *result = SEM_FAILED;
+
+  /* Get the information about the file.  */
+  struct stat64 st;
+  if (__fxstat64 (_STAT_VER, fd, &st) == 0)
+    {
+      /* Get the lock.  */
+      lll_lock (__sem_mappings_lock);
+
+      /* Search for an existing mapping given the information we have.  */
+      struct inuse_sem *fake;
+      fake = (struct inuse_sem *) alloca (sizeof (*fake) + namelen);
+      memcpy (fake->name, name, namelen);
+      fake->dev = st.st_dev;
+      fake->ino = st.st_ino;
+
+      struct inuse_sem **foundp = tfind (fake, &__sem_mappings, __sem_search);
+      if (foundp != NULL)
+	{
+	  /* There is already a mapping.  Use it.  */
+	  result = (*foundp)->sem;
+	  ++(*foundp)->refcnt;
+	}
+      else
+	{
+	  /* We haven't found a mapping.  Install ione.  */
+	  struct inuse_sem *newp;
+
+	  newp = (struct inuse_sem *) malloc (sizeof (*newp) + namelen);
+	  if (newp != NULL)
+	    {
+	      /* If the caller hasn't provided any map it now.  */
+	      if (existing == SEM_FAILED)
+		existing = (sem_t *) mmap (NULL, sizeof (sem_t),
+					   PROT_READ | PROT_WRITE, MAP_SHARED,
+					   fd, 0);
+
+	      newp->dev = st.st_dev;
+	      newp->ino = st.st_ino;
+	      newp->refcnt = 1;
+	      newp->sem = existing;
+	      memcpy (newp->name, name, namelen);
+
+	      /* Insert the new value.  */
+	      if (existing != MAP_FAILED
+		  && tsearch (newp, &__sem_mappings, __sem_search) != NULL)
+		/* Successful.  */
+		result = existing;
+	      else
+		/* Something went wrong while inserting the new
+		   value.  We fail completely.  */
+		free (newp);
+	    }
+	}
+
+      /* Release the lock.  */
+      lll_unlock (__sem_mappings_lock);
+    }
+
+  if (result != existing && existing != SEM_FAILED && existing != MAP_FAILED)
+    {
+      /* Do not disturb errno.  */
+#if 0
+      /* FIXME */
+      INTERNAL_SYSCALL_DECL (err);
+      INTERNAL_SYSCALL (munmap, err, 2, existing, sizeof (sem_t));
+#endif
+    }
+
+  return result;
+}
+
+
+sem_t *
+sem_open (const char *name, int oflag, ...)
+{
+  char *finalname;
+  sem_t *result = SEM_FAILED;
+  int fd;
+
+  /* Determine where the shmfs is mounted.  */
+  INTUSE(__pthread_once) (&__namedsem_once, __where_is_shmfs);
+
+  /* If we don't know the mount points there is nothing we can do.  Ever.  */
+  if (mountpoint.dir == NULL)
+    {
+      __set_errno (ENOSYS);
+      return SEM_FAILED;
+    }
+
+  /* Construct the filename.  */
+  while (name[0] == '/')
+    ++name;
+
+  if (name[0] == '\0')
+    {
+      /* The name "/" is not supported.  */
+      __set_errno (EINVAL);
+      return SEM_FAILED;
+    }
+  size_t namelen = strlen (name) + 1;
+
+  /* Create the name of the final file.  */
+  finalname = (char *) alloca (mountpoint.dirlen + namelen);
+  __mempcpy (__mempcpy (finalname, mountpoint.dir, mountpoint.dirlen),
+	     name, namelen);
+
+  /* If the semaphore object has to exist simply open it.  */
+  if ((oflag & O_CREAT) == 0 || (oflag & O_EXCL) == 0)
+    {
+    try_again:
+      fd = __libc_open (finalname,
+			(oflag & ~(O_CREAT|O_ACCMODE)) | O_NOFOLLOW | O_RDWR);
+
+      if (fd == -1)
+	{
+	  /* If we are supposed to create the file try this next.  */
+	  if ((oflag & O_CREAT) != 0 && errno == ENOENT)
+	    goto try_create;
+
+	  /* Return.  errno is already set.  */
+	}
+      else
+	/* Check whether we already have this semaphore mapped and
+	   create one if necessary.  */
+	result = check_add_mapping (name, namelen, fd, SEM_FAILED);
+    }
+  else
+    {
+      /* We have to open a temporary file first since it must have the
+	 correct form before we can start using it.  */
+      char *tmpfname;
+      mode_t mode;
+      unsigned int value;
+      va_list ap;
+
+    try_create:
+      va_start (ap, oflag);
+
+      mode = va_arg (ap, mode_t);
+      value = va_arg (ap, unsigned int);
+
+      va_end (ap);
+
+      if (value > SEM_VALUE_MAX)
+	{
+	  __set_errno (EINVAL);
+	  return SEM_FAILED;
+	}
+
+      /* Create the initial file content.  */
+      sem_t initsem;
+
+      struct sem *iinitsem = (struct sem *) &initsem;
+      iinitsem->count = value;
+
+      /* Initialize the remaining bytes as well.  */
+      memset ((char *) &initsem + sizeof (struct sem), '\0',
+	      sizeof (sem_t) - sizeof (struct sem));
+
+      tmpfname = (char *) alloca (mountpoint.dirlen + 6 + 1);
+      char *xxxxxx = __mempcpy (tmpfname, mountpoint.dir, mountpoint.dirlen);
+
+      int retries = 0;
+#define NRETRIES 50
+      while (1)
+	{
+	  /* Add the suffix for mktemp.  */
+	  strcpy (xxxxxx, "XXXXXX");
+
+	  /* We really want to use mktemp here.  We cannot use mkstemp
+	     since the file must be opened with a specific mode.  The
+	     mode cannot later be set since then we cannot apply the
+	     file create mask.  */
+	  if (mktemp (tmpfname) == NULL)
+	    return SEM_FAILED;
+
+	  /* Open the file.  Make sure we do not overwrite anything.  */
+	  fd = __libc_open (tmpfname, O_RDWR | O_CREAT | O_EXCL, mode);
+	  if (fd == -1)
+	    {
+	      if (errno == EEXIST)
+		{
+		  if (++retries < NRETRIES)
+		    continue;
+
+		  __set_errno (EAGAIN);
+		}
+
+	      return SEM_FAILED;
+	    }
+
+	  /* We got a file.  */
+	  break;
+	}
+
+      if (TEMP_FAILURE_RETRY (__libc_write (fd, &initsem, sizeof (sem_t)))
+	  == sizeof (sem_t)
+	  /* Map the sem_t structure from the file.  */
+	  && (result = (sem_t *) mmap (NULL, sizeof (sem_t),
+				       PROT_READ | PROT_WRITE, MAP_SHARED,
+				       fd, 0)) != MAP_FAILED)
+	{
+	  /* Create the file.  Don't overwrite an existing file.  */
+	  if (link (tmpfname, finalname) != 0)
+	    {
+	      /* Undo the mapping.  */
+	      (void) munmap (result, sizeof (sem_t));
+
+	      /* Reinitialize 'result'.  */
+	      result = SEM_FAILED;
+
+	      /* This failed.  If O_EXCL is not set and the problem was
+		 that the file exists, try again.  */
+	      if ((oflag & O_EXCL) == 0 && errno == EEXIST)
+		{
+		  /* Remove the file.  */
+		  (void) unlink (tmpfname);
+
+		  /* Close the file.  */
+		  (void) __libc_close (fd);
+
+		  goto try_again;
+		}
+	    }
+	  else
+	    /* Insert the mapping into the search tree.  This also
+	       determines whether another thread sneaked by and already
+	       added such a mapping despite the fact that we created it.  */
+	    result = check_add_mapping (name, namelen, fd, result);
+	}
+
+      /* Now remove the temporary name.  This should never fail.  If
+	 it fails we leak a file name.  Better fix the kernel.  */
+      (void) unlink (tmpfname);
+    }
+
+  /* Map the mmap error to the error we need.  */
+  if (MAP_FAILED != (void *) SEM_FAILED && result == MAP_FAILED)
+    result = SEM_FAILED;
+
+  /* We don't need the file descriptor anymore.  */
+  if (fd != -1)
+    {
+#if 0
+      /* FIXME */
+
+      /* Do not disturb errno.  */
+      INTERNAL_SYSCALL_DECL (err);
+      INTERNAL_SYSCALL (close, err, 1, fd);
+#endif
+    }
+
+  return result;
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/sem_post.c libc/nptl/sysdeps/l4/hurd/sem_post.c
--- libc/nptl/sysdeps/l4/hurd/sem_post.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/sem_post.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,49 @@
+/* sem_post -- post to a POSIX semaphore.  Generic futex-using version.
+   Copyright (C) 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Jakub Jelinek <jakub@redhat.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	 See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <sysdep.h>
+#include <lowlevellock.h>
+#include <internaltypes.h>
+#include <semaphore.h>
+
+#include <shlib-compat.h>
+
+int
+__new_sem_post (sem_t *sem)
+{
+  int *futex = (int *) sem;
+
+  int nr = atomic_increment_val (futex);
+#if 0
+  int err = lll_futex_wake (futex, nr);
+  if (__builtin_expect (err, 0) < 0)
+    {
+      __set_errno (-err);
+      return -1;
+    }
+#endif
+  return 0;
+}
+versioned_symbol (libpthread, __new_sem_post, sem_post, GLIBC_2_1);
+#if SHLIB_COMPAT (libpthread, GLIBC_2_0, GLIBC_2_1)
+strong_alias (__new_sem_post, __old_sem_post)
+compat_symbol (libpthread, __old_sem_post, sem_post, GLIBC_2_0);
+#endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/sem_timedwait.c libc/nptl/sysdeps/l4/hurd/sem_timedwait.c
--- libc/nptl/sysdeps/l4/hurd/sem_timedwait.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/sem_timedwait.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,102 @@
+/* sem_timedwait -- wait on a semaphore.  Generic futex-using version.
+   Copyright (C) 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Paul Mackerras <paulus@au.ibm.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	 See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <sysdep.h>
+#include <lowlevellock.h>
+#include <internaltypes.h>
+#include <semaphore.h>
+
+#include <pthreadP.h>
+#include <shlib-compat.h>
+
+
+int
+sem_timedwait (sem_t *sem, const struct timespec *abstime)
+{
+  /* First check for cancellation.  */
+  CANCELLATION_P (THREAD_SELF);
+
+  int *futex = (int *) sem;
+  int val;
+  int err;
+
+  if (*futex > 0)
+    {
+      val = atomic_decrement_if_positive (futex);
+      if (val > 0)
+	return 0;
+    }
+
+  err = -EINVAL;
+  if (abstime->tv_nsec < 0 || abstime->tv_nsec >= 1000000000)
+    goto error_return;
+
+  do
+    {
+      struct timeval tv;
+      struct timespec rt;
+      int sec, nsec;
+
+      /* Get the current time.  */
+      __gettimeofday (&tv, NULL);
+
+      /* Compute relative timeout.  */
+      sec = abstime->tv_sec - tv.tv_sec;
+      nsec = abstime->tv_nsec - tv.tv_usec * 1000;
+      if (nsec < 0)
+	{
+	  nsec += 1000000000;
+	  --sec;
+	}
+
+      /* Already timed out?  */
+      err = -ETIMEDOUT;
+      if (sec < 0)
+	goto error_return;
+
+      /* Do wait.  */
+      rt.tv_sec = sec;
+      rt.tv_nsec = nsec;
+
+      /* Enable asynchronous cancellation.  Required by the standard.  */
+      int oldtype = __pthread_enable_asynccancel ();
+
+#if 0
+      /* FIXME*/
+      err = lll_futex_timed_wait (futex, 0, &rt);
+#endif
+
+      /* Disable asynchronous cancellation.  */
+      __pthread_disable_asynccancel (oldtype);
+
+      if (err != 0 && err != -EWOULDBLOCK)
+	goto error_return;
+
+      val = atomic_decrement_if_positive (futex);
+    }
+  while (val <= 0);
+
+  return 0;
+
+ error_return:
+  __set_errno (-err);
+  return -1;
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/sem_trywait.c libc/nptl/sysdeps/l4/hurd/sem_trywait.c
--- libc/nptl/sysdeps/l4/hurd/sem_trywait.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/sem_trywait.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,50 @@
+/* sem_trywait -- wait on a semaphore.  Generic futex-using version.
+   Copyright (C) 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Paul Mackerras <paulus@au.ibm.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	 See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <sysdep.h>
+#include <lowlevellock.h>
+#include <internaltypes.h>
+#include <semaphore.h>
+
+#include <shlib-compat.h>
+
+
+int
+__new_sem_trywait (sem_t *sem)
+{
+  int *futex = (int *) sem;
+  int val;
+
+  if (*futex > 0)
+    {
+      val = atomic_decrement_if_positive (futex);
+      if (val > 0)
+	return 0;
+    }
+
+  __set_errno (EAGAIN);
+  return -1;
+}
+versioned_symbol (libpthread, __new_sem_trywait, sem_trywait, GLIBC_2_1);
+#if SHLIB_COMPAT (libpthread, GLIBC_2_0, GLIBC_2_1)
+strong_alias (__new_sem_trywait, __old_sem_trywait)
+compat_symbol (libpthread, __old_sem_trywait, sem_trywait, GLIBC_2_0);
+#endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/sem_wait.c libc/nptl/sysdeps/l4/hurd/sem_wait.c
--- libc/nptl/sysdeps/l4/hurd/sem_wait.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/sem_wait.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,66 @@
+/* sem_wait -- wait on a semaphore.  Generic futex-using version.
+   Copyright (C) 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Paul Mackerras <paulus@au.ibm.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	 See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <sysdep.h>
+#include <lowlevellock.h>
+#include <internaltypes.h>
+#include <semaphore.h>
+
+#include <pthreadP.h>
+#include <shlib-compat.h>
+
+
+int
+__new_sem_wait (sem_t *sem)
+{
+  /* First check for cancellation.  */
+  CANCELLATION_P (THREAD_SELF);
+
+  int *futex = (int *) sem;
+  int err;
+
+  do
+    {
+      if (atomic_decrement_if_positive (futex) > 0)
+	return 0;
+
+      /* Enable asynchronous cancellation.  Required by the standard.  */
+      int oldtype = __pthread_enable_asynccancel ();
+
+#if 0
+      /*FIXME*/
+      err = lll_futex_wait (futex, 0);
+#endif
+
+      /* Disable asynchronous cancellation.  */
+      __pthread_disable_asynccancel (oldtype);
+    }
+  while (err == 0 || err == -EWOULDBLOCK);
+
+  __set_errno (-err);
+  return -1;
+}
+
+versioned_symbol (libpthread, __new_sem_wait, sem_wait, GLIBC_2_1);
+#if SHLIB_COMPAT (libpthread, GLIBC_2_0, GLIBC_2_1)
+strong_alias (__new_sem_wait, __old_sem_wait)
+compat_symbol (libpthread, __old_sem_wait, sem_wait, GLIBC_2_0);
+#endif
diff -x CVS -rupN libc/nptl/sysdeps/l4/hurd/unregister-atfork.c libc/nptl/sysdeps/l4/hurd/unregister-atfork.c
--- libc/nptl/sysdeps/l4/hurd/unregister-atfork.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/hurd/unregister-atfork.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,111 @@
+/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <stdlib.h>
+#include "fork.h"
+#include <atomic.h>
+
+
+void
+__unregister_atfork (dso_handle)
+     void *dso_handle;
+{
+  /* Check whether there is any entry in the list which we have to
+     remove.  It is likely that this is not the case so don't bother
+     getting the lock.
+
+     We do not worry about other threads adding entries for this DSO
+     right this moment.  If this happens this is a race and we can do
+     whatever we please.  The program will crash anyway seen.  */
+  struct fork_handler *runp = __fork_handlers;
+  struct fork_handler *lastp = NULL;
+
+  while (runp != NULL)
+    if (runp->dso_handle == dso_handle)
+      break;
+    else
+      {
+	lastp = runp;
+	runp = runp->next;
+      }
+
+  if (runp == NULL)
+    /* Nothing to do.  */
+    return;
+
+  /* Get the lock to not conflict with additions or deletions.  Note
+     that there couldn't have been another thread deleting something.
+     The __unregister_atfork function is only called from the
+     dlclose() code which itself serializes the operations.  */
+  lll_lock (__fork_lock);
+
+  /* We have to create a new list with all the entries we don't remove.  */
+  struct deleted_handler
+  {
+    struct fork_handler *handler;
+    struct deleted_handler *next;
+  } *deleted = NULL;
+
+  /* Remove the entries for the DSO which is unloaded from the list.
+     It's a single linked list so readers are.  */
+  do
+    {
+      if (runp->dso_handle == dso_handle)
+	{
+	  if (lastp == NULL)
+	    __fork_handlers = runp->next;
+	  else
+	    lastp->next = runp->next;
+
+	  /* We cannot overwrite the ->next element now.  Put the deleted
+	     entries in a separate list.  */
+	  struct deleted_handler *newp = alloca (sizeof (*newp));
+	  newp->handler = runp;
+	  newp->next = deleted;
+	  deleted = newp;
+	}
+      else
+	lastp = runp;
+
+      runp = runp->next;
+    }
+  while (runp != NULL);
+
+  /* Release the lock.  */
+  lll_unlock (__fork_lock);
+
+  /* Walk the list of all entries which have to be deleted.  */
+  while (deleted != NULL)
+    {
+      /* We need to be informed by possible current users.  */
+      deleted->handler->need_signal = 1;
+      /* Make sure this gets written out first.  */
+      atomic_write_barrier ();
+
+      /* Decrement the reference counter.  If it does not reach zero
+	 wait for the last user.  */
+      atomic_decrement (&deleted->handler->refcntr);
+      unsigned int val;
+      while ((val = deleted->handler->refcntr) != 0)
+	lll_futex_wait (&deleted->handler->refcntr, val);
+
+      deleted = deleted->next;
+    }
+}
diff -x CVS -rupN libc/nptl/sysdeps/l4/smp.h libc/nptl/sysdeps/l4/smp.h
--- libc/nptl/sysdeps/l4/smp.h	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/l4/smp.h	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,37 @@
+/* Determine whether the host has multiple processors.  L4 version.
+   Copyright (C) 1996, 2002, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Library General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Library General Public License for more details.
+
+   You should have received a copy of the GNU Library General Public
+   License along with the GNU C Library; see the file COPYING.LIB.  If not,
+   write to the Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+   Boston, MA 02111-1307, USA.  */
+
+#if 0
+#include <l4.h>
+#endif
+
+
+/* Test whether the machine has more than one processor.  This test
+   does not check if the Hurd's scheduler will actually ever schedule
+   us to more than one processor.  */
+static inline int
+is_smp_system (void)
+{
+#if 0
+  /* FIXME: Enable this.  */
+  return l4_num_processors () > 1;
+#else
+  return 0;
+#endif
+}
diff -x CVS -rupN libc/nptl/sysdeps/pthread/createthread.c libc/nptl/sysdeps/pthread/createthread.c
--- libc/nptl/sysdeps/pthread/createthread.c	2005-01-23 18:39:29.000000000 +0100
+++ libc/nptl/sysdeps/pthread/createthread.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,255 +0,0 @@
-/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <sched.h>
-#include <setjmp.h>
-#include <signal.h>
-#include <stdlib.h>
-#include <atomic.h>
-#include <ldsodefs.h>
-#include <tls.h>
-
-#include "kernel-features.h"
-
-
-#define CLONE_SIGNAL    	(CLONE_SIGHAND | CLONE_THREAD)
-
-/* Unless otherwise specified, the thread "register" is going to be
-   initialized with a pointer to the TCB.  */
-#ifndef TLS_VALUE
-# define TLS_VALUE pd
-#endif
-
-#ifndef ARCH_CLONE
-# define ARCH_CLONE __clone
-#endif
-
-
-#ifndef TLS_MULTIPLE_THREADS_IN_TCB
-/* Pointer to the corresponding variable in libc.  */
-int *__libc_multiple_threads_ptr attribute_hidden;
-#endif
-
-
-static int
-do_clone (struct pthread *pd, const struct pthread_attr *attr,
-	  int clone_flags, int (*fct) (void *), STACK_VARIABLES_PARMS,
-	  int stopped)
-{
-#ifdef PREPARE_CREATE
-  PREPARE_CREATE;
-#endif
-
-  if (stopped)
-    /* We Make sure the thread does not run far by forcing it to get a
-       lock.  We lock it here too so that the new thread cannot continue
-       until we tell it to.  */
-    lll_lock (pd->lock);
-
-  /* One more thread.  We cannot have the thread do this itself, since it
-     might exist but not have been scheduled yet by the time we've returned
-     and need to check the value to behave correctly.  We must do it before
-     creating the thread, in case it does get scheduled first and then
-     might mistakenly think it was the only thread.  In the failure case,
-     we momentarily store a false value; this doesn't matter because there
-     is no kosher thing a signal handler interrupting us right here can do
-     that cares whether the thread count is correct.  */
-  atomic_increment (&__nptl_nthreads);
-
-  if (ARCH_CLONE (fct, STACK_VARIABLES_ARGS, clone_flags,
-		  pd, &pd->tid, TLS_VALUE, &pd->tid) == -1)
-    {
-      atomic_decrement (&__nptl_nthreads); /* Oops, we lied for a second.  */
-
-      /* Failed.  If the thread is detached, remove the TCB here since
-	 the caller cannot do this.  The caller remembered the thread
-	 as detached and cannot reverify that it is not since it must
-	 not access the thread descriptor again.  */
-      if (IS_DETACHED (pd))
-	__deallocate_stack (pd);
-
-      return errno;
-    }
-
-  /* Now we have the possibility to set scheduling parameters etc.  */
-  if (__builtin_expect (stopped != 0, 0))
-    {
-      INTERNAL_SYSCALL_DECL (err);
-      int res = 0;
-
-      /* Set the affinity mask if necessary.  */
-      if (attr->cpuset != NULL)
-	{
-	  res = INTERNAL_SYSCALL (sched_setaffinity, err, 3, pd->tid,
-				  sizeof (cpu_set_t), attr->cpuset);
-
-	  if (__builtin_expect (INTERNAL_SYSCALL_ERROR_P (res, err), 0))
-	    {
-	      /* The operation failed.  We have to kill the thread.  First
-		 send it the cancellation signal.  */
-	      INTERNAL_SYSCALL_DECL (err2);
-	    err_out:
-#if __ASSUME_TGKILL
-	      (void) INTERNAL_SYSCALL (tgkill, err2, 3,
-				       THREAD_GETMEM (THREAD_SELF, pid),
-				       pd->tid, SIGCANCEL);
-#else
-	      (void) INTERNAL_SYSCALL (tkill, err2, 2, pd->tid, SIGCANCEL);
-#endif
-
-	      return (INTERNAL_SYSCALL_ERROR_P (res, err)
-		      ? INTERNAL_SYSCALL_ERRNO (res, err)
-		      : 0);
-	    }
-	}
-
-      /* Set the scheduling parameters.  */
-      if ((attr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0)
-	{
-	  res = INTERNAL_SYSCALL (sched_setscheduler, err, 3, pd->tid,
-				  pd->schedpolicy, &pd->schedparam);
-
-	  if (__builtin_expect (INTERNAL_SYSCALL_ERROR_P (res, err), 0))
-	    goto err_out;
-	}
-    }
-
-  /* We now have for sure more than one thread.  The main thread might
-     not yet have the flag set.  No need to set the global variable
-     again if this is what we use.  */
-  THREAD_SETMEM (THREAD_SELF, header.multiple_threads, 1);
-
-  return 0;
-}
-
-
-static int
-create_thread (struct pthread *pd, const struct pthread_attr *attr,
-	       STACK_VARIABLES_PARMS)
-{
-#ifdef TLS_TCB_AT_TP
-  assert (pd->header.tcb != NULL);
-#endif
-
-  /* We rely heavily on various flags the CLONE function understands:
-
-     CLONE_VM, CLONE_FS, CLONE_FILES
-	These flags select semantics with shared address space and
-	file descriptors according to what POSIX requires.
-
-     CLONE_SIGNAL
-	This flag selects the POSIX signal semantics.
-
-     CLONE_SETTLS
-	The sixth parameter to CLONE determines the TLS area for the
-	new thread.
-
-     CLONE_PARENT_SETTID
-	The kernels writes the thread ID of the newly created thread
-	into the location pointed to by the fifth parameters to CLONE.
-
-	Note that it would be semantically equivalent to use
-	CLONE_CHILD_SETTID but it is be more expensive in the kernel.
-
-     CLONE_CHILD_CLEARTID
-	The kernels clears the thread ID of a thread that has called
-	sys_exit() in the location pointed to by the seventh parameter
-	to CLONE.
-
-     CLONE_DETACHED
-	No signal is generated if the thread exists and it is
-	automatically reaped.
-
-     The termination signal is chosen to be zero which means no signal
-     is sent.  */
-  int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGNAL
-		     | CLONE_SETTLS | CLONE_PARENT_SETTID
-		     | CLONE_CHILD_CLEARTID | CLONE_SYSVSEM
-#if __ASSUME_NO_CLONE_DETACHED == 0
-		     | CLONE_DETACHED
-#endif
-		     | 0);
-
-  if (__builtin_expect (THREAD_GETMEM (THREAD_SELF, report_events), 0))
-    {
-      /* The parent thread is supposed to report events.  Check whether
-	 the TD_CREATE event is needed, too.  */
-      const int _idx = __td_eventword (TD_CREATE);
-      const uint32_t _mask = __td_eventmask (TD_CREATE);
-
-      if ((_mask & (__nptl_threads_events.event_bits[_idx]
-		    | pd->eventbuf.eventmask.event_bits[_idx])) != 0)
-	{
-	  /* We always must have the thread start stopped.  */
-	  pd->stopped_start = true;
-
-	  /* Create the thread.  We always create the thread stopped
-	     so that it does not get far before we tell the debugger.  */
-	  int res = do_clone (pd, attr, clone_flags, start_thread,
-			      STACK_VARIABLES_ARGS, 1);
-	  if (res == 0)
-	    {
-	      /* Now fill in the information about the new thread in
-		 the newly created thread's data structure.  We cannot let
-		 the new thread do this since we don't know whether it was
-		 already scheduled when we send the event.  */
-	      pd->eventbuf.eventnum = TD_CREATE;
-	      pd->eventbuf.eventdata = pd;
-
-	      /* Enqueue the descriptor.  */
-	      do
-		pd->nextevent = __nptl_last_event;
-	      while (atomic_compare_and_exchange_bool_acq (&__nptl_last_event,
-							   pd, pd->nextevent)
-		     != 0);
-
-	      /* Now call the function which signals the event.  */
-	      __nptl_create_event ();
-
-	      /* And finally restart the new thread.  */
-	      lll_unlock (pd->lock);
-	    }
-
-	  return res;
-	}
-    }
-
-#ifdef NEED_DL_SYSINFO
-  assert (THREAD_SELF_SYSINFO == THREAD_SYSINFO (pd));
-#endif
-
-  /* Determine whether the newly created threads has to be started
-     stopped since we have to set the scheduling parameters or set the
-     affinity.  */
-  bool stopped = false;
-  if (attr != NULL && (attr->cpuset != NULL
-		       || (attr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0))
-    stopped = true;
-  pd->stopped_start = stopped;
-
-  /* Actually create the thread.  */
-  int res = do_clone (pd, attr, clone_flags, start_thread,
-		      STACK_VARIABLES_ARGS, stopped);
-
-  if (res == 0 && stopped)
-    /* And finally restart the new thread.  */
-    lll_unlock (pd->lock);
-
-  return res;
-}
diff -x CVS -rupN libc/nptl/sysdeps/pthread/pthread_cond_broadcast.c libc/nptl/sysdeps/pthread/pthread_cond_broadcast.c
--- libc/nptl/sysdeps/pthread/pthread_cond_broadcast.c	2004-10-10 12:41:06.000000000 +0200
+++ libc/nptl/sysdeps/pthread/pthread_cond_broadcast.c	2005-01-23 19:35:29.000000000 +0100
@@ -25,7 +25,7 @@
 #include <pthreadP.h>
 
 #include <shlib-compat.h>
-#include <kernel-features.h>
+#include <lowlevellock.h>
 
 
 int
diff -x CVS -rupN libc/nptl/sysdeps/pthread/pthread_cond_signal.c libc/nptl/sysdeps/pthread/pthread_cond_signal.c
--- libc/nptl/sysdeps/pthread/pthread_cond_signal.c	2004-10-10 12:41:07.000000000 +0200
+++ libc/nptl/sysdeps/pthread/pthread_cond_signal.c	2005-01-23 19:35:29.000000000 +0100
@@ -25,7 +25,7 @@
 #include <pthreadP.h>
 
 #include <shlib-compat.h>
-#include <kernel-features.h>
+#include <lowlevellock.h>
 
 
 int
diff -x CVS -rupN libc/nptl/sysdeps/pthread/pthread_sigmask.c libc/nptl/sysdeps/pthread/pthread_sigmask.c
--- libc/nptl/sysdeps/pthread/pthread_sigmask.c	2004-10-10 12:41:07.000000000 +0200
+++ libc/nptl/sysdeps/pthread/pthread_sigmask.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,58 +0,0 @@
-/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <errno.h>
-#include <signal.h>
-#include <pthreadP.h>
-#include <sysdep.h>
-
-
-int
-pthread_sigmask (how, newmask, oldmask)
-     int how;
-     const sigset_t *newmask;
-     sigset_t *oldmask;
-{
-  sigset_t local_newmask;
-
-  /* The only thing we have to make sure here is that SIGCANCEL and
-     SIGSETXID is not blocked.  */
-  if (newmask != NULL
-      && (__builtin_expect (__sigismember (newmask, SIGCANCEL), 0)
-	  || __builtin_expect (__sigismember (newmask, SIGSETXID), 0)))
-    {
-      local_newmask = *newmask;
-      __sigdelset (&local_newmask, SIGCANCEL);
-      __sigdelset (&local_newmask, SIGSETXID);
-      newmask = &local_newmask;
-    }
-
-#ifdef INTERNAL_SYSCALL
-  /* We know that realtime signals are available if NPTL is used.  */
-  INTERNAL_SYSCALL_DECL (err);
-  int result = INTERNAL_SYSCALL (rt_sigprocmask, err, 4, how, newmask,
-				 oldmask, _NSIG / 8);
-
-  return (INTERNAL_SYSCALL_ERROR_P (result, err)
-	  ? INTERNAL_SYSCALL_ERRNO (result, err)
-	  : 0);
-#else
-  return sigprocmask (how, newmask, oldmask) == -1 ? errno : 0;
-#endif
-}
diff -x CVS -rupN libc/nptl/sysdeps/pthread/sigaction.c libc/nptl/sysdeps/pthread/sigaction.c
--- libc/nptl/sysdeps/pthread/sigaction.c	2004-10-10 12:41:07.000000000 +0200
+++ libc/nptl/sysdeps/pthread/sigaction.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,54 +0,0 @@
-/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-/* This is tricky.  GCC doesn't like #include_next in the primary
-   source file and even if it did, the first #include_next is this
-   exact file anyway.  */
-#ifndef LIBC_SIGACTION
-
-#include <nptl/pthreadP.h>
-
-/* We use the libc implementation but we tell it to not allow
-   SIGCANCEL or SIGTIMER to be handled.  */
-# define LIBC_SIGACTION	1
-
-# include <nptl/sysdeps/pthread/sigaction.c>
-
-int
-__sigaction (sig, act, oact)
-     int sig;
-     const struct sigaction *act;
-     struct sigaction *oact;
-{
-  if (__builtin_expect (sig == SIGCANCEL || sig == SIGSETXID, 0))
-    {
-      __set_errno (EINVAL);
-      return -1;
-    }
-
-  return __libc_sigaction (sig, act, oact);
-}
-libc_hidden_weak (__sigaction)
-weak_alias (__sigaction, sigaction)
-
-#else
-
-# include_next <sigaction.c>
-
-#endif /* LIBC_SIGACTION */
diff -x CVS -rupN libc/nptl/sysdeps/pthread/sigfillset.c libc/nptl/sysdeps/pthread/sigfillset.c
--- libc/nptl/sysdeps/pthread/sigfillset.c	2003-04-21 09:47:36.000000000 +0200
+++ libc/nptl/sysdeps/pthread/sigfillset.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,21 +0,0 @@
-/* Copyright (C) 2003 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <nptl/pthreadP.h>
-
-#include <sysdeps/generic/sigfillset.c>
diff -x CVS -rupN libc/nptl/sysdeps/pthread/sigprocmask.c libc/nptl/sysdeps/pthread/sigprocmask.c
--- libc/nptl/sysdeps/pthread/sigprocmask.c	2003-04-21 09:35:27.000000000 +0200
+++ libc/nptl/sysdeps/pthread/sigprocmask.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,20 +0,0 @@
-/* Copyright (C) 1997,1998,1999,2000,2001,2003 Free Software Foundation, Inc.
-   This file is part of the GNU C Library.
-
-   The GNU C Library is free software; you can redistribute it and/or
-   modify it under the terms of the GNU Lesser General Public
-   License as published by the Free Software Foundation; either
-   version 2.1 of the License, or (at your option) any later version.
-
-   The GNU C Library is distributed in the hope that it will be useful,
-   but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-   Lesser General Public License for more details.
-
-   You should have received a copy of the GNU Lesser General Public
-   License along with the GNU C Library; if not, write to the Free
-   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
-   02111-1307 USA.  */
-
-#include <nptl/pthreadP.h>
-#include <sysdeps/unix/sysv/linux/sigprocmask.c>
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/allocatestack.c libc/nptl/sysdeps/unix/sysv/linux/allocatestack.c
--- libc/nptl/sysdeps/unix/sysv/linux/allocatestack.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/allocatestack.c	2005-01-23 20:01:28.000000000 +0100
@@ -0,0 +1,947 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <assert.h>
+#include <errno.h>
+#include <signal.h>
+#include <stdint.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/param.h>
+#include <dl-sysdep.h>
+#include <tls.h>
+#include <lowlevellock.h>
+
+
+#ifndef NEED_SEPARATE_REGISTER_STACK
+
+/* Most architectures have exactly one stack pointer.  Some have more.  */
+# define STACK_VARIABLES void *stackaddr
+
+/* How to pass the values to the 'create_thread' function.  */
+# define STACK_VARIABLES_ARGS stackaddr
+
+/* How to declare function which gets there parameters.  */
+# define STACK_VARIABLES_PARMS void *stackaddr
+
+/* How to declare allocate_stack.  */
+# define ALLOCATE_STACK_PARMS void **stack
+
+/* This is how the function is called.  We do it this way to allow
+   other variants of the function to have more parameters.  */
+# define ALLOCATE_STACK(attr, pd) allocate_stack (attr, pd, &stackaddr)
+
+#else
+
+/* We need two stacks.  The kernel will place them but we have to tell
+   the kernel about the size of the reserved address space.  */
+# define STACK_VARIABLES void *stackaddr; size_t stacksize
+
+/* How to pass the values to the 'create_thread' function.  */
+# define STACK_VARIABLES_ARGS stackaddr, stacksize
+
+/* How to declare function which gets there parameters.  */
+# define STACK_VARIABLES_PARMS void *stackaddr, size_t stacksize
+
+/* How to declare allocate_stack.  */
+# define ALLOCATE_STACK_PARMS void **stack, size_t *stacksize
+
+/* This is how the function is called.  We do it this way to allow
+   other variants of the function to have more parameters.  */
+# define ALLOCATE_STACK(attr, pd) \
+  allocate_stack (attr, pd, &stackaddr, &stacksize)
+
+#endif
+
+
+/* Default alignment of stack.  */
+#ifndef STACK_ALIGN
+# define STACK_ALIGN __alignof__ (long double)
+#endif
+
+/* Default value for minimal stack size after allocating thread
+   descriptor and guard.  */
+#ifndef MINIMAL_REST_STACK
+# define MINIMAL_REST_STACK	4096
+#endif
+
+
+/* Let the architecture add some flags to the mmap() call used to
+   allocate stacks.  */
+#ifndef ARCH_MAP_FLAGS
+# define ARCH_MAP_FLAGS 0
+#endif
+
+/* This yields the pointer that TLS support code calls the thread pointer.  */
+#if TLS_TCB_AT_TP
+# define TLS_TPADJ(pd) (pd)
+#elif TLS_DTV_AT_TP
+# define TLS_TPADJ(pd) ((struct pthread *)((char *) (pd) + TLS_PRE_TCB_SIZE))
+#endif
+
+/* Cache handling for not-yet free stacks.  */
+
+/* Maximum size in kB of cache.  */
+static size_t stack_cache_maxsize = 40 * 1024 * 1024; /* 40MiBi by default.  */
+static size_t stack_cache_actsize;
+
+/* Mutex protecting this variable.  */
+static lll_lock_t stack_cache_lock = LLL_LOCK_INITIALIZER;
+
+/* List of queued stack frames.  */
+static LIST_HEAD (stack_cache);
+
+/* List of the stacks in use.  */
+static LIST_HEAD (stack_used);
+
+/* List of the threads with user provided stacks in use.  No need to
+   initialize this, since it's done in __pthread_initialize_minimal.  */
+list_t __stack_user __attribute__ ((nocommon));
+hidden_data_def (__stack_user)
+
+#if COLORING_INCREMENT != 0
+/* Number of threads created.  */
+static unsigned int nptl_ncreated;
+#endif
+
+
+/* Check whether the stack is still used or not.  */
+#define FREE_P(descr) ((descr)->tid <= 0)
+
+
+/* We create a double linked list of all cache entries.  Double linked
+   because this allows removing entries from the end.  */
+
+
+/* Get a stack frame from the cache.  We have to match by size since
+   some blocks might be too small or far too large.  */
+static struct pthread *
+get_cached_stack (size_t *sizep, void **memp)
+{
+  size_t size = *sizep;
+  struct pthread *result = NULL;
+  list_t *entry;
+
+  lll_lock (stack_cache_lock);
+
+  /* Search the cache for a matching entry.  We search for the
+     smallest stack which has at least the required size.  Note that
+     in normal situations the size of all allocated stacks is the
+     same.  As the very least there are only a few different sizes.
+     Therefore this loop will exit early most of the time with an
+     exact match.  */
+  list_for_each (entry, &stack_cache)
+    {
+      struct pthread *curr;
+
+      curr = list_entry (entry, struct pthread, list);
+      if (FREE_P (curr) && curr->stackblock_size >= size)
+	{
+	  if (curr->stackblock_size == size)
+	    {
+	      result = curr;
+	      break;
+	    }
+
+	  if (result == NULL
+	      || result->stackblock_size > curr->stackblock_size)
+	    result = curr;
+	}
+    }
+
+  if (__builtin_expect (result == NULL, 0)
+      /* Make sure the size difference is not too excessive.  In that
+	 case we do not use the block.  */
+      || __builtin_expect (result->stackblock_size > 4 * size, 0))
+    {
+      /* Release the lock.  */
+      lll_unlock (stack_cache_lock);
+
+      return NULL;
+    }
+
+  /* Dequeue the entry.  */
+  list_del (&result->list);
+
+  /* And add to the list of stacks in use.  */
+  list_add (&result->list, &stack_used);
+
+  /* And decrease the cache size.  */
+  stack_cache_actsize -= result->stackblock_size;
+
+  /* Release the lock early.  */
+  lll_unlock (stack_cache_lock);
+
+  /* Report size and location of the stack to the caller.  */
+  *sizep = result->stackblock_size;
+  *memp = result->stackblock;
+
+  /* Cancellation handling is back to the default.  */
+  result->cancelhandling = 0;
+  result->cleanup = NULL;
+
+  /* No pending event.  */
+  result->nextevent = NULL;
+
+  /* Clear the DTV.  */
+  dtv_t *dtv = GET_DTV (TLS_TPADJ (result));
+  memset (dtv, '\0', (dtv[-1].counter + 1) * sizeof (dtv_t));
+
+  /* Re-initialize the TLS.  */
+  _dl_allocate_tls_init (TLS_TPADJ (result));
+
+  return result;
+}
+
+
+/* Add a stack frame which is not used anymore to the stack.  Must be
+   called with the cache lock held.  */
+static inline void
+__attribute ((always_inline))
+queue_stack (struct pthread *stack)
+{
+  /* We unconditionally add the stack to the list.  The memory may
+     still be in use but it will not be reused until the kernel marks
+     the stack as not used anymore.  */
+  list_add (&stack->list, &stack_cache);
+
+  stack_cache_actsize += stack->stackblock_size;
+  if (__builtin_expect (stack_cache_actsize > stack_cache_maxsize, 0))
+    {
+      /* We reduce the size of the cache.  Remove the last entries
+	 until the size is below the limit.  */
+      list_t *entry;
+      list_t *prev;
+
+      /* Search from the end of the list.  */
+      list_for_each_prev_safe (entry, prev, &stack_cache)
+	{
+	  struct pthread *curr;
+
+	  curr = list_entry (entry, struct pthread, list);
+	  if (FREE_P (curr))
+	    {
+	      /* Unlink the block.  */
+	      list_del (entry);
+
+	      /* Account for the freed memory.  */
+	      stack_cache_actsize -= curr->stackblock_size;
+
+	      /* Free the memory associated with the ELF TLS.  */
+	      _dl_deallocate_tls (TLS_TPADJ (curr), false);
+
+	      /* Remove this block.  This should never fail.  If it
+		 does something is really wrong.  */
+	      if (munmap (curr->stackblock, curr->stackblock_size) != 0)
+		abort ();
+
+	      /* Maybe we have freed enough.  */
+	      if (stack_cache_actsize <= stack_cache_maxsize)
+		break;
+	    }
+	}
+    }
+}
+
+
+static int
+internal_function
+change_stack_perm (struct pthread *pd
+#ifdef NEED_SEPARATE_REGISTER_STACK
+		   , size_t pagemask
+#endif
+		   )
+{
+#ifdef NEED_SEPARATE_REGISTER_STACK
+  void *stack = (pd->stackblock
+		 + (((((pd->stackblock_size - pd->guardsize) / 2)
+		      & pagemask) + pd->guardsize) & pagemask));
+  size_t len = pd->stackblock + pd->stackblock_size - stack;
+#else
+  void *stack = pd->stackblock + pd->guardsize;
+  size_t len = pd->stackblock_size - pd->guardsize;
+#endif
+  if (mprotect (stack, len, PROT_READ | PROT_WRITE | PROT_EXEC) != 0)
+    return errno;
+
+  return 0;
+}
+
+
+static int
+allocate_stack (const struct pthread_attr *attr, struct pthread **pdp,
+		ALLOCATE_STACK_PARMS)
+{
+  struct pthread *pd;
+  size_t size;
+  size_t pagesize_m1 = __getpagesize () - 1;
+  void *stacktop;
+
+  assert (attr != NULL);
+  assert (powerof2 (pagesize_m1 + 1));
+  assert (TCB_ALIGNMENT >= STACK_ALIGN);
+
+  /* Get the stack size from the attribute if it is set.  Otherwise we
+     use the default we determined at start time.  */
+  size = attr->stacksize ?: __default_stacksize;
+
+  /* Get memory for the stack.  */
+  if (__builtin_expect (attr->flags & ATTR_FLAG_STACKADDR, 0))
+    {
+      uintptr_t adj;
+
+      /* If the user also specified the size of the stack make sure it
+	 is large enough.  */
+      if (attr->stacksize != 0
+	  && attr->stacksize < (__static_tls_size + MINIMAL_REST_STACK))
+	return EINVAL;
+
+      /* Adjust stack size for alignment of the TLS block.  */
+#if TLS_TCB_AT_TP
+      adj = ((uintptr_t) attr->stackaddr - TLS_TCB_SIZE)
+	    & __static_tls_align_m1;
+      assert (size > adj + TLS_TCB_SIZE);
+#elif TLS_DTV_AT_TP
+      adj = ((uintptr_t) attr->stackaddr - __static_tls_size)
+	    & __static_tls_align_m1;
+      assert (size > adj);
+#endif
+
+      /* The user provided some memory.  Let's hope it matches the
+	 size...  We do not allocate guard pages if the user provided
+	 the stack.  It is the user's responsibility to do this if it
+	 is wanted.  */
+#if TLS_TCB_AT_TP
+      pd = (struct pthread *) ((uintptr_t) attr->stackaddr
+			       - TLS_TCB_SIZE - adj);
+#elif TLS_DTV_AT_TP
+      pd = (struct pthread *) (((uintptr_t) attr->stackaddr
+			        - __static_tls_size - adj)
+			       - TLS_PRE_TCB_SIZE);
+#endif
+
+      /* The user provided stack memory needs to be cleared.  */
+      memset (pd, '\0', sizeof (struct pthread));
+
+      /* The first TSD block is included in the TCB.  */
+      pd->specific[0] = pd->specific_1stblock;
+
+      /* Remember the stack-related values.  */
+      pd->stackblock = (char *) attr->stackaddr - size;
+      pd->stackblock_size = size;
+
+      /* This is a user-provided stack.  It will not be queued in the
+	 stack cache nor will the memory (except the TLS memory) be freed.  */
+      pd->user_stack = true;
+
+      /* This is at least the second thread.  */
+      pd->header.multiple_threads = 1;
+#ifndef TLS_MULTIPLE_THREADS_IN_TCB
+      __pthread_multiple_threads = *__libc_multiple_threads_ptr = 1;
+#endif
+
+#ifdef NEED_DL_SYSINFO
+      /* Copy the sysinfo value from the parent.  */
+      THREAD_SYSINFO(pd) = THREAD_SELF_SYSINFO;
+#endif
+
+      /* The process ID is also the same as that of the caller.  */
+      pd->pid = THREAD_GETMEM (THREAD_SELF, pid);
+
+      /* Allocate the DTV for this thread.  */
+      if (_dl_allocate_tls (TLS_TPADJ (pd)) == NULL)
+	{
+	  /* Something went wrong.  */
+	  assert (errno == ENOMEM);
+	  return EAGAIN;
+	}
+
+
+      /* Prepare to modify global data.  */
+      lll_lock (stack_cache_lock);
+
+      /* And add to the list of stacks in use.  */
+      list_add (&pd->list, &__stack_user);
+
+      lll_unlock (stack_cache_lock);
+    }
+  else
+    {
+      /* Allocate some anonymous memory.  If possible use the cache.  */
+      size_t guardsize;
+      size_t reqsize;
+      void *mem;
+      const int prot = (PROT_READ | PROT_WRITE
+			| ((GL(dl_stack_flags) & PF_X) ? PROT_EXEC : 0));
+
+#if COLORING_INCREMENT != 0
+      /* Add one more page for stack coloring.  Don't do it for stacks
+	 with 16 times pagesize or larger.  This might just cause
+	 unnecessary misalignment.  */
+      if (size <= 16 * pagesize_m1)
+	size += pagesize_m1 + 1;
+#endif
+
+      /* Adjust the stack size for alignment.  */
+      size &= ~__static_tls_align_m1;
+      assert (size != 0);
+
+      /* Make sure the size of the stack is enough for the guard and
+	 eventually the thread descriptor.  */
+      guardsize = (attr->guardsize + pagesize_m1) & ~pagesize_m1;
+      if (__builtin_expect (size < (guardsize + __static_tls_size
+				    + MINIMAL_REST_STACK + pagesize_m1 + 1),
+			    0))
+	/* The stack is too small (or the guard too large).  */
+	return EINVAL;
+
+      /* Try to get a stack from the cache.  */
+      reqsize = size;
+      pd = get_cached_stack (&size, &mem);
+      if (pd == NULL)
+	{
+	  /* To avoid aliasing effects on a larger scale than pages we
+	     adjust the allocated stack size if necessary.  This way
+	     allocations directly following each other will not have
+	     aliasing problems.  */
+#if MULTI_PAGE_ALIASING != 0
+	  if ((size % MULTI_PAGE_ALIASING) == 0)
+	    size += pagesize_m1 + 1;
+#endif
+
+	  mem = mmap (NULL, size, prot,
+		      MAP_PRIVATE | MAP_ANONYMOUS | ARCH_MAP_FLAGS, -1, 0);
+
+	  if (__builtin_expect (mem == MAP_FAILED, 0))
+	    {
+#ifdef ARCH_RETRY_MMAP
+	      mem = ARCH_RETRY_MMAP (size);
+	      if (__builtin_expect (mem == MAP_FAILED, 0))
+#endif
+		return errno;
+	    }
+
+	  /* SIZE is guaranteed to be greater than zero.
+	     So we can never get a null pointer back from mmap.  */
+	  assert (mem != NULL);
+
+#if COLORING_INCREMENT != 0
+	  /* Atomically increment NCREATED.  */
+	  unsigned int ncreated = atomic_increment_val (&nptl_ncreated);
+
+	  /* We chose the offset for coloring by incrementing it for
+	     every new thread by a fixed amount.  The offset used
+	     module the page size.  Even if coloring would be better
+	     relative to higher alignment values it makes no sense to
+	     do it since the mmap() interface does not allow us to
+	     specify any alignment for the returned memory block.  */
+	  size_t coloring = (ncreated * COLORING_INCREMENT) & pagesize_m1;
+
+	  /* Make sure the coloring offsets does not disturb the alignment
+	     of the TCB and static TLS block.  */
+	  if (__builtin_expect ((coloring & __static_tls_align_m1) != 0, 0))
+	    coloring = (((coloring + __static_tls_align_m1)
+			 & ~(__static_tls_align_m1))
+			& ~pagesize_m1);
+#else
+	  /* Unless specified we do not make any adjustments.  */
+# define coloring 0
+#endif
+
+	  /* Place the thread descriptor at the end of the stack.  */
+#if TLS_TCB_AT_TP
+	  pd = (struct pthread *) ((char *) mem + size - coloring) - 1;
+#elif TLS_DTV_AT_TP
+	  pd = (struct pthread *) ((((uintptr_t) mem + size - coloring
+				    - __static_tls_size)
+				    & ~__static_tls_align_m1)
+				   - TLS_PRE_TCB_SIZE);
+#endif
+
+	  /* Remember the stack-related values.  */
+	  pd->stackblock = mem;
+	  pd->stackblock_size = size;
+
+	  /* We allocated the first block thread-specific data array.
+	     This address will not change for the lifetime of this
+	     descriptor.  */
+	  pd->specific[0] = pd->specific_1stblock;
+
+	  /* This is at least the second thread.  */
+	  pd->header.multiple_threads = 1;
+#ifndef TLS_MULTIPLE_THREADS_IN_TCB
+	  __pthread_multiple_threads = *__libc_multiple_threads_ptr = 1;
+#endif
+
+#ifdef NEED_DL_SYSINFO
+	  /* Copy the sysinfo value from the parent.  */
+	  THREAD_SYSINFO(pd) = THREAD_SELF_SYSINFO;
+#endif
+
+	  /* The process ID is also the same as that of the caller.  */
+	  pd->pid = THREAD_GETMEM (THREAD_SELF, pid);
+
+	  /* Allocate the DTV for this thread.  */
+	  if (_dl_allocate_tls (TLS_TPADJ (pd)) == NULL)
+	    {
+	      /* Something went wrong.  */
+	      assert (errno == ENOMEM);
+
+	      /* Free the stack memory we just allocated.  */
+	      (void) munmap (mem, size);
+
+	      return EAGAIN;
+	    }
+
+
+	  /* Prepare to modify global data.  */
+	  lll_lock (stack_cache_lock);
+
+	  /* And add to the list of stacks in use.  */
+	  list_add (&pd->list, &stack_used);
+
+	  lll_unlock (stack_cache_lock);
+
+
+	  /* There might have been a race.  Another thread might have
+	     caused the stacks to get exec permission while this new
+	     stack was prepared.  Detect if this was possible and
+	     change the permission if necessary.  */
+	  if (__builtin_expect ((GL(dl_stack_flags) & PF_X) != 0
+				&& (prot & PROT_EXEC) == 0, 0))
+	    {
+	      int err = change_stack_perm (pd
+#ifdef NEED_SEPARATE_REGISTER_STACK
+					   , ~pagesize_m1
+#endif
+					   );
+	      if (err != 0)
+		{
+		  /* Free the stack memory we just allocated.  */
+		  (void) munmap (mem, size);
+
+		  return err;
+		}
+	    }
+
+
+	  /* Note that all of the stack and the thread descriptor is
+	     zeroed.  This means we do not have to initialize fields
+	     with initial value zero.  This is specifically true for
+	     the 'tid' field which is always set back to zero once the
+	     stack is not used anymore and for the 'guardsize' field
+	     which will be read next.  */
+	}
+
+      /* Create or resize the guard area if necessary.  */
+      if (__builtin_expect (guardsize > pd->guardsize, 0))
+	{
+#ifdef NEED_SEPARATE_REGISTER_STACK
+	  char *guard = mem + (((size - guardsize) / 2) & ~pagesize_m1);
+#else
+	  char *guard = mem;
+#endif
+	  if (mprotect (guard, guardsize, PROT_NONE) != 0)
+	    {
+	      int err;
+	    mprot_error:
+	      err = errno;
+
+	      lll_lock (stack_cache_lock);
+
+	      /* Remove the thread from the list.  */
+	      list_del (&pd->list);
+
+	      lll_unlock (stack_cache_lock);
+
+	      /* Get rid of the TLS block we allocated.  */
+	      _dl_deallocate_tls (TLS_TPADJ (pd), false);
+
+	      /* Free the stack memory regardless of whether the size
+		 of the cache is over the limit or not.  If this piece
+		 of memory caused problems we better do not use it
+		 anymore.  Uh, and we ignore possible errors.  There
+		 is nothing we could do.  */
+	      (void) munmap (mem, size);
+
+	      return err;
+	    }
+
+	  pd->guardsize = guardsize;
+	}
+      else if (__builtin_expect (pd->guardsize - guardsize > size - reqsize,
+				 0))
+	{
+	  /* The old guard area is too large.  */
+
+#ifdef NEED_SEPARATE_REGISTER_STACK
+	  char *guard = mem + (((size - guardsize) / 2) & ~pagesize_m1);
+	  char *oldguard = mem + (((size - pd->guardsize) / 2) & ~pagesize_m1);
+
+	  if (oldguard < guard
+	      && mprotect (oldguard, guard - oldguard, prot) != 0)
+	    goto mprot_error;
+
+	  if (mprotect (guard + guardsize,
+			oldguard + pd->guardsize - guard - guardsize,
+			prot) != 0)
+	    goto mprot_error;
+#else
+	  if (mprotect ((char *) mem + guardsize, pd->guardsize - guardsize,
+			prot) != 0)
+	    goto mprot_error;
+#endif
+
+	  pd->guardsize = guardsize;
+	}
+      /* The pthread_getattr_np() calls need to get passed the size
+	 requested in the attribute, regardless of how large the
+	 actually used guardsize is.  */
+      pd->reported_guardsize = guardsize;
+    }
+
+  /* Initialize the lock.  We have to do this unconditionally since the
+     stillborn thread could be canceled while the lock is taken.  */
+  pd->lock = LLL_LOCK_INITIALIZER;
+
+  /* We place the thread descriptor at the end of the stack.  */
+  *pdp = pd;
+
+#if TLS_TCB_AT_TP
+  /* The stack begins before the TCB and the static TLS block.  */
+  stacktop = ((char *) (pd + 1) - __static_tls_size);
+#elif TLS_DTV_AT_TP
+  stacktop = (char *) (pd - 1);
+#endif
+
+#ifdef NEED_SEPARATE_REGISTER_STACK
+  *stack = pd->stackblock;
+  *stacksize = stacktop - *stack;
+#else
+  *stack = stacktop;
+#endif
+
+  return 0;
+}
+
+
+void
+internal_function
+__deallocate_stack (struct pthread *pd)
+{
+  lll_lock (stack_cache_lock);
+
+  /* Remove the thread from the list of threads with user defined
+     stacks.  */
+  list_del (&pd->list);
+
+  /* Not much to do.  Just free the mmap()ed memory.  Note that we do
+     not reset the 'used' flag in the 'tid' field.  This is done by
+     the kernel.  If no thread has been created yet this field is
+     still zero.  */
+  if (__builtin_expect (! pd->user_stack, 1))
+    (void) queue_stack (pd);
+  else
+    /* Free the memory associated with the ELF TLS.  */
+    _dl_deallocate_tls (TLS_TPADJ (pd), false);
+
+  lll_unlock (stack_cache_lock);
+}
+
+
+int
+internal_function
+__make_stacks_executable (void **stack_endp)
+{
+  /* First the main thread's stack.  */
+  int err = _dl_make_stack_executable (stack_endp);
+  if (err != 0)
+    return err;
+
+#ifdef NEED_SEPARATE_REGISTER_STACK
+  const size_t pagemask = ~(__getpagesize () - 1);
+#endif
+
+  lll_lock (stack_cache_lock);
+
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    {
+      err = change_stack_perm (list_entry (runp, struct pthread, list)
+#ifdef NEED_SEPARATE_REGISTER_STACK
+			       , pagemask
+#endif
+			       );
+      if (err != 0)
+	break;
+    }
+
+  /* Also change the permission for the currently unused stacks.  This
+     might be wasted time but better spend it here than adding a check
+     in the fast path.  */
+  if (err == 0)
+    list_for_each (runp, &stack_cache)
+      {
+	err = change_stack_perm (list_entry (runp, struct pthread, list)
+#ifdef NEED_SEPARATE_REGISTER_STACK
+				 , pagemask
+#endif
+				 );
+	if (err != 0)
+	  break;
+      }
+
+  lll_unlock (stack_cache_lock);
+
+  return err;
+}
+
+
+/* In case of a fork() call the memory allocation in the child will be
+   the same but only one thread is running.  All stacks except that of
+   the one running thread are not used anymore.  We have to recycle
+   them.  */
+void
+__reclaim_stacks (void)
+{
+  struct pthread *self = (struct pthread *) THREAD_SELF;
+
+  /* No locking necessary.  The caller is the only stack in use.  */
+
+  /* Mark all stacks except the still running one as free.  */
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    {
+      struct pthread *curp;
+
+      curp = list_entry (runp, struct pthread, list);
+      if (curp != self)
+	{
+	  /* This marks the stack as free.  */
+	  curp->tid = 0;
+
+	  /* The PID field must be initialized for the new process.  */
+	  curp->pid = self->pid;
+
+	  /* Account for the size of the stack.  */
+	  stack_cache_actsize += curp->stackblock_size;
+	}
+    }
+
+  /* Add the stack of all running threads to the cache.  */
+  list_splice (&stack_used, &stack_cache);
+
+  /* Remove the entry for the current thread to from the cache list
+     and add it to the list of running threads.  Which of the two
+     lists is decided by the user_stack flag.  */
+  list_del (&self->list);
+
+  /* Re-initialize the lists for all the threads.  */
+  INIT_LIST_HEAD (&stack_used);
+  INIT_LIST_HEAD (&__stack_user);
+
+  if (__builtin_expect (THREAD_GETMEM (self, user_stack), 0))
+    list_add (&self->list, &__stack_user);
+  else
+    list_add (&self->list, &stack_used);
+
+  /* There is one thread running.  */
+  __nptl_nthreads = 1;
+
+  /* Initialize the lock.  */
+  stack_cache_lock = LLL_LOCK_INITIALIZER;
+}
+
+
+#if HP_TIMING_AVAIL
+# undef __find_thread_by_id
+/* Find a thread given the thread ID.  */
+attribute_hidden
+struct pthread *
+__find_thread_by_id (pid_t tid)
+{
+  struct pthread *result = NULL;
+
+  lll_lock (stack_cache_lock);
+
+  /* Iterate over the list with system-allocated threads first.  */
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    {
+      struct pthread *curp;
+
+      curp = list_entry (runp, struct pthread, list);
+
+      if (curp->tid == tid)
+	{
+	  result = curp;
+	  goto out;
+	}
+    }
+
+  /* Now the list with threads using user-allocated stacks.  */
+  list_for_each (runp, &__stack_user)
+    {
+      struct pthread *curp;
+
+      curp = list_entry (runp, struct pthread, list);
+
+      if (curp->tid == tid)
+	{
+	  result = curp;
+	  goto out;
+	}
+    }
+
+ out:
+  lll_unlock (stack_cache_lock);
+
+  return result;
+}
+#endif
+
+int
+attribute_hidden
+__nptl_setxid (struct xid_command *cmdp)
+{
+  int result;
+  lll_lock (stack_cache_lock);
+
+  __xidcmd = cmdp;
+  cmdp->cntr = 0;
+
+  INTERNAL_SYSCALL_DECL (err);
+
+  struct pthread *self = THREAD_SELF;
+
+  /* Iterate over the list with system-allocated threads first.  */
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    {
+      struct pthread *t = list_entry (runp, struct pthread, list);
+      if (t != self)
+	{
+	  int val;
+#if __ASSUME_TGKILL
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid),
+				  t->tid, SIGSETXID);
+#else
+# ifdef __NR_tgkill
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid),
+				  t->tid, SIGSETXID);
+	  if (INTERNAL_SYSCALL_ERROR_P (val, err)
+	      && INTERNAL_SYSCALL_ERRNO (val, err) == ENOSYS)
+# endif
+	    val = INTERNAL_SYSCALL (tkill, err, 2, t->tid, SIGSETXID);
+#endif
+
+	  if (!INTERNAL_SYSCALL_ERROR_P (val, err))
+	    atomic_increment (&cmdp->cntr);
+	}
+    }
+
+  /* Now the list with threads using user-allocated stacks.  */
+  list_for_each (runp, &__stack_user)
+    {
+      struct pthread *t = list_entry (runp, struct pthread, list);
+      if (t != self)
+	{
+	  int val;
+#if __ASSUME_TGKILL
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid),
+				  t->tid, SIGSETXID);
+#else
+# ifdef __NR_tgkill
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid),
+				  t->tid, SIGSETXID);
+	  if (INTERNAL_SYSCALL_ERROR_P (val, err)
+	      && INTERNAL_SYSCALL_ERRNO (val, err) == ENOSYS)
+# endif
+	    val = INTERNAL_SYSCALL (tkill, err, 2, t->tid, SIGSETXID);
+#endif
+
+	  if (!INTERNAL_SYSCALL_ERROR_P (val, err))
+	    atomic_increment (&cmdp->cntr);
+	}
+    }
+
+  int cur = cmdp->cntr;
+  while (cur != 0)
+    {
+      lll_futex_wait (&cmdp->cntr, cur);
+      cur = cmdp->cntr;
+    }
+
+  /* This must be last, otherwise the current thread might not have
+     permissions to send SIGSETXID syscall to the other threads.  */
+  result = INTERNAL_SYSCALL_NCS (cmdp->syscall_no, err, 3,
+				 cmdp->id[0], cmdp->id[1], cmdp->id[2]);
+  if (INTERNAL_SYSCALL_ERROR_P (result, err))
+    {
+      __set_errno (INTERNAL_SYSCALL_ERRNO (result, err));
+      result = -1;
+    }
+
+  lll_unlock (stack_cache_lock);
+  return result;
+}
+
+static inline void __attribute__((always_inline))
+init_one_static_tls (struct pthread *curp, struct link_map *map)
+{
+  dtv_t *dtv = GET_DTV (TLS_TPADJ (curp));
+# if TLS_TCB_AT_TP
+  void *dest = (char *) curp - map->l_tls_offset;
+# elif TLS_DTV_AT_TP
+  void *dest = (char *) curp + map->l_tls_offset + TLS_PRE_TCB_SIZE;
+# else
+#  error "Either TLS_TCB_AT_TP or TLS_DTV_AT_TP must be defined"
+# endif
+
+  /* Fill in the DTV slot so that a later LD/GD access will find it.  */
+  dtv[map->l_tls_modid].pointer.val = dest;
+  dtv[map->l_tls_modid].pointer.is_static = true;
+
+  /* Initialize the memory.  */
+  memset (__mempcpy (dest, map->l_tls_initimage, map->l_tls_initimage_size),
+	  '\0', map->l_tls_blocksize - map->l_tls_initimage_size);
+}
+
+void
+attribute_hidden
+__pthread_init_static_tls (struct link_map *map)
+{
+  lll_lock (stack_cache_lock);
+
+  /* Iterate over the list with system-allocated threads first.  */
+  list_t *runp;
+  list_for_each (runp, &stack_used)
+    init_one_static_tls (list_entry (runp, struct pthread, list), map);
+
+  /* Now the list with threads using user-allocated stacks.  */
+  list_for_each (runp, &__stack_user)
+    init_one_static_tls (list_entry (runp, struct pthread, list), map);
+
+  lll_unlock (stack_cache_lock);
+}
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/createthread.c libc/nptl/sysdeps/unix/sysv/linux/createthread.c
--- libc/nptl/sysdeps/unix/sysv/linux/createthread.c	2003-03-10 07:29:56.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/createthread.c	2005-01-23 18:39:29.000000000 +0100
@@ -1,6 +1,6 @@
-/* Copyright (C) 2003 Free Software Foundation, Inc.
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
    This file is part of the GNU C Library.
-   Contributed by Martin Schwidefsky <schwidefsky@de.ibm.com>.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
 
    The GNU C Library is free software; you can redistribute it and/or
    modify it under the terms of the GNU Lesser General Public
@@ -9,7 +9,7 @@
 
    The GNU C Library is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
-   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	 See the GNU
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
    Lesser General Public License for more details.
 
    You should have received a copy of the GNU Lesser General Public
@@ -17,8 +17,239 @@
    Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
    02111-1307 USA.  */
 
-/* Value passed to 'clone' for initialization of the thread register.  */
-#define TLS_VALUE pd
+#include <sched.h>
+#include <setjmp.h>
+#include <signal.h>
+#include <stdlib.h>
+#include <atomic.h>
+#include <ldsodefs.h>
+#include <tls.h>
 
-/* Get the real implementation.	 */
-#include <nptl/sysdeps/pthread/createthread.c>
+#include "kernel-features.h"
+
+
+#define CLONE_SIGNAL    	(CLONE_SIGHAND | CLONE_THREAD)
+
+/* Unless otherwise specified, the thread "register" is going to be
+   initialized with a pointer to the TCB.  */
+#ifndef TLS_VALUE
+# define TLS_VALUE pd
+#endif
+
+#ifndef ARCH_CLONE
+# define ARCH_CLONE __clone
+#endif
+
+
+#ifndef TLS_MULTIPLE_THREADS_IN_TCB
+/* Pointer to the corresponding variable in libc.  */
+int *__libc_multiple_threads_ptr attribute_hidden;
+#endif
+
+
+static int
+do_clone (struct pthread *pd, const struct pthread_attr *attr,
+	  int clone_flags, int (*fct) (void *), STACK_VARIABLES_PARMS,
+	  int stopped)
+{
+#ifdef PREPARE_CREATE
+  PREPARE_CREATE;
+#endif
+
+  if (stopped)
+    /* We Make sure the thread does not run far by forcing it to get a
+       lock.  We lock it here too so that the new thread cannot continue
+       until we tell it to.  */
+    lll_lock (pd->lock);
+
+  /* One more thread.  We cannot have the thread do this itself, since it
+     might exist but not have been scheduled yet by the time we've returned
+     and need to check the value to behave correctly.  We must do it before
+     creating the thread, in case it does get scheduled first and then
+     might mistakenly think it was the only thread.  In the failure case,
+     we momentarily store a false value; this doesn't matter because there
+     is no kosher thing a signal handler interrupting us right here can do
+     that cares whether the thread count is correct.  */
+  atomic_increment (&__nptl_nthreads);
+
+  if (ARCH_CLONE (fct, STACK_VARIABLES_ARGS, clone_flags,
+		  pd, &pd->tid, TLS_VALUE, &pd->tid) == -1)
+    {
+      atomic_decrement (&__nptl_nthreads); /* Oops, we lied for a second.  */
+
+      /* Failed.  If the thread is detached, remove the TCB here since
+	 the caller cannot do this.  The caller remembered the thread
+	 as detached and cannot reverify that it is not since it must
+	 not access the thread descriptor again.  */
+      if (IS_DETACHED (pd))
+	__deallocate_stack (pd);
+
+      return errno;
+    }
+
+  /* Now we have the possibility to set scheduling parameters etc.  */
+  if (__builtin_expect (stopped != 0, 0))
+    {
+      INTERNAL_SYSCALL_DECL (err);
+      int res = 0;
+
+      /* Set the affinity mask if necessary.  */
+      if (attr->cpuset != NULL)
+	{
+	  res = INTERNAL_SYSCALL (sched_setaffinity, err, 3, pd->tid,
+				  sizeof (cpu_set_t), attr->cpuset);
+
+	  if (__builtin_expect (INTERNAL_SYSCALL_ERROR_P (res, err), 0))
+	    {
+	      /* The operation failed.  We have to kill the thread.  First
+		 send it the cancellation signal.  */
+	      INTERNAL_SYSCALL_DECL (err2);
+	    err_out:
+#if __ASSUME_TGKILL
+	      (void) INTERNAL_SYSCALL (tgkill, err2, 3,
+				       THREAD_GETMEM (THREAD_SELF, pid),
+				       pd->tid, SIGCANCEL);
+#else
+	      (void) INTERNAL_SYSCALL (tkill, err2, 2, pd->tid, SIGCANCEL);
+#endif
+
+	      return (INTERNAL_SYSCALL_ERROR_P (res, err)
+		      ? INTERNAL_SYSCALL_ERRNO (res, err)
+		      : 0);
+	    }
+	}
+
+      /* Set the scheduling parameters.  */
+      if ((attr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0)
+	{
+	  res = INTERNAL_SYSCALL (sched_setscheduler, err, 3, pd->tid,
+				  pd->schedpolicy, &pd->schedparam);
+
+	  if (__builtin_expect (INTERNAL_SYSCALL_ERROR_P (res, err), 0))
+	    goto err_out;
+	}
+    }
+
+  /* We now have for sure more than one thread.  The main thread might
+     not yet have the flag set.  No need to set the global variable
+     again if this is what we use.  */
+  THREAD_SETMEM (THREAD_SELF, header.multiple_threads, 1);
+
+  return 0;
+}
+
+
+static int
+create_thread (struct pthread *pd, const struct pthread_attr *attr,
+	       STACK_VARIABLES_PARMS)
+{
+#ifdef TLS_TCB_AT_TP
+  assert (pd->header.tcb != NULL);
+#endif
+
+  /* We rely heavily on various flags the CLONE function understands:
+
+     CLONE_VM, CLONE_FS, CLONE_FILES
+	These flags select semantics with shared address space and
+	file descriptors according to what POSIX requires.
+
+     CLONE_SIGNAL
+	This flag selects the POSIX signal semantics.
+
+     CLONE_SETTLS
+	The sixth parameter to CLONE determines the TLS area for the
+	new thread.
+
+     CLONE_PARENT_SETTID
+	The kernels writes the thread ID of the newly created thread
+	into the location pointed to by the fifth parameters to CLONE.
+
+	Note that it would be semantically equivalent to use
+	CLONE_CHILD_SETTID but it is be more expensive in the kernel.
+
+     CLONE_CHILD_CLEARTID
+	The kernels clears the thread ID of a thread that has called
+	sys_exit() in the location pointed to by the seventh parameter
+	to CLONE.
+
+     CLONE_DETACHED
+	No signal is generated if the thread exists and it is
+	automatically reaped.
+
+     The termination signal is chosen to be zero which means no signal
+     is sent.  */
+  int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGNAL
+		     | CLONE_SETTLS | CLONE_PARENT_SETTID
+		     | CLONE_CHILD_CLEARTID | CLONE_SYSVSEM
+#if __ASSUME_NO_CLONE_DETACHED == 0
+		     | CLONE_DETACHED
+#endif
+		     | 0);
+
+  if (__builtin_expect (THREAD_GETMEM (THREAD_SELF, report_events), 0))
+    {
+      /* The parent thread is supposed to report events.  Check whether
+	 the TD_CREATE event is needed, too.  */
+      const int _idx = __td_eventword (TD_CREATE);
+      const uint32_t _mask = __td_eventmask (TD_CREATE);
+
+      if ((_mask & (__nptl_threads_events.event_bits[_idx]
+		    | pd->eventbuf.eventmask.event_bits[_idx])) != 0)
+	{
+	  /* We always must have the thread start stopped.  */
+	  pd->stopped_start = true;
+
+	  /* Create the thread.  We always create the thread stopped
+	     so that it does not get far before we tell the debugger.  */
+	  int res = do_clone (pd, attr, clone_flags, start_thread,
+			      STACK_VARIABLES_ARGS, 1);
+	  if (res == 0)
+	    {
+	      /* Now fill in the information about the new thread in
+		 the newly created thread's data structure.  We cannot let
+		 the new thread do this since we don't know whether it was
+		 already scheduled when we send the event.  */
+	      pd->eventbuf.eventnum = TD_CREATE;
+	      pd->eventbuf.eventdata = pd;
+
+	      /* Enqueue the descriptor.  */
+	      do
+		pd->nextevent = __nptl_last_event;
+	      while (atomic_compare_and_exchange_bool_acq (&__nptl_last_event,
+							   pd, pd->nextevent)
+		     != 0);
+
+	      /* Now call the function which signals the event.  */
+	      __nptl_create_event ();
+
+	      /* And finally restart the new thread.  */
+	      lll_unlock (pd->lock);
+	    }
+
+	  return res;
+	}
+    }
+
+#ifdef NEED_DL_SYSINFO
+  assert (THREAD_SELF_SYSINFO == THREAD_SYSINFO (pd));
+#endif
+
+  /* Determine whether the newly created threads has to be started
+     stopped since we have to set the scheduling parameters or set the
+     affinity.  */
+  bool stopped = false;
+  if (attr != NULL && (attr->cpuset != NULL
+		       || (attr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0))
+    stopped = true;
+  pd->stopped_start = stopped;
+
+  /* Actually create the thread.  */
+  int res = do_clone (pd, attr, clone_flags, start_thread,
+		      STACK_VARIABLES_ARGS, stopped);
+
+  if (res == 0 && stopped)
+    /* And finally restart the new thread.  */
+    lll_unlock (pd->lock);
+
+  return res;
+}
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/init.c libc/nptl/sysdeps/unix/sysv/linux/init.c
--- libc/nptl/sysdeps/unix/sysv/linux/init.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/init.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,350 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <assert.h>
+#include <limits.h>
+#include <signal.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <sys/param.h>
+#include <sys/resource.h>
+#include <pthreadP.h>
+#include <atomic.h>
+#include <ldsodefs.h>
+#include <tls.h>
+#include <fork.h>
+#include <version.h>
+#include <shlib-compat.h>
+#include <smp.h>
+#include <lowlevellock.h>
+
+
+#ifndef __NR_set_tid_address
+/* XXX For the time being...  Once we can rely on the kernel headers
+   having the definition remove these lines.  */
+#if defined __s390__
+# define __NR_set_tid_address	252
+#elif defined __ia64__
+# define __NR_set_tid_address	1233
+#elif defined __i386__
+# define __NR_set_tid_address	258
+#elif defined __x86_64__
+# define __NR_set_tid_address	218
+#elif defined __powerpc__
+# define __NR_set_tid_address	232
+#elif defined __sparc__
+# define __NR_set_tid_address	166
+#else
+# error "define __NR_set_tid_address"
+#endif
+#endif
+
+
+/* Default stack size.  */
+size_t __default_stacksize attribute_hidden;
+
+/* Size and alignment of static TLS block.  */
+size_t __static_tls_size;
+size_t __static_tls_align_m1;
+
+/* Flag whether the machine is SMP or not.  */
+int __is_smp attribute_hidden;
+
+/* Version of the library, used in libthread_db to detect mismatches.  */
+static const char nptl_version[] __attribute_used__ = VERSION;
+
+
+#if defined USE_TLS && !defined SHARED
+extern void __libc_setup_tls (size_t tcbsize, size_t tcbalign);
+#endif
+
+
+#ifdef SHARED
+static const struct pthread_functions pthread_functions =
+  {
+    .ptr_pthread_attr_destroy = __pthread_attr_destroy,
+# if SHLIB_COMPAT(libpthread, GLIBC_2_0, GLIBC_2_1)
+    .ptr___pthread_attr_init_2_0 = __pthread_attr_init_2_0,
+# endif
+    .ptr___pthread_attr_init_2_1 = __pthread_attr_init_2_1,
+    .ptr_pthread_attr_getdetachstate = __pthread_attr_getdetachstate,
+    .ptr_pthread_attr_setdetachstate = __pthread_attr_setdetachstate,
+    .ptr_pthread_attr_getinheritsched = __pthread_attr_getinheritsched,
+    .ptr_pthread_attr_setinheritsched = __pthread_attr_setinheritsched,
+    .ptr_pthread_attr_getschedparam = __pthread_attr_getschedparam,
+    .ptr_pthread_attr_setschedparam = __pthread_attr_setschedparam,
+    .ptr_pthread_attr_getschedpolicy = __pthread_attr_getschedpolicy,
+    .ptr_pthread_attr_setschedpolicy = __pthread_attr_setschedpolicy,
+    .ptr_pthread_attr_getscope = __pthread_attr_getscope,
+    .ptr_pthread_attr_setscope = __pthread_attr_setscope,
+    .ptr_pthread_condattr_destroy = __pthread_condattr_destroy,
+    .ptr_pthread_condattr_init = __pthread_condattr_init,
+    .ptr___pthread_cond_broadcast = __pthread_cond_broadcast,
+    .ptr___pthread_cond_destroy = __pthread_cond_destroy,
+    .ptr___pthread_cond_init = __pthread_cond_init,
+    .ptr___pthread_cond_signal = __pthread_cond_signal,
+    .ptr___pthread_cond_wait = __pthread_cond_wait,
+    .ptr___pthread_cond_timedwait = __pthread_cond_timedwait,
+# if SHLIB_COMPAT(libpthread, GLIBC_2_0, GLIBC_2_3_2)
+    .ptr___pthread_cond_broadcast_2_0 = __pthread_cond_broadcast_2_0,
+    .ptr___pthread_cond_destroy_2_0 = __pthread_cond_destroy_2_0,
+    .ptr___pthread_cond_init_2_0 = __pthread_cond_init_2_0,
+    .ptr___pthread_cond_signal_2_0 = __pthread_cond_signal_2_0,
+    .ptr___pthread_cond_wait_2_0 = __pthread_cond_wait_2_0,
+    .ptr___pthread_cond_timedwait_2_0 = __pthread_cond_timedwait_2_0,
+# endif
+    .ptr_pthread_equal = __pthread_equal,
+    .ptr___pthread_exit = __pthread_exit,
+    .ptr_pthread_getschedparam = __pthread_getschedparam,
+    .ptr_pthread_setschedparam = __pthread_setschedparam,
+    .ptr_pthread_mutex_destroy = INTUSE(__pthread_mutex_destroy),
+    .ptr_pthread_mutex_init = INTUSE(__pthread_mutex_init),
+    .ptr_pthread_mutex_lock = INTUSE(__pthread_mutex_lock),
+    .ptr_pthread_mutex_unlock = INTUSE(__pthread_mutex_unlock),
+    .ptr_pthread_self = __pthread_self,
+    .ptr_pthread_setcancelstate = __pthread_setcancelstate,
+    .ptr_pthread_setcanceltype = __pthread_setcanceltype,
+    .ptr___pthread_cleanup_upto = __pthread_cleanup_upto,
+    .ptr___pthread_once = __pthread_once_internal,
+    .ptr___pthread_rwlock_rdlock = __pthread_rwlock_rdlock_internal,
+    .ptr___pthread_rwlock_wrlock = __pthread_rwlock_wrlock_internal,
+    .ptr___pthread_rwlock_unlock = __pthread_rwlock_unlock_internal,
+    .ptr___pthread_key_create = __pthread_key_create_internal,
+    .ptr___pthread_getspecific = __pthread_getspecific_internal,
+    .ptr___pthread_setspecific = __pthread_setspecific_internal,
+    .ptr__pthread_cleanup_push_defer = __pthread_cleanup_push_defer,
+    .ptr__pthread_cleanup_pop_restore = __pthread_cleanup_pop_restore,
+    .ptr_nthreads = &__nptl_nthreads,
+    .ptr___pthread_unwind = &__pthread_unwind,
+    .ptr__nptl_deallocate_tsd = __nptl_deallocate_tsd,
+    .ptr__nptl_setxid = __nptl_setxid
+  };
+# define ptr_pthread_functions &pthread_functions
+#else
+# define ptr_pthread_functions NULL
+#endif
+
+
+/* For asynchronous cancellation we use a signal.  This is the handler.  */
+static void
+sigcancel_handler (int sig, siginfo_t *si, void *ctx)
+{
+  /* Safety check.  It would be possible to call this function for
+     other signals and send a signal from another process.  This is not
+     correct and might even be a security problem.  Try to catch as
+     many incorrect invocations as possible.  */
+  if (sig != SIGCANCEL
+#ifdef __ASSUME_CORRECT_SI_PID
+      /* Kernels before 2.5.75 stored the thread ID and not the process
+	 ID in si_pid so we skip this test.  */
+      || si->si_pid != THREAD_GETMEM (THREAD_SELF, pid)
+#endif
+      || si->si_code != SI_TKILL)
+    return;
+
+  struct pthread *self = THREAD_SELF;
+
+  int oldval = THREAD_GETMEM (self, cancelhandling);
+  while (1)
+    {
+      /* We are canceled now.  When canceled by another thread this flag
+	 is already set but if the signal is directly send (internally or
+	 from another process) is has to be done here.  */
+      int newval = oldval | CANCELING_BITMASK | CANCELED_BITMASK;
+
+      if (oldval == newval || (oldval & EXITING_BITMASK) != 0)
+	/* Already canceled or exiting.  */
+	break;
+
+      int curval = THREAD_ATOMIC_CMPXCHG_VAL (self, cancelhandling, newval,
+					      oldval);
+      if (curval == oldval)
+	{
+	  /* Set the return value.  */
+	  THREAD_SETMEM (self, result, PTHREAD_CANCELED);
+
+	  /* Make sure asynchronous cancellation is still enabled.  */
+	  if ((newval & CANCELTYPE_BITMASK) != 0)
+	    /* Run the registered destructors and terminate the thread.  */
+	    __do_cancel ();
+
+	  break;
+	}
+
+      oldval = curval;
+    }
+}
+
+
+struct xid_command *__xidcmd attribute_hidden;
+
+/* For asynchronous cancellation we use a signal.  This is the handler.  */
+static void
+sighandler_setxid (int sig, siginfo_t *si, void *ctx)
+{
+  /* Safety check.  It would be possible to call this function for
+     other signals and send a signal from another process.  This is not
+     correct and might even be a security problem.  Try to catch as
+     many incorrect invocations as possible.  */
+  if (sig != SIGSETXID
+#ifdef __ASSUME_CORRECT_SI_PID
+      /* Kernels before 2.5.75 stored the thread ID and not the process
+	 ID in si_pid so we skip this test.  */
+      || si->si_pid != THREAD_GETMEM (THREAD_SELF, pid)
+#endif
+      || si->si_code != SI_TKILL)
+    return;
+
+  INTERNAL_SYSCALL_DECL (err);
+  INTERNAL_SYSCALL_NCS (__xidcmd->syscall_no, err, 3, __xidcmd->id[0],
+			__xidcmd->id[1], __xidcmd->id[2]);
+
+  if (atomic_decrement_val (&__xidcmd->cntr) == 0)
+    lll_futex_wake (&__xidcmd->cntr, 1);
+}
+
+
+/* When using __thread for this, we do it in libc so as not
+   to give libpthread its own TLS segment just for this.  */
+extern void **__libc_dl_error_tsd (void) __attribute__ ((const));
+
+
+void
+__pthread_initialize_minimal_internal (void)
+{
+#ifndef SHARED
+  /* Unlike in the dynamically linked case the dynamic linker has not
+     taken care of initializing the TLS data structures.  */
+  __libc_setup_tls (TLS_TCB_SIZE, TLS_TCB_ALIGN);
+
+  /* We must prevent gcc from being clever and move any of the
+     following code ahead of the __libc_setup_tls call.  This function
+     will initialize the thread register which is subsequently
+     used.  */
+  __asm __volatile ("");
+#endif
+
+  /* Minimal initialization of the thread descriptor.  */
+  struct pthread *pd = THREAD_SELF;
+  INTERNAL_SYSCALL_DECL (err);
+  pd->pid = pd->tid = INTERNAL_SYSCALL (set_tid_address, err, 1, &pd->tid);
+  THREAD_SETMEM (pd, specific[0], &pd->specific_1stblock[0]);
+  THREAD_SETMEM (pd, user_stack, true);
+  if (LLL_LOCK_INITIALIZER != 0)
+    THREAD_SETMEM (pd, lock, LLL_LOCK_INITIALIZER);
+#if HP_TIMING_AVAIL
+  THREAD_SETMEM (pd, cpuclock_offset, GL(dl_cpuclock_offset));
+#endif
+
+  /* Set initial thread's stack block from 0 up to __libc_stack_end.
+     It will be bigger than it actually is, but for unwind.c/pt-longjmp.c
+     purposes this is good enough.  */
+  THREAD_SETMEM (pd, stackblock_size, (size_t) __libc_stack_end);
+
+  /* Initialize the list of all running threads with the main thread.  */
+  INIT_LIST_HEAD (&__stack_user);
+  list_add (&pd->list, &__stack_user);
+
+
+  /* Install the cancellation signal handler.  If for some reason we
+     cannot install the handler we do not abort.  Maybe we should, but
+     it is only asynchronous cancellation which is affected.  */
+  struct sigaction sa;
+  sa.sa_sigaction = sigcancel_handler;
+  sa.sa_flags = SA_SIGINFO;
+  sigemptyset (&sa.sa_mask);
+
+  (void) __libc_sigaction (SIGCANCEL, &sa, NULL);
+
+  /* Install the handle to change the threads' uid/gid.  */
+  sa.sa_sigaction = sighandler_setxid;
+  sa.sa_flags = SA_SIGINFO | SA_RESTART;
+
+  (void) __libc_sigaction (SIGSETXID, &sa, NULL);
+
+  /* The parent process might have left the signal blocked.  Just in
+     case, unblock it.  We reuse the signal mask in the sigaction
+     structure.  It is already cleared.  */
+  __sigaddset (&sa.sa_mask, SIGCANCEL);
+  (void) INTERNAL_SYSCALL (rt_sigprocmask, err, 4, SIG_UNBLOCK, &sa.sa_mask,
+			   NULL, _NSIG / 8);
+
+
+  /* Determine the default allowed stack size.  This is the size used
+     in case the user does not specify one.  */
+  struct rlimit limit;
+  if (getrlimit (RLIMIT_STACK, &limit) != 0
+      || limit.rlim_cur == RLIM_INFINITY)
+    /* The system limit is not usable.  Use an architecture-specific
+       default.  */
+    __default_stacksize = ARCH_STACK_DEFAULT_SIZE;
+  else if (limit.rlim_cur < PTHREAD_STACK_MIN)
+    /* The system limit is unusably small.
+       Use the minimal size acceptable.  */
+    __default_stacksize = PTHREAD_STACK_MIN;
+  else
+    {
+      /* Round the resource limit up to page size.  */
+      const uintptr_t pagesz = __sysconf (_SC_PAGESIZE);
+      __default_stacksize = (limit.rlim_cur + pagesz - 1) & -pagesz;
+    }
+
+  /* Get the size of the static and alignment requirements for the TLS
+     block.  */
+  size_t static_tls_align;
+  _dl_get_tls_static_info (&__static_tls_size, &static_tls_align);
+
+  /* Make sure the size takes all the alignments into account.  */
+  if (STACK_ALIGN > static_tls_align)
+    static_tls_align = STACK_ALIGN;
+  __static_tls_align_m1 = static_tls_align - 1;
+
+  __static_tls_size = roundup (__static_tls_size, static_tls_align);
+
+#ifdef SHARED
+  /* Transfer the old value from the dynamic linker's internal location.  */
+  *__libc_dl_error_tsd () = *(*GL(dl_error_catch_tsd)) ();
+  GL(dl_error_catch_tsd) = &__libc_dl_error_tsd;
+
+  /* Make __rtld_lock_{,un}lock_recursive use pthread_mutex_{,un}lock,
+     keep the lock count from the ld.so implementation.  */
+  GL(dl_rtld_lock_recursive) = (void *) INTUSE (__pthread_mutex_lock);
+  GL(dl_rtld_unlock_recursive) = (void *) INTUSE (__pthread_mutex_unlock);
+  unsigned int rtld_lock_count = GL(dl_load_lock).mutex.__data.__count;
+  GL(dl_load_lock).mutex.__data.__count = 0;
+  while (rtld_lock_count-- > 0)
+    INTUSE (__pthread_mutex_lock) (&GL(dl_load_lock).mutex);
+
+  GL(dl_make_stack_executable_hook) = &__make_stacks_executable;
+#endif
+
+  GL(dl_init_static_tls) = &__pthread_init_static_tls;
+
+  /* Register the fork generation counter with the libc.  */
+#ifndef TLS_MULTIPLE_THREADS_IN_TCB
+  __libc_multiple_threads_ptr =
+#endif
+    __libc_pthread_init (&__fork_generation, __reclaim_stacks,
+			 ptr_pthread_functions);
+
+  /* Determine whether the machine is SMP or not.  */
+  __is_smp = is_smp_system ();
+}
+strong_alias (__pthread_initialize_minimal_internal,
+	      __pthread_initialize_minimal)
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/Makefile libc/nptl/sysdeps/unix/sysv/linux/Makefile
--- libc/nptl/sysdeps/unix/sysv/linux/Makefile	2004-01-03 09:55:30.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/Makefile	2005-01-23 19:35:29.000000000 +0100
@@ -21,10 +21,14 @@ ifeq ($(subdir),nptl)
 sysdep_routines += register-atfork unregister-atfork libc_pthread_init \
 		   libc_multiple_threads
 
-libpthread-sysdep_routines += pt-fork pthread_mutex_cond_lock
+libpthread-sysdep_routines += pt-fork pthread_mutex_cond_lock pt-vfork \
+			      ptw-llseek
 
 gen-as-const-headers += lowlevelcond.sym lowlevelrwlock.sym \
 			lowlevelbarrier.sym unwindbuf.sym
+
+tests += tst-vfork1 tst-vfork2 tst-vfork1x tst-vfork2x
+tests-reverse += tst-vfork1x tst-vfork2x
 endif
 
 ifeq ($(subdir),posix)
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/pthread_cancel.c libc/nptl/sysdeps/unix/sysv/linux/pthread_cancel.c
--- libc/nptl/sysdeps/unix/sysv/linux/pthread_cancel.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/pthread_cancel.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,102 @@
+/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <signal.h>
+#include "pthreadP.h"
+#include "atomic.h"
+#include <sysdep.h>
+#include <kernel-features.h>
+
+
+int
+pthread_cancel (th)
+     pthread_t th;
+{
+  volatile struct pthread *pd = (volatile struct pthread *) th;
+
+  /* Make sure the descriptor is valid.  */
+  if (INVALID_TD_P (pd))
+    /* Not a valid thread handle.  */
+    return ESRCH;
+
+#ifdef SHARED
+  pthread_cancel_init ();
+#endif
+  int result = 0;
+  int oldval;
+  int newval;
+  do
+    {
+      oldval = pd->cancelhandling;
+      newval = oldval | CANCELING_BITMASK | CANCELED_BITMASK;
+
+      /* Avoid doing unnecessary work.  The atomic operation can
+	 potentially be expensive if the bug has to be locked and
+	 remote cache lines have to be invalidated.  */
+      if (oldval == newval)
+	break;
+
+      /* If the cancellation is handled asynchronously just send a
+	 signal.  We avoid this if possible since it's more
+	 expensive.  */
+      if (CANCEL_ENABLED_AND_CANCELED_AND_ASYNCHRONOUS (newval))
+	{
+	  /* Mark the cancellation as "in progress".  */
+	  atomic_bit_set (&pd->cancelhandling, CANCELING_BIT);
+
+	  /* The cancellation handler will take care of marking the
+	     thread as canceled.  */
+	  INTERNAL_SYSCALL_DECL (err);
+
+	  /* One comment: The PID field in the TCB can temporarily be
+	     changed (in fork).  But this must not affect this code
+	     here.  Since this function would have to be called while
+	     the thread is executing fork, it would have to happen in
+	     a signal handler.  But this is no allowed, pthread_cancel
+	     is not guaranteed to be async-safe.  */
+	  int val;
+#if __ASSUME_TGKILL
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid), pd->tid,
+				  SIGCANCEL);
+#else
+# ifdef __NR_tgkill
+	  val = INTERNAL_SYSCALL (tgkill, err, 3,
+				  THREAD_GETMEM (THREAD_SELF, pid), pd->tid,
+				  SIGCANCEL);
+	  if (INTERNAL_SYSCALL_ERROR_P (val, err)
+	      && INTERNAL_SYSCALL_ERRNO (val, err) == ENOSYS)
+# endif
+	    val = INTERNAL_SYSCALL (tkill, err, 2, pd->tid, SIGCANCEL);
+#endif
+
+	  if (INTERNAL_SYSCALL_ERROR_P (val, err))
+	    result = INTERNAL_SYSCALL_ERRNO (val, err);
+
+	  break;
+	}
+    }
+  /* Mark the thread as canceled.  This has to be done
+     atomically since other bits could be modified as well.  */
+  while (atomic_compare_and_exchange_bool_acq (&pd->cancelhandling, newval,
+					       oldval));
+
+  return result;
+}
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/pthread_condattr_setclock.c libc/nptl/sysdeps/unix/sysv/linux/pthread_condattr_setclock.c
--- libc/nptl/sysdeps/unix/sysv/linux/pthread_condattr_setclock.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/pthread_condattr_setclock.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,72 @@
+/* Copyright (C) 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2003.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <assert.h>
+#include <errno.h>
+#include <stdbool.h>
+#include <time.h>
+#include <sysdep.h>
+#include "pthreadP.h"
+#include <kernel-features.h>
+
+
+int
+pthread_condattr_setclock (attr, clock_id)
+     pthread_condattr_t *attr;
+     clockid_t clock_id;
+{
+  /* Only a few clocks are allowed.  CLOCK_REALTIME is always allowed.
+     CLOCK_MONOTONIC only if the kernel has the necessary support.  */
+  if (clock_id == CLOCK_MONOTONIC)
+    {
+#ifndef __ASSUME_POSIX_TIMERS
+# ifdef __NR_clock_getres
+      /* Check whether the clock is available.  */
+      static int avail;
+
+      if (avail == 0)
+	{
+	  struct timespec ts;
+
+	  INTERNAL_SYSCALL_DECL (err);
+	  int val;
+	  val = INTERNAL_SYSCALL (clock_getres, err, 2, CLOCK_MONOTONIC, &ts);
+	  avail = INTERNAL_SYSCALL_ERROR_P (val, err) ? -1 : 1;
+	}
+
+      if (avail < 0)
+# endif
+	/* Not available.  */
+	return EINVAL;
+#endif
+    }
+  else if (clock_id != CLOCK_REALTIME)
+    /* If more clocks are allowed some day the storing of the clock ID
+       in the pthread_cond_t structure needs to be adjusted.  */
+    return EINVAL;
+
+  /* Make sure the value fits in the bits we reserved.  */
+  assert (clock_id < (1 << COND_CLOCK_BITS));
+
+  int *valuep = &((struct pthread_condattr *) attr)->value;
+
+  *valuep = (*valuep & ~(1 << (COND_CLOCK_BITS + 1)) & ~1) | (clock_id << 1);
+
+  return 0;
+}
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/pthread_create.c libc/nptl/sysdeps/unix/sysv/linux/pthread_create.c
--- libc/nptl/sysdeps/unix/sysv/linux/pthread_create.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/pthread_create.c	2005-01-23 20:07:20.000000000 +0100
@@ -0,0 +1,522 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <stdbool.h>
+#include <stdlib.h>
+#include <string.h>
+#include "pthreadP.h"
+#include <hp-timing.h>
+#include <ldsodefs.h>
+#include <atomic.h>
+#include <libc-internal.h>
+#include <resolv.h>
+
+#include <shlib-compat.h>
+
+
+/* Local function to start thread and handle cleanup.  */
+static int start_thread (void *arg);
+
+
+/* Nozero if debugging mode is enabled.  */
+int __pthread_debug;
+
+/* Globally enabled events.  */
+static td_thr_events_t __nptl_threads_events;
+
+/* Pointer to descriptor with the last event.  */
+static struct pthread *__nptl_last_event;
+
+/* Number of threads running.  */
+unsigned int __nptl_nthreads = 1;
+
+
+/* Code to allocate and deallocate a stack.  */
+#include "allocatestack.c"
+
+/* Code to create the thread.  */
+#include "createthread.c"
+
+
+struct pthread *
+internal_function
+__find_in_stack_list (pd)
+     struct pthread *pd;
+{
+  list_t *entry;
+  struct pthread *result = NULL;
+
+  lll_lock (stack_cache_lock);
+
+  list_for_each (entry, &stack_used)
+    {
+      struct pthread *curp;
+
+      curp = list_entry (entry, struct pthread, list);
+      if (curp == pd)
+	{
+	  result = curp;
+	  break;
+	}
+    }
+
+  if (result == NULL)
+    list_for_each (entry, &__stack_user)
+      {
+	struct pthread *curp;
+
+	curp = list_entry (entry, struct pthread, list);
+	if (curp == pd)
+	  {
+	    result = curp;
+	    break;
+	  }
+      }
+
+  lll_unlock (stack_cache_lock);
+
+  return result;
+}
+
+
+/* Deallocate POSIX thread-local-storage.  */
+void
+attribute_hidden
+__nptl_deallocate_tsd (void)
+{
+  struct pthread *self = THREAD_SELF;
+
+  /* Maybe no data was ever allocated.  This happens often so we have
+     a flag for this.  */
+  if (THREAD_GETMEM (self, specific_used))
+    {
+      size_t round;
+      size_t cnt;
+
+      round = 0;
+      do
+	{
+	  size_t idx;
+
+	  /* So far no new nonzero data entry.  */
+	  THREAD_SETMEM (self, specific_used, false);
+
+	  for (cnt = idx = 0; cnt < PTHREAD_KEY_1STLEVEL_SIZE; ++cnt)
+	    {
+	      struct pthread_key_data *level2;
+
+	      level2 = THREAD_GETMEM_NC (self, specific, cnt);
+
+	      if (level2 != NULL)
+		{
+		  size_t inner;
+
+		  for (inner = 0; inner < PTHREAD_KEY_2NDLEVEL_SIZE;
+		       ++inner, ++idx)
+		    {
+		      void *data = level2[inner].data;
+
+		      if (data != NULL)
+			{
+			  /* Always clear the data.  */
+			  level2[inner].data = NULL;
+
+			  /* Make sure the data corresponds to a valid
+			     key.  This test fails if the key was
+			     deallocated and also if it was
+			     re-allocated.  It is the user's
+			     responsibility to free the memory in this
+			     case.  */
+			  if (level2[inner].seq
+			      == __pthread_keys[idx].seq
+			      /* It is not necessary to register a destructor
+				 function.  */
+			      && __pthread_keys[idx].destr != NULL)
+			    /* Call the user-provided destructor.  */
+			    __pthread_keys[idx].destr (data);
+			}
+		    }
+		}
+	      else
+		idx += PTHREAD_KEY_1STLEVEL_SIZE;
+	    }
+
+	  if (THREAD_GETMEM (self, specific_used) == 0)
+	    /* No data has been modified.  */
+	    goto just_free;
+	}
+      /* We only repeat the process a fixed number of times.  */
+      while (__builtin_expect (++round < PTHREAD_DESTRUCTOR_ITERATIONS, 0));
+
+      /* Just clear the memory of the first block for reuse.  */
+      memset (&THREAD_SELF->specific_1stblock, '\0',
+	      sizeof (self->specific_1stblock));
+
+    just_free:
+      /* Free the memory for the other blocks.  */
+      for (cnt = 1; cnt < PTHREAD_KEY_1STLEVEL_SIZE; ++cnt)
+	{
+	  struct pthread_key_data *level2;
+
+	  level2 = THREAD_GETMEM_NC (self, specific, cnt);
+	  if (level2 != NULL)
+	    {
+	      /* The first block is allocated as part of the thread
+		 descriptor.  */
+	      free (level2);
+	      THREAD_SETMEM_NC (self, specific, cnt, NULL);
+	    }
+	}
+
+      THREAD_SETMEM (self, specific_used, false);
+    }
+}
+
+
+/* Deallocate a thread's stack after optionally making sure the thread
+   descriptor is still valid.  */
+void
+internal_function
+__free_tcb (struct pthread *pd)
+{
+  /* The thread is exiting now.  */
+  if (__builtin_expect (atomic_bit_test_set (&pd->cancelhandling,
+					     TERMINATED_BIT) == 0, 1))
+    {
+      /* Remove the descriptor from the list.  */
+      if (DEBUGGING_P && __find_in_stack_list (pd) == NULL)
+	/* Something is really wrong.  The descriptor for a still
+	   running thread is gone.  */
+	abort ();
+
+      /* Queue the stack memory block for reuse and exit the process.  The
+	 kernel will signal via writing to the address returned by
+	 QUEUE-STACK when the stack is available.  */
+      __deallocate_stack (pd);
+    }
+}
+
+
+static int
+start_thread (void *arg)
+{
+  struct pthread *pd = (struct pthread *) arg;
+
+#if HP_TIMING_AVAIL
+  /* Remember the time when the thread was started.  */
+  hp_timing_t now;
+  HP_TIMING_NOW (now);
+  THREAD_SETMEM (pd, cpuclock_offset, now);
+#endif
+
+  /* Initialize resolver state pointer.  */
+  __resp = &pd->res;
+
+  /* This is where the try/finally block should be created.  For
+     compilers without that support we do use setjmp.  */
+  struct pthread_unwind_buf unwind_buf;
+
+  /* No previous handlers.  */
+  unwind_buf.priv.data.prev = NULL;
+  unwind_buf.priv.data.cleanup = NULL;
+
+  int not_first_call;
+  not_first_call = setjmp ((struct __jmp_buf_tag *) unwind_buf.cancel_jmp_buf);
+  if (__builtin_expect (! not_first_call, 1))
+    {
+      /* Store the new cleanup handler info.  */
+      THREAD_SETMEM (pd, cleanup_jmp_buf, &unwind_buf);
+
+      if (__builtin_expect (pd->stopped_start, 0))
+	{
+	  int oldtype = CANCEL_ASYNC ();
+
+	  /* Get the lock the parent locked to force synchronization.  */
+	  lll_lock (pd->lock);
+	  /* And give it up right away.  */
+	  lll_unlock (pd->lock);
+
+	  CANCEL_RESET (oldtype);
+	}
+
+      /* Run the code the user provided.  */
+#ifdef CALL_THREAD_FCT
+      THREAD_SETMEM (pd, result, CALL_THREAD_FCT (pd));
+#else
+      THREAD_SETMEM (pd, result, pd->start_routine (pd->arg));
+#endif
+    }
+
+  /* Run the destructor for the thread-local data.  */
+  __nptl_deallocate_tsd ();
+
+  /* Clean up any state libc stored in thread-local variables.  */
+  __libc_thread_freeres ();
+
+  /* If this is the last thread we terminate the process now.  We
+     do not notify the debugger, it might just irritate it if there
+     is no thread left.  */
+  if (__builtin_expect (atomic_decrement_and_test (&__nptl_nthreads), 0))
+    /* This was the last thread.  */
+    exit (0);
+
+  /* Report the death of the thread if this is wanted.  */
+  if (__builtin_expect (pd->report_events, 0))
+    {
+      /* See whether TD_DEATH is in any of the mask.  */
+      const int idx = __td_eventword (TD_DEATH);
+      const uint32_t mask = __td_eventmask (TD_DEATH);
+
+      if ((mask & (__nptl_threads_events.event_bits[idx]
+		   | pd->eventbuf.eventmask.event_bits[idx])) != 0)
+	{
+	  /* Yep, we have to signal the death.  Add the descriptor to
+	     the list but only if it is not already on it.  */
+	  if (pd->nextevent == NULL)
+	    {
+	      pd->eventbuf.eventnum = TD_DEATH;
+	      pd->eventbuf.eventdata = pd;
+
+	      do
+		pd->nextevent = __nptl_last_event;
+	      while (atomic_compare_and_exchange_bool_acq (&__nptl_last_event,
+							   pd, pd->nextevent));
+	    }
+
+	  /* Now call the function to signal the event.  */
+	  __nptl_death_event ();
+	}
+    }
+
+  /* The thread is exiting now.  Don't set this bit until after we've hit
+     the event-reporting breakpoint, so that td_thr_get_info on us while at
+     the breakpoint reports TD_THR_RUN state rather than TD_THR_ZOMBIE.  */
+  atomic_bit_set (&pd->cancelhandling, EXITING_BIT);
+
+  /* If the thread is detached free the TCB.  */
+  if (IS_DETACHED (pd))
+    /* Free the TCB.  */
+    __free_tcb (pd);
+
+  /* We cannot call '_exit' here.  '_exit' will terminate the process.
+
+     The 'exit' implementation in the kernel will signal when the
+     process is really dead since 'clone' got passed the CLONE_CLEARTID
+     flag.  The 'tid' field in the TCB will be set to zero.
+
+     The exit code is zero since in case all threads exit by calling
+     'pthread_exit' the exit status must be 0 (zero).  */
+  __exit_thread_inline (0);
+
+  /* NOTREACHED */
+  return 0;
+}
+
+
+/* Default thread attributes for the case when the user does not
+   provide any.  */
+static const struct pthread_attr default_attr =
+  {
+    /* Just some value > 0 which gets rounded to the nearest page size.  */
+    .guardsize = 1,
+  };
+
+
+int
+__pthread_create_2_1 (newthread, attr, start_routine, arg)
+     pthread_t *newthread;
+     const pthread_attr_t *attr;
+     void *(*start_routine) (void *);
+     void *arg;
+{
+  STACK_VARIABLES;
+  const struct pthread_attr *iattr;
+  struct pthread *pd;
+  int err;
+
+  iattr = (struct pthread_attr *) attr;
+  if (iattr == NULL)
+    /* Is this the best idea?  On NUMA machines this could mean
+       accessing far-away memory.  */
+    iattr = &default_attr;
+
+  err = ALLOCATE_STACK (iattr, &pd);
+  if (__builtin_expect (err != 0, 0))
+    /* Something went wrong.  Maybe a parameter of the attributes is
+       invalid or we could not allocate memory.  */
+    return err;
+
+
+  /* Initialize the TCB.  All initializations with zero should be
+     performed in 'get_cached_stack'.  This way we avoid doing this if
+     the stack freshly allocated with 'mmap'.  */
+
+#ifdef TLS_TCB_AT_TP
+  /* Reference to the TCB itself.  */
+  pd->header.self = pd;
+
+  /* Self-reference for TLS.  */
+  pd->header.tcb = pd;
+#endif
+
+  /* Store the address of the start routine and the parameter.  Since
+     we do not start the function directly the stillborn thread will
+     get the information from its thread descriptor.  */
+  pd->start_routine = start_routine;
+  pd->arg = arg;
+
+  /* Copy the thread attribute flags.  */
+  struct pthread *self = THREAD_SELF;
+  pd->flags = ((iattr->flags & ~(ATTR_FLAG_SCHED_SET | ATTR_FLAG_POLICY_SET))
+	       | (self->flags & (ATTR_FLAG_SCHED_SET | ATTR_FLAG_POLICY_SET)));
+
+  /* Initialize the field for the ID of the thread which is waiting
+     for us.  This is a self-reference in case the thread is created
+     detached.  */
+  pd->joinid = iattr->flags & ATTR_FLAG_DETACHSTATE ? pd : NULL;
+
+  /* The debug events are inherited from the parent.  */
+  pd->eventbuf = self->eventbuf;
+
+
+  /* Copy the parent's scheduling parameters.  The flags will say what
+     is valid and what is not.  */
+  pd->schedpolicy = self->schedpolicy;
+  pd->schedparam = self->schedparam;
+
+  /* Determine scheduling parameters for the thread.  */
+  if (attr != NULL
+      && __builtin_expect ((iattr->flags & ATTR_FLAG_NOTINHERITSCHED) != 0, 0)
+      && (iattr->flags & (ATTR_FLAG_SCHED_SET | ATTR_FLAG_POLICY_SET)) != 0)
+    {
+      INTERNAL_SYSCALL_DECL (err);
+
+      /* Use the scheduling parameters the user provided.  */
+      if (iattr->flags & ATTR_FLAG_POLICY_SET)
+	pd->schedpolicy = iattr->schedpolicy;
+      else if ((pd->flags & ATTR_FLAG_POLICY_SET) == 0)
+	{
+	  pd->schedpolicy = INTERNAL_SYSCALL (sched_getscheduler, err, 1, 0);
+	  pd->flags |= ATTR_FLAG_POLICY_SET;
+	}
+
+      if (iattr->flags & ATTR_FLAG_SCHED_SET)
+	memcpy (&pd->schedparam, &iattr->schedparam,
+		sizeof (struct sched_param));
+      else if ((pd->flags & ATTR_FLAG_SCHED_SET) == 0)
+	{
+	  INTERNAL_SYSCALL (sched_getparam, err, 2, 0, &pd->schedparam);
+	  pd->flags |= ATTR_FLAG_SCHED_SET;
+	}
+
+      /* Check for valid priorities.  */
+      int minprio = INTERNAL_SYSCALL (sched_get_priority_min, err, 1,
+				      iattr->schedpolicy);
+      int maxprio = INTERNAL_SYSCALL (sched_get_priority_max, err, 1,
+				      iattr->schedpolicy);
+      if (pd->schedparam.sched_priority < minprio
+	  || pd->schedparam.sched_priority > maxprio)
+	{
+	  err = EINVAL;
+	  goto errout;
+	}
+    }
+
+  /* Pass the descriptor to the caller.  */
+  *newthread = (pthread_t) pd;
+
+  /* Remember whether the thread is detached or not.  In case of an
+     error we have to free the stacks of non-detached stillborn
+     threads.  */
+  bool is_detached = IS_DETACHED (pd);
+
+  /* Start the thread.  */
+  err = create_thread (pd, iattr, STACK_VARIABLES_ARGS);
+  if (err != 0)
+    {
+      /* Something went wrong.  Free the resources.  */
+      if (!is_detached)
+	{
+	errout:
+	  __deallocate_stack (pd);
+	}
+      return err;
+    }
+
+  return 0;
+}
+versioned_symbol (libpthread, __pthread_create_2_1, pthread_create, GLIBC_2_1);
+
+
+#if SHLIB_COMPAT(libpthread, GLIBC_2_0, GLIBC_2_1)
+int
+__pthread_create_2_0 (newthread, attr, start_routine, arg)
+     pthread_t *newthread;
+     const pthread_attr_t *attr;
+     void *(*start_routine) (void *);
+     void *arg;
+{
+  /* The ATTR attribute is not really of type `pthread_attr_t *'.  It has
+     the old size and access to the new members might crash the program.
+     We convert the struct now.  */
+  struct pthread_attr new_attr;
+
+  if (attr != NULL)
+    {
+      struct pthread_attr *iattr = (struct pthread_attr *) attr;
+      size_t ps = __getpagesize ();
+
+      /* Copy values from the user-provided attributes.  */
+      new_attr.schedparam = iattr->schedparam;
+      new_attr.schedpolicy = iattr->schedpolicy;
+      new_attr.flags = iattr->flags;
+
+      /* Fill in default values for the fields not present in the old
+	 implementation.  */
+      new_attr.guardsize = ps;
+      new_attr.stackaddr = NULL;
+      new_attr.stacksize = 0;
+      new_attr.cpuset = NULL;
+
+      /* We will pass this value on to the real implementation.  */
+      attr = (pthread_attr_t *) &new_attr;
+    }
+
+  return __pthread_create_2_1 (newthread, attr, start_routine, arg);
+}
+compat_symbol (libpthread, __pthread_create_2_0, pthread_create,
+	       GLIBC_2_0);
+#endif
+
+/* Information for libthread_db.  */
+
+#include "../nptl_db/db_info.c"
+
+/* If pthread_create is present, libgcc_eh.a and libsupc++.a expects some other POSIX thread
+   functions to be present as well.  */
+PTHREAD_STATIC_FN_REQUIRE (pthread_mutex_lock)
+PTHREAD_STATIC_FN_REQUIRE (pthread_mutex_unlock)
+
+PTHREAD_STATIC_FN_REQUIRE (pthread_once)
+PTHREAD_STATIC_FN_REQUIRE (pthread_cancel)
+
+PTHREAD_STATIC_FN_REQUIRE (pthread_key_create)
+PTHREAD_STATIC_FN_REQUIRE (pthread_setspecific)
+PTHREAD_STATIC_FN_REQUIRE (pthread_getspecific)
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/pthread_sigmask.c libc/nptl/sysdeps/unix/sysv/linux/pthread_sigmask.c
--- libc/nptl/sysdeps/unix/sysv/linux/pthread_sigmask.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/pthread_sigmask.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,58 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <signal.h>
+#include <pthreadP.h>
+#include <sysdep.h>
+
+
+int
+pthread_sigmask (how, newmask, oldmask)
+     int how;
+     const sigset_t *newmask;
+     sigset_t *oldmask;
+{
+  sigset_t local_newmask;
+
+  /* The only thing we have to make sure here is that SIGCANCEL and
+     SIGSETXID is not blocked.  */
+  if (newmask != NULL
+      && (__builtin_expect (__sigismember (newmask, SIGCANCEL), 0)
+	  || __builtin_expect (__sigismember (newmask, SIGSETXID), 0)))
+    {
+      local_newmask = *newmask;
+      __sigdelset (&local_newmask, SIGCANCEL);
+      __sigdelset (&local_newmask, SIGSETXID);
+      newmask = &local_newmask;
+    }
+
+#ifdef INTERNAL_SYSCALL
+  /* We know that realtime signals are available if NPTL is used.  */
+  INTERNAL_SYSCALL_DECL (err);
+  int result = INTERNAL_SYSCALL (rt_sigprocmask, err, 4, how, newmask,
+				 oldmask, _NSIG / 8);
+
+  return (INTERNAL_SYSCALL_ERROR_P (result, err)
+	  ? INTERNAL_SYSCALL_ERRNO (result, err)
+	  : 0);
+#else
+  return sigprocmask (how, newmask, oldmask) == -1 ? errno : 0;
+#endif
+}
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/sem_open.c libc/nptl/sysdeps/unix/sysv/linux/sem_open.c
--- libc/nptl/sysdeps/unix/sysv/linux/sem_open.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/sem_open.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,404 @@
+/* Copyright (C) 2002, 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <errno.h>
+#include <fcntl.h>
+#include <mntent.h>
+#include <paths.h>
+#include <pthread.h>
+#include <search.h>
+#include <semaphore.h>
+#include <stdarg.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <sys/statfs.h>
+#include <linux_fsinfo.h>
+#include "semaphoreP.h"
+
+
+
+/* Information about the mount point.  */
+struct mountpoint_info mountpoint attribute_hidden;
+
+/* This is the default mount point.  */
+static const char defaultmount[] = "/dev/shm";
+/* This is the default directory.  */
+static const char defaultdir[] = "/dev/shm/sem.";
+
+/* Protect the `mountpoint' variable above.  */
+pthread_once_t __namedsem_once attribute_hidden = PTHREAD_ONCE_INIT;
+
+
+/* Determine where the shmfs is mounted (if at all).  */
+void
+attribute_hidden
+__where_is_shmfs (void)
+{
+  char buf[512];
+  struct statfs f;
+  struct mntent resmem;
+  struct mntent *mp;
+  FILE *fp;
+
+  /* The canonical place is /dev/shm.  This is at least what the
+     documentation tells everybody to do.  */
+  if (__statfs (defaultmount, &f) == 0 && f.f_type == SHMFS_SUPER_MAGIC)
+    {
+      /* It is in the normal place.  */
+      mountpoint.dir = (char *) defaultdir;
+      mountpoint.dirlen = sizeof (defaultdir) - 1;
+
+      return;
+    }
+
+  /* OK, do it the hard way.  Look through the /proc/mounts file and if
+     this does not exist through /etc/fstab to find the mount point.  */
+  fp = __setmntent ("/proc/mounts", "r");
+  if (__builtin_expect (fp == NULL, 0))
+    {
+      fp = __setmntent (_PATH_MNTTAB, "r");
+      if (__builtin_expect (fp == NULL, 0))
+	/* There is nothing we can do.  Blind guesses are not helpful.  */
+	return;
+    }
+
+  /* Now read the entries.  */
+  while ((mp = __getmntent_r (fp, &resmem, buf, sizeof buf)) != NULL)
+    /* The original name is "shm" but this got changed in early Linux
+       2.4.x to "tmpfs".  */
+    if (strcmp (mp->mnt_type, "tmpfs") == 0
+	|| strcmp (mp->mnt_type, "shm") == 0)
+      {
+	/* Found it.  There might be more than one place where the
+           filesystem is mounted but one is enough for us.  */
+	size_t namelen;
+
+	/* First make sure this really is the correct entry.  At least
+	   some versions of the kernel give wrong information because
+	   of the implicit mount of the shmfs for SysV IPC.  */
+	if (__statfs (mp->mnt_dir, &f) != 0 || f.f_type != SHMFS_SUPER_MAGIC)
+	  continue;
+
+	namelen = strlen (mp->mnt_dir);
+
+	if (namelen == 0)
+	  /* Hum, maybe some crippled entry.  Keep on searching.  */
+	  continue;
+
+	mountpoint.dir = (char *) malloc (namelen + 4 + 2);
+	if (mountpoint.dir != NULL)
+	  {
+	    char *cp = __mempcpy (mountpoint.dir, mp->mnt_dir, namelen);
+	    if (cp[-1] != '/')
+	      *cp++ = '/';
+	    cp = stpcpy (cp, "sem.");
+	    mountpoint.dirlen = cp - mountpoint.dir;
+	  }
+
+	break;
+      }
+
+  /* Close the stream.  */
+  __endmntent (fp);
+}
+
+
+/* Comparison function for search of existing mapping.  */
+int
+attribute_hidden
+__sem_search (const void *a, const void *b)
+{
+  const struct inuse_sem *as = (const struct inuse_sem *) a;
+  const struct inuse_sem *bs = (const struct inuse_sem *) b;
+
+  if (as->ino != bs->ino)
+    /* Cannot return the difference the type is larger than int.  */
+    return as->ino < bs->ino ? -1 : (as->ino == bs->ino ? 0 : 1);
+
+  if (as->dev != bs->dev)
+    /* Cannot return the difference the type is larger than int.  */
+    return as->dev < bs->dev ? -1 : (as->dev == bs->dev ? 0 : 1);
+
+  return strcmp (as->name, bs->name);
+}
+
+
+/* The search tree for existing mappings.  */
+void *__sem_mappings attribute_hidden;
+
+/* Lock to protect the search tree.  */
+lll_lock_t __sem_mappings_lock = LLL_LOCK_INITIALIZER;
+
+
+/* Search for existing mapping and if possible add the one provided.  */
+static sem_t *
+check_add_mapping (const char *name, size_t namelen, int fd, sem_t *existing)
+{
+  sem_t *result = SEM_FAILED;
+
+  /* Get the information about the file.  */
+  struct stat64 st;
+  if (__fxstat64 (_STAT_VER, fd, &st) == 0)
+    {
+      /* Get the lock.  */
+      lll_lock (__sem_mappings_lock);
+
+      /* Search for an existing mapping given the information we have.  */
+      struct inuse_sem *fake;
+      fake = (struct inuse_sem *) alloca (sizeof (*fake) + namelen);
+      memcpy (fake->name, name, namelen);
+      fake->dev = st.st_dev;
+      fake->ino = st.st_ino;
+
+      struct inuse_sem **foundp = tfind (fake, &__sem_mappings, __sem_search);
+      if (foundp != NULL)
+	{
+	  /* There is already a mapping.  Use it.  */
+	  result = (*foundp)->sem;
+	  ++(*foundp)->refcnt;
+	}
+      else
+	{
+	  /* We haven't found a mapping.  Install ione.  */
+	  struct inuse_sem *newp;
+
+	  newp = (struct inuse_sem *) malloc (sizeof (*newp) + namelen);
+	  if (newp != NULL)
+	    {
+	      /* If the caller hasn't provided any map it now.  */
+	      if (existing == SEM_FAILED)
+		existing = (sem_t *) mmap (NULL, sizeof (sem_t),
+					   PROT_READ | PROT_WRITE, MAP_SHARED,
+					   fd, 0);
+
+	      newp->dev = st.st_dev;
+	      newp->ino = st.st_ino;
+	      newp->refcnt = 1;
+	      newp->sem = existing;
+	      memcpy (newp->name, name, namelen);
+
+	      /* Insert the new value.  */
+	      if (existing != MAP_FAILED
+		  && tsearch (newp, &__sem_mappings, __sem_search) != NULL)
+		/* Successful.  */
+		result = existing;
+	      else
+		/* Something went wrong while inserting the new
+		   value.  We fail completely.  */
+		free (newp);
+	    }
+	}
+
+      /* Release the lock.  */
+      lll_unlock (__sem_mappings_lock);
+    }
+
+  if (result != existing && existing != SEM_FAILED && existing != MAP_FAILED)
+    {
+      /* Do not disturb errno.  */
+      INTERNAL_SYSCALL_DECL (err);
+      INTERNAL_SYSCALL (munmap, err, 2, existing, sizeof (sem_t));
+    }
+
+  return result;
+}
+
+
+sem_t *
+sem_open (const char *name, int oflag, ...)
+{
+  char *finalname;
+  sem_t *result = SEM_FAILED;
+  int fd;
+
+  /* Determine where the shmfs is mounted.  */
+  INTUSE(__pthread_once) (&__namedsem_once, __where_is_shmfs);
+
+  /* If we don't know the mount points there is nothing we can do.  Ever.  */
+  if (mountpoint.dir == NULL)
+    {
+      __set_errno (ENOSYS);
+      return SEM_FAILED;
+    }
+
+  /* Construct the filename.  */
+  while (name[0] == '/')
+    ++name;
+
+  if (name[0] == '\0')
+    {
+      /* The name "/" is not supported.  */
+      __set_errno (EINVAL);
+      return SEM_FAILED;
+    }
+  size_t namelen = strlen (name) + 1;
+
+  /* Create the name of the final file.  */
+  finalname = (char *) alloca (mountpoint.dirlen + namelen);
+  __mempcpy (__mempcpy (finalname, mountpoint.dir, mountpoint.dirlen),
+	     name, namelen);
+
+  /* If the semaphore object has to exist simply open it.  */
+  if ((oflag & O_CREAT) == 0 || (oflag & O_EXCL) == 0)
+    {
+    try_again:
+      fd = __libc_open (finalname,
+			(oflag & ~(O_CREAT|O_ACCMODE)) | O_NOFOLLOW | O_RDWR);
+
+      if (fd == -1)
+	{
+	  /* If we are supposed to create the file try this next.  */
+	  if ((oflag & O_CREAT) != 0 && errno == ENOENT)
+	    goto try_create;
+
+	  /* Return.  errno is already set.  */
+	}
+      else
+	/* Check whether we already have this semaphore mapped and
+	   create one if necessary.  */
+	result = check_add_mapping (name, namelen, fd, SEM_FAILED);
+    }
+  else
+    {
+      /* We have to open a temporary file first since it must have the
+	 correct form before we can start using it.  */
+      char *tmpfname;
+      mode_t mode;
+      unsigned int value;
+      va_list ap;
+
+    try_create:
+      va_start (ap, oflag);
+
+      mode = va_arg (ap, mode_t);
+      value = va_arg (ap, unsigned int);
+
+      va_end (ap);
+
+      if (value > SEM_VALUE_MAX)
+	{
+	  __set_errno (EINVAL);
+	  return SEM_FAILED;
+	}
+
+      /* Create the initial file content.  */
+      sem_t initsem;
+
+      struct sem *iinitsem = (struct sem *) &initsem;
+      iinitsem->count = value;
+
+      /* Initialize the remaining bytes as well.  */
+      memset ((char *) &initsem + sizeof (struct sem), '\0',
+	      sizeof (sem_t) - sizeof (struct sem));
+
+      tmpfname = (char *) alloca (mountpoint.dirlen + 6 + 1);
+      char *xxxxxx = __mempcpy (tmpfname, mountpoint.dir, mountpoint.dirlen);
+
+      int retries = 0;
+#define NRETRIES 50
+      while (1)
+	{
+	  /* Add the suffix for mktemp.  */
+	  strcpy (xxxxxx, "XXXXXX");
+
+	  /* We really want to use mktemp here.  We cannot use mkstemp
+	     since the file must be opened with a specific mode.  The
+	     mode cannot later be set since then we cannot apply the
+	     file create mask.  */
+	  if (mktemp (tmpfname) == NULL)
+	    return SEM_FAILED;
+
+	  /* Open the file.  Make sure we do not overwrite anything.  */
+	  fd = __libc_open (tmpfname, O_RDWR | O_CREAT | O_EXCL, mode);
+	  if (fd == -1)
+	    {
+	      if (errno == EEXIST)
+		{
+		  if (++retries < NRETRIES)
+		    continue;
+
+		  __set_errno (EAGAIN);
+		}
+
+	      return SEM_FAILED;
+	    }
+
+	  /* We got a file.  */
+	  break;
+	}
+
+      if (TEMP_FAILURE_RETRY (__libc_write (fd, &initsem, sizeof (sem_t)))
+	  == sizeof (sem_t)
+	  /* Map the sem_t structure from the file.  */
+	  && (result = (sem_t *) mmap (NULL, sizeof (sem_t),
+				       PROT_READ | PROT_WRITE, MAP_SHARED,
+				       fd, 0)) != MAP_FAILED)
+	{
+	  /* Create the file.  Don't overwrite an existing file.  */
+	  if (link (tmpfname, finalname) != 0)
+	    {
+	      /* Undo the mapping.  */
+	      (void) munmap (result, sizeof (sem_t));
+
+	      /* Reinitialize 'result'.  */
+	      result = SEM_FAILED;
+
+	      /* This failed.  If O_EXCL is not set and the problem was
+		 that the file exists, try again.  */
+	      if ((oflag & O_EXCL) == 0 && errno == EEXIST)
+		{
+		  /* Remove the file.  */
+		  (void) unlink (tmpfname);
+
+		  /* Close the file.  */
+		  (void) __libc_close (fd);
+
+		  goto try_again;
+		}
+	    }
+	  else
+	    /* Insert the mapping into the search tree.  This also
+	       determines whether another thread sneaked by and already
+	       added such a mapping despite the fact that we created it.  */
+	    result = check_add_mapping (name, namelen, fd, result);
+	}
+
+      /* Now remove the temporary name.  This should never fail.  If
+	 it fails we leak a file name.  Better fix the kernel.  */
+      (void) unlink (tmpfname);
+    }
+
+  /* Map the mmap error to the error we need.  */
+  if (MAP_FAILED != (void *) SEM_FAILED && result == MAP_FAILED)
+    result = SEM_FAILED;
+
+  /* We don't need the file descriptor anymore.  */
+  if (fd != -1)
+    {
+      /* Do not disturb errno.  */
+      INTERNAL_SYSCALL_DECL (err);
+      INTERNAL_SYSCALL (close, err, 1, fd);
+    }
+
+  return result;
+}
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/sigaction.c libc/nptl/sysdeps/unix/sysv/linux/sigaction.c
--- libc/nptl/sysdeps/unix/sysv/linux/sigaction.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/sigaction.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,54 @@
+/* Copyright (C) 2002, 2003, 2004 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+/* This is tricky.  GCC doesn't like #include_next in the primary
+   source file and even if it did, the first #include_next is this
+   exact file anyway.  */
+#ifndef LIBC_SIGACTION
+
+#include <nptl/pthreadP.h>
+
+/* We use the libc implementation but we tell it to not allow
+   SIGCANCEL or SIGTIMER to be handled.  */
+# define LIBC_SIGACTION	1
+
+# include <nptl/sysdeps/pthread/sigaction.c>
+
+int
+__sigaction (sig, act, oact)
+     int sig;
+     const struct sigaction *act;
+     struct sigaction *oact;
+{
+  if (__builtin_expect (sig == SIGCANCEL || sig == SIGSETXID, 0))
+    {
+      __set_errno (EINVAL);
+      return -1;
+    }
+
+  return __libc_sigaction (sig, act, oact);
+}
+libc_hidden_weak (__sigaction)
+weak_alias (__sigaction, sigaction)
+
+#else
+
+# include_next <sigaction.c>
+
+#endif /* LIBC_SIGACTION */
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/sigfillset.c libc/nptl/sysdeps/unix/sysv/linux/sigfillset.c
--- libc/nptl/sysdeps/unix/sysv/linux/sigfillset.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/sigfillset.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,21 @@
+/* Copyright (C) 2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <nptl/pthreadP.h>
+
+#include <sysdeps/generic/sigfillset.c>
diff -x CVS -rupN libc/nptl/sysdeps/unix/sysv/linux/sigprocmask.c libc/nptl/sysdeps/unix/sysv/linux/sigprocmask.c
--- libc/nptl/sysdeps/unix/sysv/linux/sigprocmask.c	1970-01-01 01:00:00.000000000 +0100
+++ libc/nptl/sysdeps/unix/sysv/linux/sigprocmask.c	2005-01-23 19:35:29.000000000 +0100
@@ -0,0 +1,20 @@
+/* Copyright (C) 1997,1998,1999,2000,2001,2003 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+   The GNU C Library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 2.1 of the License, or (at your option) any later version.
+
+   The GNU C Library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with the GNU C Library; if not, write to the Free
+   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307 USA.  */
+
+#include <nptl/pthreadP.h>
+#include <sysdeps/unix/sysv/linux/sigprocmask.c>
diff -x CVS -rupN libc/nptl/unwind.c libc/nptl/unwind.c
--- libc/nptl/unwind.c	2004-10-10 12:41:05.000000000 +0200
+++ libc/nptl/unwind.c	2005-01-23 19:35:29.000000000 +0100
@@ -105,9 +105,11 @@ unwind_cleanup (_Unwind_Reason_Code reas
   /* When we get here a C++ catch block didn't rethrow the object.  We
      cannot handle this case and therefore abort.  */
 # define STR_N_LEN(str) str, strlen (str)
+#if 0
   INTERNAL_SYSCALL_DECL (err);
   INTERNAL_SYSCALL (write, err, 3, STDERR_FILENO,
 		    STR_N_LEN ("FATAL: exception not rethrown\n"));
+#endif
   abort ();
 }
 
